{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten, Conv2D, Bidirectional\n",
    "from keras.utils import plot_model, vis_utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import string\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Task 4 improving the models imports\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has been preprocessed by removing all the \" characters: sed -i 's/\"//g' *.txt\n",
    "# as this caused issues reading the data as a csv file. \n",
    "# Also had to remove a blank line from subtask A 2016 test data \n",
    "# TODO instead change the quote char in the read_csv call\n",
    "\n",
    "# Load the data\n",
    "fileGlob = glob.glob('./task3Data/*.txt')\n",
    "\n",
    "traindf = pd.concat([pd.read_csv(f, sep='\\t', header=None, keep_default_na=False) for f in fileGlob], ignore_index = True)\n",
    "traindf.columns = ['id','topic','label','raw']\n",
    "#traindf = traindf.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "Convert the text into vector my using a pre-trained system. \n",
    "Convert text into a vector by using a NN to train the embeddings. \n",
    "\n",
    "Looks like i need to handle the weird character replacement that's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do some preprocessing on the data to remove stop words, punctuation and probably stem the words too. \n",
    "# Need to handle the /u002c and other unicdoe character artifacts that are happening. \n",
    "# If removing punctuation then may want to simply remove them \n",
    "# but if expanding contractinons will need to convert them first. - a library exists to do this\n",
    "# Need to check to see what's happening with emojis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\m'\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\,'\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\o'\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\l'\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def preprocess(tweet, stop_words, target):\n",
    "    # Handle utf8 unicode problems\n",
    "    #print(tweet)\n",
    "    #tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    tweet = tweet.encode('utf8').decode('unicode_escape', 'ignore') \n",
    "    tweet = contractions.fix(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    if target.lower() in tweet:\n",
    "        tweet = tweet.replace(target,\"<TARGETTOKEN>\")\n",
    "        \n",
    "    tweetLine = word_tokenize(tweet)\n",
    "    # remove all tokens that are not alphabetic or stopwords, also lower the words\n",
    "    tweetLine = [word for word in tweetLine if word not in stop_words and word not in string.punctuation]\n",
    "    return tweetLine\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "traindf['text'] = traindf.apply(lambda row: preprocess(row['raw'], stop_words, row['topic']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "['TARGETTOKEN', 'systems', 'technical', 'university', 'come', 'visit', 'TARGETTOKEN', 'global', 'training', 'providers', 'TARGETTOKEN', 'stu15', 'TARGETTOKEN', 'training', 'starts', '01/sep', 'http', '//t.co/yntxyrlyod']\n",
      "39\n",
      "['work', 'friday', 'night', 'lt', 'lt', 'lt', 'lt', 'lt', 'TARGETTOKEN', 'bound', 'morning', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to ensure tweets are tweet length\n",
    "maxi = 0\n",
    "for text in traindf.text:\n",
    "    length = len(' '.join(text))\n",
    "    if length > maxi:\n",
    "        maxi = length\n",
    "        sanityCheck = text\n",
    "print(maxi)\n",
    "print(sanityCheck)\n",
    "\n",
    "maxi = 0\n",
    "for text in traindf.text:\n",
    "    length = len(text)\n",
    "    if length > maxi:\n",
    "        maxi = length\n",
    "        sanityCheck = text\n",
    "print(maxi)\n",
    "print(sanityCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't stop thinking about the fact that I'm going to be in the presence of Snoop Dogg on Sunday\n",
      "['stop', 'thinking', 'fact', 'going', 'presence', 'TARGETTOKEN', 'sunday']\n",
      "2\n",
      "snoop dogg\n",
      "['stop', 'thinking', 'fact', 'going', 'presence', 'TARGETTOKEN', 'sunday']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 10000\n",
    "sample = traindf.loc[traindf.id == 641648318754516992]\n",
    "\n",
    "print(sample.raw.item())\n",
    "print(sample.text.item())\n",
    "print(sample.label.item())\n",
    "print(sample.topic.item())\n",
    "sampleLine = preprocess(sample.raw.item(),stop_words, sample.topic.item())\n",
    "print(sampleLine)\n",
    "print(sample.topic.item() in sample.raw.item().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index-word relationship\n",
    "word2idx = {'<PAD>': 0, '<UNK>' : 1,'TARGETTOKEN' : 2 }\n",
    "idx2word ={}\n",
    "sents_as_ids = []\n",
    "for line in traindf.text:\n",
    "    sentId = []\n",
    "    for word in line:\n",
    "        if word in word2idx:\n",
    "            sentId.append(word2idx[word])\n",
    "            continue\n",
    "        count = len(word2idx)\n",
    "        word2idx[word] = count\n",
    "        idx2word[count] = word\n",
    "        sentId.append(count)\n",
    "    sents_as_ids.append(sentId)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTextToNumSeq(text, word2idx,MAXIMUM_LENGTH):\n",
    "    numSeq = []\n",
    "    for word in text:\n",
    "        if word in word2idx:\n",
    "            numSeq.append(word2idx[word])\n",
    "        else:\n",
    "            # If unseen put in unknown\n",
    "            numSeq.append(1) \n",
    "                \n",
    "    numSeq = pad_sequences([numSeq],MAXIMUM_LENGTH )\n",
    "    return numSeq\n",
    "\n",
    "MAXIMUM_LENGTH = 50 # Motivated because max sequence of words i had was 32\n",
    "\n",
    "traindf['numSeq'] = traindf.apply(lambda row: convertTextToNumSeq(row['text'], word2idx, MAXIMUM_LENGTH),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    3248\n",
      " 1    3230\n",
      "-1     850\n",
      " 2     255\n",
      "-2      75\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(traindf.numSeq, traindf.label, stratify=traindf.label, random_state =2)\n",
    "labelDist = y_val.value_counts()\n",
    "print(labelDist)\n",
    "x_train = np.array([x for y in x_train for x in y]).reshape(len(x_train),MAXIMUM_LENGTH)\n",
    "x_val = np.array([x for y in x_val for x in y]).reshape(len(x_val),MAXIMUM_LENGTH)\n",
    "\n",
    "\n",
    "labelCount = len(labelDist)\n",
    "#Y data is categorical therefore must be converted to a vector\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(np.array(y_train).reshape(len(y_train),1))\n",
    "y_val = onehot_encoder.transform(np.array(y_val).reshape(len(y_val),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 6,080,905\n",
      "Trainable params: 6,080,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 60000\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(labelCount, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "weightsFilePath=\"task3Weights.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/5\n",
      "22974/22974 [==============================] - 29s 1ms/step - loss: 1.1272 - acc: 0.4759 - val_loss: 1.0289 - val_acc: 0.5504\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55040, saving model to task3Weights.best.hdf5\n",
      "Epoch 2/5\n",
      "22974/22974 [==============================] - 29s 1ms/step - loss: 0.8300 - acc: 0.6672 - val_loss: 1.0087 - val_acc: 0.5615\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55040 to 0.56150, saving model to task3Weights.best.hdf5\n",
      "Epoch 3/5\n",
      "22974/22974 [==============================] - 29s 1ms/step - loss: 0.4665 - acc: 0.8298 - val_loss: 1.2047 - val_acc: 0.5418\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.56150\n",
      "Epoch 4/5\n",
      "22974/22974 [==============================] - 29s 1ms/step - loss: 0.2621 - acc: 0.9079 - val_loss: 1.5104 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.56150\n",
      "Epoch 5/5\n",
      "22974/22974 [==============================] - 29s 1ms/step - loss: 0.1631 - acc: 0.9434 - val_loss: 1.8075 - val_acc: 0.5108\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.56150\n"
     ]
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "checkpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(x_train,y_train,epochs=5,batch_size=128,validation_data=(x_val, y_val), callbacks=[checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VOW9+PHPNxsJZGFJ2BIg7LsKhE1s1aIVrYq1VEHZBIvaa91uF+21tlp7623v77rUVq+yi4rWpUVb9Vq11UIl7DsIsiYhEAIkJGSf7++PcwhDmJABmZzJ5Pt+veaVszwz850Dc77zPM85zyOqijHGGAMQ5XUAxhhjwoclBWOMMbUsKRhjjKllScEYY0wtSwrGGGNqWVIwxhhTy5KCiTgikikiKiIxQZSdLiL/bIy4GpuIfE1Etnkdh2laLCkYT4nIbhGpFJHUOtvXuCf2TG8iOyWWRBEpEZH3vI7lbKjqZ6ra1+s4TNNiScGEg13ApBMrIjIYaOldOKf5DlABXCkiHRvzjYOp7RhzPllSMOHgJWCq3/o0YKF/ARFJEZGFIlIgIntE5GERiXL3RYvIf4vIIRHZCXwrwHPniMh+EckVkcdFJPos4psGPA+sBybXee0uIvKWG1ehiDzrt+97IrJFRI6JyGYRGepuVxHp5Vduvog87i5fJiI5IvITEckH5olIGxF5132PI+5yht/z24rIPBHJc/f/yf+1/Mp1FpE33dfZJSL3+O0bISIrRaRYRA6IyP+cxfExEcSSggkHnwPJItLfPVlPBBbVKfM7IAXoAVyKk0Ruc/d9D7gWGAJkARPqPHc+UA30cst8E7g9mMBEpBtwGfCy+5jqty8aeBfYA2QC6cBid993gV+45ZOB64HCYN4T6Ai0BboBs3C+p/Pc9a5AGfCsX/mXcGpWA4H2wJMBPkcU8A6wzo1zLHCfiFzlFnkaeFpVk4GewOtBxmoijarawx6ePYDdwBXAw8CvgXHAh0AMoDgn22igEhjg97w7gL+7yx8Dd/rt+6b73BigA07TT4Lf/knAJ+7ydOCfZ4jvYWCtu5wO1ABD3PXRQAEQE+B5HwD31vOaCvTyW58PPO4uX+Z+1vgzxHQRcMRd7gT4gDYByl0G5LjLI4G9dfY/BMxzlz8FHgVSvf4/YQ9vH9ZeacLFSzgnpu7UaToCUoFYnF/kJ+zBOUkDdAb21dl3Qjf3uftF5MS2qDrlz2Qq8CKAquaKyD9wmpPWAF2APapaHeB5XYAvg3yPugpUtfzEioi0xPn1Pw5o425OcmsqXYDDqnqkgdfsBnQWkaN+26KBz9zlmcBjwFYR2QU8qqrvnmP8pgmz5iMTFlR1D06H8zXAW3V2HwKqcE5sJ3QFct3l/TgnR/99J+zDqSmkqmpr95GsqgMbiklELgZ6Aw+JSL7bxj8SuMXtAN4HdK2nM3gfTjNMIMc5tSO9bud13aGL/x3oC4xUp3nn6ydCdN+nrYi0buDj7AN2+R2D1qqapKrXAKjqdlWdhNP89F/AGyLSqoHXNBHIkoIJJzOBb6hqqf9GVa3BaeP+lYgkue38D3Cy3+F14B4RyRCRNsCDfs/dD/wf8P9EJFlEokSkp4hcGkQ803CasgbgNNlcBAwCEoCrgWychPSEiLQSkXgRGeM+dzbwQxEZJo5ebtwAa3ESS7SIjMPpIzmTJJx+hKMi0hb4eZ3P9x7wB7dDOlZEvh7gNbKBY24HdoL73oNEZDiAiEwWkTRV9QEnahO+II6RiTCWFEzYUNUvVXVlPbt/AJQCO4F/Aq8Ac919L+K04a8DVnN6TWMqEAdsBo4Ab+C0xddLROKBm4DfqWq+32MXTlPXNDdZXYfTgb0XyAFudj/LH4FfuXEeA/6E03kMcK/7vKPAre6+M3kKJxEdwumUf7/O/ik4NamtwEHgvrov4MZ6LU5i2+W+1myczntwmqY2iUgJTqfzRFUtayAuE4FE1SbZMcYY47CagjHGmFqWFIwxxtSypGCMMaaWJQVjjDG1mtzNa6mpqZqZmel1GMYY06SsWrXqkKqmNVSuySWFzMxMVq6s76pFY4wxgYjInoZLWfORMcYYP5YUjDHG1LKkYIwxplaT61MIpKqqipycHMrLyxsuHCHi4+PJyMggNjbW61CMMREkIpJCTk4OSUlJZGZm4jc8csRSVQoLC8nJyaF79+5eh2OMiSAR0XxUXl5Ou3btmkVCABAR2rVr16xqRsaYxhERSQFoNgnhhOb2eY0xjSMimo+MMSZSHSwuZ0NuERtzixnbvz2D0lMaftJXYEnhPCgsLGTs2LEA5OfnEx0dTVqac+NgdnY2cXFxDb7GbbfdxoMPPkjfvn1DGqsxJjypKvnF5WzIKWJjXjEbc4vYkFtEwbEKAESgbWKcJYWmoF27dqxduxaAX/ziFyQmJvLDH/7wlDInJsWOigrcYjdv3ryQx2mMCQ+qSs6RMjblFdXWAjbmFlFYWglAlECv9ol8rVcqg9JTGJSewoDOySS2CP0p25JCCO3YsYPrr7+eIUOGsGbNGj788EMeffRRVq9eTVlZGTfffDOPPPIIAJdccgnPPvssgwYNIjU1lTvvvJP33nuPli1b8uc//5n27dt7/GmMMedCVdl7+PgpJ/+NeUUcPV4FQHSU0Lt9It/o1742AfTvlETLOG9OzxGXFB59ZxOb84rP62sO6JzMz69rcJ73gLZu3crChQvJysoC4IknnqBt27ZUV1dz+eWXM2HCBAYMGHDKc4qKirj00kt54okneOCBB5g7dy4PPvhgoJc3xoQRn0/ZXVjqJgA3CeQVcay8GoDYaKFPhyTGDezIwPQUBqen0K9jEvGx0R5HflLEJYVw07Nnz9qEAPDqq68yZ84cqqurycvLY/PmzaclhYSEBK6++moAhg0bxmeffdaoMRtjGlbjU3YWlLAxr4gNOc7Jf3NeMSUVTgKIi4mif8ckrruwM4PTUxjUOYU+HRNpERM+CSCQiEsK5/qLPlRatWpVu7x9+3aefvppsrOzad26NZMnTw54r4F/x3R0dDTV1dWNEqsxJrDqGh87CkrYkFPEprxiNuQ6CaCsqgaA+Ngo+ndK5sah6Qzq7DQB9e6QSGx007vqP+KSQjgrLi4mKSmJ5ORk9u/fzwcffMC4ceO8DssY46ey2scXB46d0gm8ZX8xFdU+AFrGRTOgUzI3D+/CILcJqGdaK2KaYAIIxJJCIxo6dCgDBgygX79+dOvWjTFjxngdkjHNWkV1Ddvyj7Ex1/n1vymviK37j1FZ4ySAxBYxDOyczORR3ZwmoPRkuqcmEh0VuTePiqp6HcNZycrK0rqT7GzZsoX+/ft7FJF3muvnNuZclFfVsGV/cW0H8IbcIr44cIxqn3MOTI6Pqf3lf6ITuFvblkRFSAIQkVWqmtVQOaspGGMizvHKajbX3gBWzKa8IrYfLKHGTQBtWsYyKD2F7/XtUdsJ3KVtgg0fgyUFY0wTV1JRzabcU+8C3llQgnv+J9W9C/jKAR0Y2DmFwRkpdE6JtwRQD0sKxpgmo6isik15fvcA5Baxq7CUE63gHZJbMKhzCt8a3Km2KahDcgtLAGfBkoIxJiwdKa1kY96pdwHvKTxeu79zSjyD0lO4YUi62w+QTPukeA8jjgyWFIwxnissqTjlLuANuUXkHi2r3d+lbQKDOqdwU5ZzGeigzsm0S2zhYcSRy5KCMabR5R0t4+01uazdd5SNuUXsLzp5E2dmu5Zc1LU1U0Y7l4EO7JxM65YNjzRszo+QJgURGQc8DUQDs1X1iTr7uwFzgTTgMDBZVXNCGVMoXH755Tz44INcddVVtdueeuoptm3bxnPPPRfwOYmJiZSUlDRWiMZ4TlVZtecI85bu5v1N+fhU6Z7aihHd29beBTygczIpCTbvuJdClhREJBr4PXAlkAOsEJElqrrZr9h/AwtVdYGIfAP4NTAlVDGFyqRJk1i8ePEpSWHx4sX85je/8TAqY8JDZbWPd9fnMW/pbjbkFpEcH8PMS7ozZVQ3urRt6XV4po5Q1hRGADtUdSeAiCwGxgP+SWEA8IC7/AnwpxDGEzITJkzg4YcfprKykri4OHbv3k1eXh5Dhgxh7NixHDlyhKqqKh5//HHGjx/vdbjGNIqCYxW8snwvi5bvoeBYBT3TWvHLGwbxnaHpng0LbRoWyn+ZdGCf33oOMLJOmXXAjThNTN8GkkSknaoWnvO7vvcg5G8456cH1HEwXP1Evbvbtm3LiBEjeO+99xg/fjyLFy/mpptuIiEhgbfffpvk5GQOHTrEqFGjuP766+3yOBPRNuYWMW/pbt5Zl0dljY9L+6Rx24RMvt47LWLuDo5kXqfrHwLPish04FMgF6ipW0hEZgGzALp27dqY8QXtRBPSiaQwZ84cVJWf/vSnfPrpp0RFRZGbm8uBAwfo2LGj1+Eac15V1/j4cPMB5i3dTfbuw7SMi+bm4V2YdnEmvdoneh2eOQuhTAq5QBe/9Qx3Wy1VzcOpKSAiicB3VPVo3RdS1ReAF8AZ++iM73qGX/ShNH78eO6//35Wr17N8ePHGTZsGPPnz6egoIBVq1YRGxtLZmZmwKGyjWmqio5XsXjFXhb+aw+5R8vIaJPAf1zTn5uGd7EO4yYqlElhBdBbRLrjJIOJwC3+BUQkFTisqj7gIZwrkZqkxMRELr/8cmbMmMGkSZMAZwa19u3bExsbyyeffMKePXs8jtKY82PHwWPMW7qbt1bnUlZVw8jubfnZtQO4ckCHiB5BtDkIWVJQ1WoRuRv4AOeS1LmquklEHgNWquoS4DLg1yKiOM1H/xaqeBrDpEmT+Pa3v83ixYsBuPXWW7nuuusYPHgwWVlZ9OvXz+MIjTl3Pp/yj+0FzFu6m0+/KCAuJorxF3Zm+phMBnZO8To8c57Y0NlNWHP93KZxlVZU8+bqHOYv283OglLSklowdVQ3Jo3sSqrdVdxk2NDZxpivZN/h4yxYtpvXVu7jWHk1F2ak8NTNF3HN4E7ExUTGLGPmdJYUjDG1VJXluw4z95+7+NuWA4gIVw/qyG1jujO0a2u7nLoZiJikoKrN6j9sU2v2M+GtvKqGJeucu4637C+mdctY7ry0J1NGd6NTSoLX4ZlGFBFJIT4+nsLCQtq1a9csEoOqUlhYSHy8DRNsvpoDxeUs+nwPryzfS2FpJX06JPLrGwdzw0XpJMRFex2e8UBEJIWMjAxycnIoKCjwOpRGEx8fT0ZGhtdhmCZq7b6jzFu6i7+s30+NKmP7tee2Md25uGfz+GFl6hcRSSE2Npbu3bt7HYYxYa2qxsf7G/OZu3QXa/YeJbFFDFNGd2Pa6EwyU1t5HZ4JExGRFIwx9TtSWskr2Xt56V97yC8up1u7lvz8ugFMGJZBUrzddWxOZUnBmAi1Lf8Y85bu4u01uVRU+7ikVyq/+vYgLu/b3gamM/WypGBMBKnxKR9vPci8pbtY9mUhLWKiuHFoOtMv7k7fjkleh2eaAEsKxkSAY+VVvL4yhwXLdrP38HE6pcTz43F9mTS8K21a2VSWJniWFIxpwnYfKmX+st38ceU+SitrGNatDT8e15erBnYkNtruOjZnz5KCMU2MqrJ0RyHzlu7i420HiYkSvjW4E7eN6c6FXVp7HZ5p4iwpGNNElFXW8PaaXOYv28UXB0po1yqOH1zei8mjutE+2W5kNOeHJQVjwlze0TJe+nwPr2bv5ejxKgZ0Sua3Ey7gugs7Ex9rdx2b88uSgjFhSFVZvfcIc5fu5v2N+agq3xzQkdvGZDKie1u769iEjCUFY8JIZbWPv2xwBqZbn1NEUnwMMy/pzpRR3ejStqXX4ZlmwJKCMWHgUEkFL3++l0XL91BwrIIeaa345fiB3Dg0g1Yt7GtqGo/9bzPGQxtzi5i3dDfvrMujssbHpX3SuG1CJl/vnWZ3HRtPWFIwppFV1/j4cPMB5i3dTfbuwyTERnPz8C5MuziTXu0TvQ7PNHOWFIxpJEXHq3ht5V4WLNtD7tEy0lsn8B/X9Oem4V1ISbCB6Ux4sKRgTIjtOFjC/GW7eHNVLmVVNYzo3pafXdufK/p3IMbuOjZhxpKCMSHg8yn/2F7AvKW7+fSLAuKio7j+os7cNiaTgZ1TvA7PmHpZUjDmPCqtqOat1TnMW7abnQWlpCW14IEr+3DLyK6kJrbwOjxjGmRJwZjzoKSimmc+2s6r2Xs5Vl7NBRkpPHXzRVwzuBNxMdZEZJoOSwrGfEWHSiq4bd4KNuUVcfXgTswYk8nQrm3srmPTJFlSMOYr2FNYytS52RwoLmf2tCy+0a+D1yEZ85VYUjDmHG3MLWL6vGyqfcor3xvF0K5tvA7JmK/MkoIx52DpjkPc8dIqUhJiWTxjOL3a21SXJjJYUjDmLL2zLo8HXl9Lj9REFswYQccUm8vARA5LCsachflLd/Hou5sZ3q0tL07NIqWl3YlsIoslBWOCoKr89oNt/OHvX/LNAR14ZtIQm+DGRCRLCsY0oLrGx0NvbeCPq3KYNKIrj98wiGgbwdREKEsKxpxBWWUN//bKaj7eepB7x/bmvit62/0HJqJZUjCmHkdKK5mxYAXr9h3l8RsGMXlUN69DMibkQnr/vYiME5FtIrJDRB4MsL+riHwiImtEZL2IXBPKeIwJVu7RMiY8v4xNecX84dahlhBMsxGymoKIRAO/B64EcoAVIrJEVTf7FXsYeF1VnxORAcBfgcxQxWRMMLblH2Pa3GxKK6t5acYIRvZo53VIxjSaUNYURgA7VHWnqlYCi4HxdcookOwupwB5IYzHmAZl7zrMd59fhk+VP9452hKCaXZC2aeQDuzzW88BRtYp8wvg/0TkB0Ar4IpALyQis4BZAF27dj3vgRoD8MGmfO55dQ3pbRJYOGMEGW1aeh2SMY3O6zF9JwHzVTUDuAZ4SUROi0lVX1DVLFXNSktLa/QgTeR7Zfle7lq0iv6dknnjzostIZhmK5Q1hVygi996hrvN30xgHICq/ktE4oFU4GAI4zKmlqryzEc7ePJvX3BZ3zT+cOtQWsbZRXmm+QplTWEF0FtEuotIHDARWFKnzF5gLICI9AfigYIQxmRMrRqf8vCfNvLk377gO0MzeHFqliUE0+yF7BugqtUicjfwARANzFXVTSLyGLBSVZcA/w68KCL343Q6T1dVDVVMxpxQXlXDfYvX8v6mfO68tCc/GdfXbkozhhDfvKaqf8W5zNR/2yN+y5uBMaGMwZi6isqqmLVwJct3HeZn1w5g5iXdvQ7JmLBhdWXTrBwoLmfa3Gy+LCjh6YkXMf6idK9DMiasWFIwzcaXBSVMnZPN0eOVzJ0+nK/1tivZjKnLkoJpFtbsPcKM+SuIjhIWzxrN4IwUr0MyJixZUjAR75NtB/n+otWkJbVg4YwRZKa28jokY8KWJQUT0d5clcOP31xPv45JzL9tBGlJLbwOyZiwZknBRCRV5YVPd/Lr97Yyplc7np88jKR4mzrTmIZYUjARx+dTfvXXLcz55y6uvaAT/++mC2kRY1NnGhMMSwomolRW+/jhH9exZF0e0y/O5JFrBxBlU2caEzRLCiZilFRUc9eiVXy2/RA/HteXuy7taXcpG3OWLCmYiHCopILb5q1g8/5ifjvhAr6b1aXhJxljTmNJwTR5ewuPM3XucvKLy3lx6jC+0a+D1yEZ02RZUjBN2sbcIqbPW0G1z8fLt49iWLc2XodkTJPW4NDZIvIDEbFvmgk7y3YcYuILnxMXLbxx52hLCMacB8HMp9ABWCEir4vIOLGeOxMG3lmXx7R52aS3TuCt74+hV/skr0MyJiI0mBRU9WGgNzAHmA5sF5H/FJGeIY7NmIDmL93FPYvXMKRLG16/YzQdU+K9DsmYiBHUzGvuxDf57qMaaAO8ISK/CWFsxpxCVfnN+1v5xTububJ/BxbOHEFKS7tL2ZjzqcGOZhG5F5gKHAJmAz9S1SoRiQK2Az8ObYjGQHWNj4fe2sAfV+UwaURXfjl+IDHRoZxN1pjmKZirj9oCN6rqHv+NquoTkWtDE5YxJ5VV1nD3K6v5aOtB7h3bm/uu6G03pRkTIsEkhfeAwydWRCQZ6K+qy1V1S8giMwY4UlrJzAUrWLPvKI/fMIjJo7p5HZIxES2Y+vdzQInfeom7zZiQyj1axoTnl7Exr5jnbh1qCcGYRhBMTUHcjmagttnIbnozIbUt/xjT5mZTWlHNwhkjGNWjndchGdMsBFNT2Cki94hIrPu4F9gZ6sBM85W96zDffX4ZPlVev3O0JQRjGlEwSeFO4GIgF8gBRgKzQhmUab7+b1M+U+YsJzWxBW/edTH9OyV7HZIxzUqDzUCqehCY2AixmGbu1ey9/MfbGxic0Zp504fTtlWc1yEZ0+wEc59CPDATGAjU3jqqqjNCGJdpRlSVZz7awZN/+4LL+qbxh1uH0jLOuq2M8UIwzUcvAR2Bq4B/ABnAsVAGZZqPGp/ysz9v5Mm/fcGNQ9N5cWqWJQRjPBTMt6+Xqn5XRMar6gIReQX4LNSBmchXXlXDfYvX8v6mfO64tAcPjutnN6UZ47FgkkKV+/eoiAzCGf+ofehCMs1BUVkVsxauZPmuw/zs2gHMvKS71yEZYwguKbzgzqfwMLAESAR+FtKoTEQ7UFzOtLnZfFlQwtMTL2L8Releh2SMcZ0xKbiD3hWr6hHgU6BHo0RlItaXBSVMnZPN0eOVzJ0+nK/1TvM6JGOMnzN2NKuqDxsF1Zwna/YeYcJzyyivqmHxrNGWEIwJQ8FcffQ3EfmhiHQRkbYnHiGPzESUT7Yd5JYXl5MUH8ubd13M4IwUr0MyxgQQTJ/Cze7ff/PbplhTkgnSm6ty+Mmb6+nTIYn5M4bTPslmSjMmXAVzR/M5XxYiIuOAp4FoYLaqPlFn/5PA5e5qS6C9qrY+1/cz4UVVeeHTnfz6va1c3LMd/ztlGEnxNlOaMeEsmDuapwbarqoLG3heNPB74EqcMZNWiMgSVd3s9xr3+5X/ATAkyLhNmPP5lF/9dQtz/rmLb13Qif+56UJaxER7HZYxpgHBNB8N91uOB8YCq4EzJgVgBLBDVXcCiMhiYDywuZ7yk4CfBxGPCXOV1T5+9MY6/rw2j+kXZ/LItQOIirKb0oxpCoJpPvqB/7qItAYWB/Ha6cA+v/UTI6yeRkS6Ad2Bj+vZPwt3ZNauXbsG8dbGKyUV1dy1aBWfbT/Ej8f15a5Le9pdysY0Iecy83kpzgn8fJoIvKGqNYF2quoLqpqlqllpaXYZY7g6VFLBpBc+Z9mXhfxmwgV8/7JelhCMaWKC6VN4B+dqI3CSyADg9SBeOxfo4ree4W4LZCKnXt1kmpi9hceZOnc5+cXlvDBlGGP7d/A6JGPMOQimT+G//ZargT2qmhPE81YAvUWkO04ymAjcUreQiPQD2gD/CuI1TRjamFvE9HkrqPb5ePn2UQzr1sbrkIwx5yiYpLAX2K+q5QAikiAimaq6+0xPUtVqEbkb+ADnktS5qrpJRB4DVqrqErfoRGCx/zzQpulYtuMQs15aRXJ8DItnjaZX+ySvQzLGfAXS0LlYRFYCF6tqpbseByxV1eFnfGKIZGVl6cqVK714a1PHu+vzuP+1tXRPbcWCGSPolJLgdUjGmHqIyCpVzWqoXDA1hZgTCQFAVSvdxGCasflLd/Hou5vJ6taG2VOHk9LSbkozJhIEc/VRgYhcf2JFRMYDh0IXkglnqspv3t/KL97ZzBX9O/DSzJGWEIyJIMHUFO4EXhaRZ931HCDgXc4mslXX+HjorQ38cVUOk0Z05ZfjBxITfS5XNRtjwlUwN699CYwSkUR3vSTkUZmwU1ZZw92vrOajrQe5Z2xv7r+it92DYEwEavBnnoj8p4i0VtUSVS0RkTYi8nhjBGfCw5HSSm6d/TkfbzvI4zcM4oEr+1hCMCZCBVP3v1pVj55YcWdhuyZ0IZlwknu0jO/+77/YmFvMH24ZyuRR3bwOyRgTQsH0KUSLSAtVrQDnPgWgRWjDMuFgW/4xps3NprSimoUzRzCqRzuvQzLGhFgwSeFl4CMRmQcIMB1YEMqgjPfW7jvK1DnLiY+N5vU7R9O/U7LXIRljGkEwHc3/JSLrgCtwxkD6ALA2hAi2Nb+YaXOzad0yjpdvH0mXti29DskY00iCvZ7wAE5C+C7wDWBLyCIyntp9qJTJs7NJiI22hGBMM1RvTUFE+uBMfDMJ52a113CGxbi8vueYpi3vaBm3zl5Ojc/H4lmjLSEY0wydqfloK/AZcK2q7gAQkfvPUN40YYdKKpg8ZznFZVW88r1RNrCdMc3UmZqPbgT2A5+IyIsiMhano9lEmKKyKqbOySbvaBlzpg9ncEaK1yEZYzxSb1JQ1T+p6kSgH/AJcB/QXkSeE5FvNlaAJrSOV1Yzc/4Kth88xvOThzGie1uvQzLGeKjBjmZVLVXVV1T1OpzZ09YAPwl5ZCbkKqpruOOlVazee4SnJw7hsr7tvQ7JGOOxsxrNTFWPuPMljw1VQKZxVNf4uPfVtXy2/RBP3HgB1wzu5HVIxpgwYENcNkM+n/KTNzfw/qZ8fnbtAG4a3qXhJxljmgVLCs2MqvLYu5t5c3UO913Rm5mXdPc6JGNMGLGk0Mw8+eEXzF+2m5mXdOfesb29DscYE2YsKTQjL366k2c+3sHNWV14+Fv9bfhrY8xpLCk0E69m7+VXf93CtwZ34j9vHGwJwRgTkCWFZmDJujx++vYGLuubxpM3X0R0lCUEY0xglhQi3MdbD/DAa2sZ3q0tz906jLgY+yc3xtTPzhAR7POdhdy1aDX9OyUzZ3oWCXHRXodkjAlzlhQi1Lp9R5k5fwVd2rZkwYwRJMXHeh2SMaYJsKQQgbblH2PavGzaJsaxaOZI2raK8zokY0wTYUkhwuwpLGXKnOXERUfx8sxRdEyJ9zokY0wTYkkhguQXlXPr7OVU1fhYdPtIurazSXKMMWenwTmaTdNQ6E6Sc/R4Fa98byR9OtgkOcaYs2c1hQhQXF7FtHnZ7Dt8nNnTsrggo7XXIRljmihLCk1cWWUNM+evYOt+Z5KcUT3aeR2SMaYJs+ajJqyy2sedi1axcs8Rnpk4hMv72SQ5xpjWz04GAAAST0lEQVSvxmoKTVSNT7n/tbX844sCfv3twVx3YWevQzLGRICQJgURGSci20Rkh4g8WE+Zm0Rks4hsEpFXQhlPpPD5lIfeWs9fNuzn4W/1Z+KIrl6HZIyJECFrPhKRaOD3wJVADrBCRJao6ma/Mr2Bh4AxqnpERKz9owGqyuN/2cLrK3O4Z2xvbv9aD69DMsZEkFDWFEYAO1R1p6pWAouB8XXKfA/4vaoeAVDVgyGMJyI8/dF25i7dxfSLM7n/CpskxxhzfoUyKaQD+/zWc9xt/voAfURkqYh8LiLjAr2QiMwSkZUisrKgoCBE4Ya/Of/cxVN/286EYRk8cu0AmxPBGHPeed3RHAP0Bi4DJgEvishpF9mr6guqmqWqWWlpaY0cYnh4fcU+fvnuZq4e1JEnbhxMlM2JYIwJgVAmhVygi996hrvNXw6wRFWrVHUX8AVOkjB+/rJ+Pw++tZ6v90njqYkXERPtdS43xkSqUJ5dVgC9RaS7iMQBE4Eldcr8CaeWgIik4jQn7QxhTE3OJ9sOct9raxjWrQ3/O3kYLWJsTgRjTOiELCmoajVwN/ABsAV4XVU3ichjInK9W+wDoFBENgOfAD9S1cJQxdTULN9ZyJ0vraJPhyTmTB9uk+QYY0JOVNXrGM5KVlaWrly50uswQm5DThGTXvycDskteP2O0bRLbOF1SMaYJkxEVqlqVkPlrHE6DG0/cIypc5eTkhDLottHWkIwxjQaSwphZt/h40yes5yY6Chevn0knVISvA7JGNOMWFIIIweKnUlyyqt8LJo5kszUVl6HZIxpZiwphIkjpZVMmbOcwpIKFswYQd+ONkmOMabx2dDZYeCYO0nO7sLjLLhtBBd1sUlyjDHesJqCx8qrapi5YCWb84p57tahjO5pk+QYY7xjNQUPVVb7uGvRKlbsPszTE4cwtn8Hr0MyxjRzVlPwSI1PeeD1tXyyrYBf3TCY622SHGNMGLCk4AFV5T/e3sC76/fz0NX9uGWkTZJjjAkPlhQamaryn3/dwuIV+7j78l7ccWlPr0Oqn8/ndQTGmEZmfQqN7Hcf7+DFz5xJcv79m31C90aqUFkKFcVQXnTmR8AyxVBTCYntIbkzJKe7f+ssJ3WG2PjQfQ5jTKOypNCI5i3dxf98+AXfGRrEJDk+H1QeO/UkHfBkfrT+Mlpz5oBi4iE+xe/RGlp3O7keHQclB6A4Dw7vgt2fOa9bV8t2Z0gc7t84uxHPmKbAkkKo1FSf8gv87+t38PmnG3i0SyyT03cR9Y+/1H8yr3C30cBghXGJ0CL55Ek8sQOk9oH45Don+xS3XGu/bckQcw5jKlWUwLH9UJzrJIvav+5yzgo4HmCg2/gUN0Gk159A4pPPPh5jzHllo6TWp7qyzkk6QPPKmZphKksafo8WKaeepAOeyOtsi3dP7i2SITpMc3pVmZs48gInjuI8pwZSV1xS4CYq/+WENmDTkBpz1oIdJTVMzyohcGgH7F8bfHt6ddmZX0+iTz+ZJ/asc6JPYevRKJ76Zz5pae356Y2jSEhq65zQWyRBVITOjxCbAG17OI/6VFf6JY4ASePLj6EkH7ROZ3dMQuBkkZJxclvLdpY4jDlHzScpbPsLfPjIyfWoWEhofeqv8uT003+R1/fLPa5VgyeeFbsPM+Uvy+mR1p//mjWKhITYEH/IJiQmDtp0cx71qak+2acRqLlqzzI4lge+6lOfFx135tpGcjq0SovcpGzMV9B8ksKFt0Cfq/3a0+ND+mtyY24RM+atoHNKAgtnjiDFEsLZi46BlHTnwfDAZXw+KD0YuLZRnOf0cRTnOVdS+YuKgaROZ2iuSnf6aMK1ic6YEGk+/+MT05xHI9hxsISpc7NJdifJSbVJckInKgqSOjqP9GGBy6g6nd/1dY7nb4Bt75/eZChRkNjxzP0cSZ2cWo8xEaL5JIVGsu/wcSbPXk6UCItuH0nn1jZJjudEoFWq8+h0YeAyqlB2pP7O8YJtTj9HoAsIWrU/mSwS2zvrie3d9zyxnObUUq2vw4Q5Swrn0cHicibPWc7xympeu2M03W2SnKZDBFq2dR4dB9Vfrry4/s7xI3tOXpJbt4McnL6OVu2dGmurNL9lv8Rx4m9CW6cWZEwjs6Rwnhw9XsmUOdkUHKtg0e0j6d/JrrmPSPHJzqN9v/rL+GqcxFByEEoLnEfJQafvo6TA+XssH/I3Ost1O8rBubqttqaRduZk0jLV+j7MeWP/k86Dkopqps1bwa7CUuZPH87Qrm28Dsl4KSraOVkntm+47Ilmq9MSR8GpSeTQDudvdXmAF3FrOa3S/GobdWshfk1Z53LTomk2LCl8ReVVNXxvwUo25hbx3K1DubhXqtchmabEv9kqre+Zy6o6fRonaiB1ax+lBc5y3hrnb+WxwK/TIqXh2seJ5GLDkzQ7lhS+gqoaH3e/sprPdxXy5E0X8c2BHb0OyUQyEeemxxZJ0C6I0XWrys7chFVSAAe3QOmnTm0lkNiWwdU+rCM9YlhSOEc1PuXfX1/H37Yc5PEbBnHDkHSvQzLmVLEJDd8geEJ1JRw/dHotpPTQyeUjuyEn2zrSI5wlhXOgqvzszxtZsi6Pn4zrx+RRQXzpjAlnMXEnL6ttyImO9NrkEaA/pOSA25FeAL6q01+jtiM9zRmWpFWq02HeKtVdTzt1W3xrSyKNxJLCWVJVnnh/K68s38v3L+vJXZeF8SQ5xoSCf0d6h4FnLntaR3qdpqzSQqeGkrfWqZVUBBiaHZwkcqIzvW4SOSWhuH8T2tgwJufIksJZ+sPfv+R//7GTKaO68aOrGugYNKa5O5uOdHCbsdxEUeo+jtf5W3rIuQu99JAzn0jA941ymqhqE0U7vyQSILEktLXLel12FM7CgmW7+e0H2/j2kHQevX7gmSfJMcacvZg4SO7kPIJRUwXHD7sJo8BNHoV+SaTAqY0c3OJsKztC4HlKxKldnJZE0vyatOo0cUVH5nhmlhSC9OaqHH6+ZBNXDujAbydcQFSUJQRjPBcdC0kdnEcwaqqh7HCdGoibREoL3G2FcGg7lP7LnTCqnjln4ls30Bfil1hatmsyY2RZUgjC+xvz+fGb6xnTqx2/mzSEmGjr8DKmSYqOCf7GQnA61cuO1Gm+KqhTGzkEh3fCvuX1X5kFzv0hpzRj1e0LqVM78egmQ0sKDfhsewH3vLqGCzJSeGFKFvGx1nllTLMRFX3yBB4Mn8/p5zil+apuk9YhZ5ys3FXO9kDDnIAzE2HdJHLhJMi85Px9vgAsKZzBqj2HmbVwFT3SWjF/+ghatbDDZYw5g6iokx3r9Gm4vKqbRAr9mq8OnWzGOpFYinJh/zrI/FrIP4Kd5eqxKa+I6fNW0DElnpdmjiSlZWR2KhljPCRuB3dCG0jt5XU0AIS0cVxExonINhHZISIPBtg/XUQKRGSt+7g9lPEEa2dBCVPnZJPUIoZFt48kLckGEDPGNA8hqymISDTwe+BKIAdYISJLVHVznaKvqerdoYrjbOUeLWPy7OWIwKLbR5Juk+QYY5qRUNYURgA7VHWnqlYCi4HxIXy/r6zgWAWTZy/nWEU1C2eMpEdaotchGWNMowplUkgH9vmt57jb6vqOiKwXkTdEpEugFxKRWSKyUkRWFhQUhCJWio5XMWXOcvKLypl/23AGdLZJcowxzY/XF9y/A2Sq6gXAh8CCQIVU9QVVzVLVrLS0tPMeRGlFNdPnZ7OzoJQXp2YxrFvb8/4exhjTFIQyKeQC/r/8M9xttVS1UFUr3NXZwLAQxhNQeVUNs15ayfqcIn53yxAu6W2T5Bhjmq9QJoUVQG8R6S4iccBEYIl/ARHxH+DkemBLCOM5TVWNjx+8uoalOwr57YQLuMomyTHGNHMhu/pIVatF5G7gAyAamKuqm0TkMWClqi4B7hGR64Fq4DAwPVTx1OXzKT9+Yz0fbj7AY+MHcuPQjMZ6a2OMCVuiWs9gT2EqKytLV65c+ZVe48QkOYs+38uPrurLv10eHjeNGGNMqIjIKlXNaqic1x3NnvjtB9tY9Ple7ri0B9+3SXKMMaZWs0sKz/39S/7w9y+5dWRXHhzXz+ZEMMYYP80qKbz0+R7+6/2tjL+oM78cP8gSgjHG1NFsksKf1+byyJ83ckX/9vz3dy+0SXKMMSaAZpMUOibHc2X/Djx7y1BibZIcY4wJqNkMnT2yRztG9mjndRjGGBPW7CezMcaYWpYUjDHG1LKkYIwxppYlBWOMMbUsKRhjjKllScEYY0wtSwrGGGNqWVIwxhhTq8kNnS0iBcCec3x6KnDoPIZzvlhcZ8fiOnvhGpvFdXa+SlzdVLXB+YybXFL4KkRkZTDjiTc2i+vsWFxnL1xjs7jOTmPEZc1HxhhjallSMMYYU6u5JYUXvA6gHhbX2bG4zl64xmZxnZ2Qx9Ws+hSMMcacWXOrKRhjjDkDSwrGGGNqRWRSEJFxIrJNRHaIyIMB9rcQkdfc/ctFJDNM4pouIgUistZ93N5Icc0VkYMisrGe/SIiz7hxrxeRoWES12UiUuR3vB5phJi6iMgnIrJZRDaJyL0ByjT68QoyLi+OV7yIZIvIOjeuRwOUafTvY5BxefJ9dN87WkTWiMi7AfaF9nipakQ9gGjgS6AHEAesAwbUKfN94Hl3eSLwWpjENR141oNj9nVgKLCxnv3XAO8BAowClodJXJcB7zbyseoEDHWXk4AvAvw7NvrxCjIuL46XAInuciywHBhVp4wX38dg4vLk++i+9wPAK4H+vUJ9vCKxpjAC2KGqO1W1ElgMjK9TZjywwF1+AxgrIhIGcXlCVT8FDp+hyHhgoTo+B1qLSKcwiKvRqep+VV3tLh8DtgDpdYo1+vEKMq5G5x6DEnc11n3Uvbql0b+PQcblCRHJAL4FzK6nSEiPVyQmhXRgn996Dqd/OWrLqGo1UASEegLnYOIC+I7b5PCGiHQJcUzBCjZ2L4x2mwDeE5GBjfnGbrV9CM6vTH+eHq8zxAUeHC+3KWQtcBD4UFXrPV6N+H0MJi7w5vv4FPBjwFfP/pAer0hMCk3ZO0Cmql4AfMjJXwMmsNU447lcCPwO+FNjvbGIJAJvAvepanFjvW9DGojLk+OlqjWqehGQAYwQkUGN8b4NCSKuRv8+isi1wEFVXRXq96pPJCaFXMA/o2e42wKWEZEYIAUo9DouVS1U1Qp3dTYwLMQxBSuYY9roVLX4RBOAqv4ViBWR1FC/r4jE4px4X1bVtwIU8eR4NRSXV8fL7/2PAp8A4+rs8uL72GBcHn0fxwDXi8hunCbmb4jIojplQnq8IjEprAB6i0h3EYnD6YhZUqfMEmCauzwB+FjdXhsv46rT7nw9TrtwOFgCTHWvqhkFFKnqfq+DEpGOJ9pSRWQEzv/nkJ5M3PebA2xR1f+pp1ijH69g4vLoeKWJSGt3OQG4Ethap1ijfx+DicuL76OqPqSqGaqaiXOO+FhVJ9cpFtLjFXO+XihcqGq1iNwNfIBzxc9cVd0kIo8BK1V1Cc6X5yUR2YHTkTkxTOK6R0SuB6rduKaHOi4AEXkV58qUVBHJAX6O0/GGqj4P/BXnipodwHHgtjCJawJwl4hUA2XAxEZI7mOAKcAGtz0a4KdAV7+4vDhewcTlxfHqBCwQkWicJPS6qr7r9fcxyLg8+T4G0pjHy4a5MMYYUysSm4+MMcacI0sKxhhjallSMMYYU8uSgjHGmFqWFIwxxtSypGBMHSJS4zcy5loJMKLtV3jtTKln1FdjwkHE3adgzHlQ5g5/YEyzYzUFY4IkIrtF5DcissEdi7+Xuz1TRD52B077SES6uts7iMjb7gB060TkYvelokXkRXHG8f8/945aY8KCJQVjTpdQp/noZr99Rao6GHgWZzRLcAaXW+AOnPYy8Iy7/RngH+4AdEOBTe723sDvVXUgcBT4Tog/jzFBszuajalDREpUNTHA9t3AN1R1pzv4XL6qthORQ0AnVa1yt+9X1VQRKQAy/AZVOzGs9Yeq2ttd/wkQq6qPh/6TGdMwqykYc3a0nuWzUeG3XIP17ZkwYknBmLNzs9/ff7nLyzg5KNmtwGfu8kfAXVA7oUtKYwVpzLmyXyjGnC7Bb6RRgPdV9cRlqW1EZD3Or/1J7rYfAPNE5EdAASdHRb0XeEFEZuLUCO4CPB9y3JgzsT4FY4Lk9ilkqeohr2MxJlSs+cgYY0wtqykYY4ypZTUFY4wxtSwpGGOMqWVJwRhjTC1LCsYYY2pZUjDGGFPr/wO09On0jzgkRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracies')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With stop word removal,string.punctuation word removal, vocab size 60000, padding at 50, get 0.56 val accuracy \n",
    "# adding in TARGETTOKEN  increased accuracy by less than 1% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a6c6be46f980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the weights from the model with the best val accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightsFilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "model.load_weights(weightsFilePath)\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in y_pred])\n",
    "y_pred_text = onehot_encoder.inverse_transform(y_pred)\n",
    "y_val_text = onehot_encoder.inverse_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val_text, y_pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageFScore(cm):\n",
    "    (noClasses,_) = cm.shape\n",
    "    fsum = 0\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for i in range(noClasses):\n",
    "        correct = cm[i][i]\n",
    "        # if row or col total is zero set to 1 to avoid nans\n",
    "        rowTotal = max(sum(cm[i]),1)\n",
    "        colTotal = max(sum(cm[:,i]),1)\n",
    "        recall = correct / rowTotal\n",
    "        recalls.append(recall)\n",
    "        precision = correct / colTotal\n",
    "        precisions.append(precision)\n",
    "        \n",
    "        # Get denominator, if 0 set to 1 to avoid nans\n",
    "        denominator = precision + recall if precision + recall > 0 else 1\n",
    "        f1 = 2*precision*recall / denominator\n",
    "        fsum += f1\n",
    "    return fsum/noClasses, recalls, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   99   58   20    0]\n",
      " [   1 1024 1797  723    0]\n",
      " [   0  782 3414 1998    0]\n",
      " [   0  109  744 1479    0]\n",
      " [   0    6   12  113    0]]\n"
     ]
    }
   ],
   "source": [
    "# Rows are the actual, columns are the predicted. strongly negative,  negative, neutral, positve, strongly positive\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5069306930693069\n",
      "0.5666390041493776\n",
      "0.34133394876528966\n",
      "0.0\n",
      "Average fscore: 0.27412543884924134\n",
      "testAccuracy 0.4779869133209468\n",
      "Recalls for each class: [0.0, 0.28885754583921014, 0.5511785598966742, 0.6342195540308748, 0.0]\n",
      "Precisions for each class [0.0, 0.5069306930693069, 0.5666390041493776, 0.34133394876528966, 0.0]\n"
     ]
    }
   ],
   "source": [
    "valAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"valAccuracy {valAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\o'\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  \n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "testdf = pd.read_csv('./SemEval2017-task4-test/SemEval2017-task4-test.subtask-CE.english.txt', sep='\\t', header=None, keep_default_na=False)\n",
    "testdf.columns = ['id','topic','label','raw']\n",
    "\n",
    "testdf['text'] = testdf.apply(lambda row: preprocess(row['raw'], stop_words, row['topic']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['numSeq'] = testdf.apply(lambda row: convertTextToNumSeq(row['text'], word2idx, MAXIMUM_LENGTH),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    6194\n",
      "-1    3545\n",
      " 1    2332\n",
      "-2     177\n",
      " 2     131\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_test = testdf['numSeq']\n",
    "y_test = testdf['label']\n",
    "\n",
    "# Prelim analysis to indicate class imbalance\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Onehot encode the y data\n",
    "y_test = onehot_encoder.transform(np.array(y_test).reshape(len(y_test),1))\n",
    "x_test = np.array([x for y in x_test for x in y]).reshape(len(x_test),MAXIMUM_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and prepare data for confusion matrix\n",
    "y_testpred = model.predict(x_test)\n",
    "y_testpred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in y_testpred])\n",
    "y_testpred_text = onehot_encoder.inverse_transform(y_testpred)\n",
    "y_test_text = onehot_encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   99   58   20    0]\n",
      " [   1 1024 1797  723    0]\n",
      " [   0  782 3414 1998    0]\n",
      " [   0  109  744 1479    0]\n",
      " [   0    6   12  113    0]]\n",
      "Average fscore: 0.27412543884924134\n",
      "testAccuracy 0.4779869133209468\n",
      "Recalls for each class: [0.0, 0.28885754583921014, 0.5511785598966742, 0.6342195540308748, 0.0]\n",
      "Precisions for each class [0.0, 0.5069306930693069, 0.5666390041493776, 0.34133394876528966, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(y_test_text, y_testpred_text, labels=[-2,-1,0,1,2])\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"testAccuracy {testAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy was 47.8%, the drop compared to other subtasks is expected as there are more classes for this task, therefore the challenge is harder. Again the average fscore is low because of the poor precision and recall for the extreme cases of strongly negative and strongly positive. The test and trainign data have a large class imbalance which makes this problem harder. Future work could improve the model by addressing the class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Improving the classifier\n",
    "Steps: try training on all the trainign data\n",
    "- try training on the training and then doing an additional epoch on the validation set\n",
    "- try dealing with class imbalance\n",
    "- try calculating loss and accuracy based on the proximity of the classes to each other\n",
    "- try a different model usch as a bi-lstm or a cnn\n",
    "- try other embeddings\n",
    "- try getting other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different embeddings\n",
    "# Spacy embeddings. \n",
    "\n",
    "def spacyEmbeddings(wordList, nlp):\n",
    "    spacyList = []\n",
    "    for word in wordList:\n",
    "        spacyList.append(nlp(word).vector)\n",
    "    return spacyList\n",
    "\n",
    "# Pads the spacy vec to consistent length\n",
    "def padSpacy(spacyList, MAXIMUM_LENGTH):\n",
    "    vecSize = len(spacyList[0])\n",
    "    listLength = len(spacyList)\n",
    "    zeroVec = np.zeros(vecSize)\n",
    "    for i in range(MAXIMUM_LENGTH - listLength):\n",
    "        spacyList.append(zeroVec)\n",
    "    return np.array(spacyList)\n",
    "\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "traindf['spacy'] = traindf.apply(lambda row: spacyEmbeddings(row['text'], nlp),axis=1)\n",
    "\n",
    "# Get average vecotor for NBOW model. N.B. Doing this before the padding to avoid diluting signal\n",
    "traindf['spacyAvg'] = traindf.apply(lambda row: np.average(row['spacy'], axis=0),axis=1)\n",
    "\n",
    "# Trying to use spacy 300 dim vector as a sequence created objects too big for my laptop's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    3248\n",
      " 1    3230\n",
      "-1     850\n",
      " 2     255\n",
      "-2      75\n",
      "Name: label, dtype: int64\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "spacy_x_train, spacy_x_val, spacy_y_train, spacy_y_val = train_test_split(traindf.spacyAvg, traindf.label, stratify=traindf.label, random_state =2)\n",
    "labelDist = spacy_y_val.value_counts()\n",
    "print(labelDist)\n",
    "spacy_x_train = np.array([x for y in spacy_x_train for x in y]).reshape(len(spacy_x_train),300)\n",
    "spacy_x_val = np.array([x for y in spacy_x_val for x in y]).reshape(len(spacy_x_val),300)\n",
    "print(spacy_x_train[0].shape)\n",
    "\n",
    "labelCount = len(labelDist)\n",
    "#Y data is categorical therefore must be converted to a vector\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "spacy_y_train = onehot_encoder.fit_transform(np.array(spacy_y_train).reshape(len(spacy_y_train),1))\n",
    "spacy_y_val = onehot_encoder.transform(np.array(spacy_y_val).reshape(len(spacy_y_val),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 52,677\n",
      "Trainable params: 52,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This is a NBOW model as it takes the average pacy vector for the sentence and then uses that as an input\n",
    "spacyModel = Sequential()\n",
    "spacyModel.add(Dense(64, activation='relu',input_shape=(300,)))\n",
    "spacyModel.add(Dropout(0.2))\n",
    "spacyModel.add(Dense(128, activation='relu'))\n",
    "spacyModel.add(Dropout(0.2))\n",
    "spacyModel.add(Dense(128, activation='relu'))\n",
    "spacyModel.add(Dropout(0.2))\n",
    "spacyModel.add(Dense(64, activation='relu'))\n",
    "spacyModel.add(Dense(5, activation='softmax'))\n",
    "spacyModel.summary()\n",
    "\n",
    "spacyModel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "spacyWeightsFilePath=\"task3Spacy.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/30\n",
      "22974/22974 [==============================] - 5s 236us/step - loss: 1.0827 - acc: 0.5094 - val_loss: 0.9729 - val_acc: 0.5713\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.57130, saving model to task3Spacy.best.hdf5\n",
      "Epoch 2/30\n",
      "22974/22974 [==============================] - 1s 40us/step - loss: 0.9662 - acc: 0.5621 - val_loss: 0.9427 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.57130 to 0.58057, saving model to task3Spacy.best.hdf5\n",
      "Epoch 3/30\n",
      "22974/22974 [==============================] - 1s 28us/step - loss: 0.9444 - acc: 0.5772 - val_loss: 0.9459 - val_acc: 0.5832\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58057 to 0.58318, saving model to task3Spacy.best.hdf5\n",
      "Epoch 4/30\n",
      "22974/22974 [==============================] - 1s 30us/step - loss: 0.9281 - acc: 0.5846 - val_loss: 0.9346 - val_acc: 0.5793\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.58318\n",
      "Epoch 5/30\n",
      "22974/22974 [==============================] - 1s 29us/step - loss: 0.9116 - acc: 0.5888 - val_loss: 0.9345 - val_acc: 0.5812\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.58318\n",
      "Epoch 6/30\n",
      "22974/22974 [==============================] - 1s 29us/step - loss: 0.9037 - acc: 0.5915 - val_loss: 0.9302 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.58318\n",
      "Epoch 7/30\n",
      "22974/22974 [==============================] - 1s 29us/step - loss: 0.8940 - acc: 0.5971 - val_loss: 0.9325 - val_acc: 0.5746\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.58318\n",
      "Epoch 8/30\n",
      "22974/22974 [==============================] - 1s 35us/step - loss: 0.8850 - acc: 0.6001 - val_loss: 0.9288 - val_acc: 0.5759\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.58318\n",
      "Epoch 9/30\n",
      "22974/22974 [==============================] - 1s 37us/step - loss: 0.8717 - acc: 0.6072 - val_loss: 0.9316 - val_acc: 0.5789\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.58318\n",
      "Epoch 10/30\n",
      "22974/22974 [==============================] - 1s 36us/step - loss: 0.8669 - acc: 0.6124 - val_loss: 0.9337 - val_acc: 0.5759\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.58318\n",
      "Epoch 11/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.8537 - acc: 0.6188 - val_loss: 0.9447 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.58318\n",
      "Epoch 12/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.8451 - acc: 0.6214 - val_loss: 0.9589 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.58318\n",
      "Epoch 13/30\n",
      "22974/22974 [==============================] - 1s 40us/step - loss: 0.8276 - acc: 0.6305 - val_loss: 0.9555 - val_acc: 0.5723\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.58318\n",
      "Epoch 14/30\n",
      "22974/22974 [==============================] - 1s 29us/step - loss: 0.8198 - acc: 0.6325 - val_loss: 0.9564 - val_acc: 0.5716\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.58318\n",
      "Epoch 15/30\n",
      "22974/22974 [==============================] - 1s 30us/step - loss: 0.8159 - acc: 0.6358 - val_loss: 0.9666 - val_acc: 0.5675\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.58318\n",
      "Epoch 16/30\n",
      "22974/22974 [==============================] - 1s 36us/step - loss: 0.8037 - acc: 0.6435 - val_loss: 0.9667 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.58318\n",
      "Epoch 17/30\n",
      "22974/22974 [==============================] - 1s 36us/step - loss: 0.7950 - acc: 0.6455 - val_loss: 0.9786 - val_acc: 0.5593\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.58318\n",
      "Epoch 18/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.7890 - acc: 0.6479 - val_loss: 0.9757 - val_acc: 0.5708\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.58318\n",
      "Epoch 19/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7748 - acc: 0.6563 - val_loss: 0.9946 - val_acc: 0.5640\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.58318\n",
      "Epoch 20/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7698 - acc: 0.6589 - val_loss: 0.9893 - val_acc: 0.5632\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.58318\n",
      "Epoch 21/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.7622 - acc: 0.6628 - val_loss: 0.9990 - val_acc: 0.5581\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.58318\n",
      "Epoch 22/30\n",
      "22974/22974 [==============================] - 1s 39us/step - loss: 0.7578 - acc: 0.6682 - val_loss: 1.0043 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.58318\n",
      "Epoch 23/30\n",
      "22974/22974 [==============================] - 1s 39us/step - loss: 0.7476 - acc: 0.6698 - val_loss: 1.0005 - val_acc: 0.5662\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.58318\n",
      "Epoch 24/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7402 - acc: 0.6727 - val_loss: 1.0200 - val_acc: 0.5661\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.58318\n",
      "Epoch 25/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7366 - acc: 0.6778 - val_loss: 1.0306 - val_acc: 0.5599\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.58318\n",
      "Epoch 26/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7247 - acc: 0.6816 - val_loss: 1.0352 - val_acc: 0.5562\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.58318\n",
      "Epoch 27/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7229 - acc: 0.6846 - val_loss: 1.0204 - val_acc: 0.5640\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.58318\n",
      "Epoch 28/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7195 - acc: 0.6843 - val_loss: 1.0704 - val_acc: 0.5670\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.58318\n",
      "Epoch 29/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7095 - acc: 0.6905 - val_loss: 1.0404 - val_acc: 0.5516\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.58318\n",
      "Epoch 30/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7077 - acc: 0.6896 - val_loss: 1.0795 - val_acc: 0.5616\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.58318\n"
     ]
    }
   ],
   "source": [
    "spacyCheckpoint = ModelCheckpoint(spacyWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "spaceyHistory = spacyModel.fit(spacy_x_train, spacy_y_train, epochs=30,batch_size=128,validation_data=(spacy_x_val, spacy_y_val), callbacks=[spacyCheckpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_y_test = testdf['label']\n",
    "spacy_y_test = onehot_encoder.transform(np.array(spacy_y_test).reshape(len(spacy_y_test),1))\n",
    "\n",
    "testdf['spacy'] = testdf.apply(lambda row: spacyEmbeddings(row['text'], nlp),axis=1)\n",
    "\n",
    "# Get average vecotor for NBOW model. N.B. Doing this before the padding to avoid diluting signal\n",
    "testdf['spacyAvg'] = testdf.apply(lambda row: np.average(row['spacy'], axis=0),axis=1)\n",
    "\n",
    "spacy_x_test = testdf['spacyAvg']\n",
    "spacy_x_test = np.array([x for y in spacy_x_test for x in y]).reshape(len(spacy_x_test),300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "spacyModel.load_weights(spacyWeightsFilePath)\n",
    "\n",
    "# Get predictions and prepare data for confusion matrix\n",
    "spacy_y_testpred = spacyModel.predict(spacy_x_test)\n",
    "spacy_y_testpred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in spacy_y_testpred])\n",
    "spacy_y_testpred_text = onehot_encoder.inverse_transform(spacy_y_testpred)\n",
    "spacy_y_test_text = onehot_encoder.inverse_transform(spacy_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  128   45    4    0]\n",
      " [   0 1385 1980  180    0]\n",
      " [   0  684 4797  713    0]\n",
      " [   0   90 1097 1145    0]\n",
      " [   0    2   11  118    0]]\n",
      "Average fscore: 0.3327734799848806\n",
      "testAccuracy 0.5918894902657726\n",
      "Recalls for each class: [0.0, 0.3906911142454161, 0.7744591540200194, 0.49099485420240135, 0.0]\n",
      "Precisions for each class [0.0, 0.6050677151594582, 0.6049180327868853, 0.5300925925925926, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(spacy_y_test_text, spacy_y_testpred_text, labels=[-2,-1,0,1,2])\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"testAccuracy {testAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy NBOW method gets 58.3% validation accuracy and 59.2% test accuracy. This performance is similar to the LSTM at validation but outperforms the LSTM for test accuracy and test fscore. I believe the test performance improvement is largely due to the large number of dropout layers, which help defend against overfitting. This has made the Spacy NBOW model more robust than the LSTM. The Spacy NBOW model also takes much less time to train so might be more useful for real world applications that may require fast training/processing. Multiple architectures were experimented with before settling on this architecture, including much deeper networks, much wider networks, networks without dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Alternative model2: CNN\n",
    "In this I try a CNN model to see if that can improve the accuracy using learnt embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 48, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 6,159,045\n",
      "Trainable params: 159,045\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform transfer learning to transfer weights learnt from original model to accelerate learning\n",
    "model.load_weights(weightsFilePath)\n",
    "embeddingLayer = model.get_layer(index=0)\n",
    "embeddingLayer.trainable=False # Massively Reduce number of trainable weights which may help reduce overfitting\n",
    "cnnModel = Sequential()\n",
    "cnnModel.add(embeddingLayer)\n",
    "cnnModel.add(Conv1D(64, 3))\n",
    "cnnModel.add(Dropout(0.5))\n",
    "cnnModel.add(MaxPooling1D(3))\n",
    "cnnModel.add(Flatten())\n",
    "cnnModel.add(Dense(128, activation='relu'))\n",
    "cnnModel.add(Dropout(0.5))\n",
    "cnnModel.add(Dense(64, activation='relu'))\n",
    "cnnModel.add(Dense(5, activation='softmax'))\n",
    "cnnModel.summary()\n",
    "\n",
    "cnnModel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "cnnWeightsFilePath=\"task3CNN.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/20\n",
      "22974/22974 [==============================] - 5s 199us/step - loss: 0.7192 - acc: 0.7354 - val_loss: 1.1024 - val_acc: 0.5569\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55693, saving model to task3CNN.best.hdf5\n",
      "Epoch 2/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.4884 - acc: 0.8336 - val_loss: 1.1838 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.55693\n",
      "Epoch 3/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.4490 - acc: 0.8423 - val_loss: 1.1553 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55693\n",
      "Epoch 4/20\n",
      "22974/22974 [==============================] - 2s 90us/step - loss: 0.4242 - acc: 0.8491 - val_loss: 1.1402 - val_acc: 0.5477\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.55693\n",
      "Epoch 5/20\n",
      "22974/22974 [==============================] - 2s 92us/step - loss: 0.4076 - acc: 0.8581 - val_loss: 1.2775 - val_acc: 0.5424\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.55693\n",
      "Epoch 6/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.3916 - acc: 0.8609 - val_loss: 1.2816 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.55693\n",
      "Epoch 7/20\n",
      "22974/22974 [==============================] - 2s 88us/step - loss: 0.3788 - acc: 0.8658 - val_loss: 1.3029 - val_acc: 0.5475\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.55693\n",
      "Epoch 8/20\n",
      "22974/22974 [==============================] - 2s 94us/step - loss: 0.3733 - acc: 0.8684 - val_loss: 1.3202 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.55693\n",
      "Epoch 9/20\n",
      "22974/22974 [==============================] - 2s 91us/step - loss: 0.3643 - acc: 0.8710 - val_loss: 1.3849 - val_acc: 0.5543\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.55693\n",
      "Epoch 10/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.3586 - acc: 0.8742 - val_loss: 1.3823 - val_acc: 0.5424\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.55693\n",
      "Epoch 11/20\n",
      "22974/22974 [==============================] - 2s 92us/step - loss: 0.3511 - acc: 0.8759 - val_loss: 1.4137 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.55693\n",
      "Epoch 12/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.3438 - acc: 0.8776 - val_loss: 1.4122 - val_acc: 0.5491\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.55693\n",
      "Epoch 13/20\n",
      "22974/22974 [==============================] - 2s 87us/step - loss: 0.3421 - acc: 0.8769 - val_loss: 1.4743 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.55693\n",
      "Epoch 14/20\n",
      "22974/22974 [==============================] - 2s 91us/step - loss: 0.3337 - acc: 0.8807 - val_loss: 1.5137 - val_acc: 0.5414\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.55693\n",
      "Epoch 15/20\n",
      "22974/22974 [==============================] - 2s 88us/step - loss: 0.3316 - acc: 0.8803 - val_loss: 1.5033 - val_acc: 0.5477\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.55693\n",
      "Epoch 16/20\n",
      "22974/22974 [==============================] - 2s 86us/step - loss: 0.3272 - acc: 0.8826 - val_loss: 1.5383 - val_acc: 0.5236\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.55693\n",
      "Epoch 17/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.3209 - acc: 0.8852 - val_loss: 1.6372 - val_acc: 0.5313\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.55693\n",
      "Epoch 18/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.3149 - acc: 0.8862 - val_loss: 1.5701 - val_acc: 0.5346\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.55693\n",
      "Epoch 19/20\n",
      "22974/22974 [==============================] - 2s 89us/step - loss: 0.3169 - acc: 0.8870 - val_loss: 1.6158 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.55693\n",
      "Epoch 20/20\n",
      "22974/22974 [==============================] - 2s 92us/step - loss: 0.3090 - acc: 0.8887 - val_loss: 1.5771 - val_acc: 0.5358\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.55693\n"
     ]
    }
   ],
   "source": [
    "cnnCheckpoint = ModelCheckpoint(cnnWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "cnnHistory = cnnModel.fit(x_train, y_train, epochs=20,batch_size=128,validation_data=(x_val, y_val), callbacks=[cnnCheckpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main issue with the CNN has been overfitting, Adding dropout layers and maxpooling has not helped. Architectures that used multiple conv layers to reduce the trainable dimensions also did not help. There is a ~20% diff between training and val accuracies from the start and that only increases as training continues. Experiments with different architectures and with non-trainable vs trainable embeddings did not yield signifcant improvements. The best validation accuracy was 55.7%. This is not much lower than the LSTM and is much faster to train, the overfitting remains its main challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Model 3: Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 100)           6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 6,235,781\n",
      "Trainable params: 6,235,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstmModel = Sequential()\n",
    "bilstmModel.add(Embedding(VOCAB_SIZE, EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
    "bilstmModel.add(Bidirectional(LSTM(128)))\n",
    "bilstmModel.add(Dropout(0.5))\n",
    "bilstmModel.add(Dense(labelCount, activation='softmax'))\n",
    "bilstmModel.summary()\n",
    "\n",
    "bilstmModel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "bilstmWeightsFilePath=\"task3bilstm.best.hdf5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/5\n",
      "22974/22974 [==============================] - 37s 2ms/step - loss: 1.1134 - acc: 0.4895 - val_loss: 0.9889 - val_acc: 0.5652\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56516, saving model to task3bilstm.best.hdf5\n",
      "Epoch 2/5\n",
      "22974/22974 [==============================] - 32s 1ms/step - loss: 0.8131 - acc: 0.6768 - val_loss: 1.0127 - val_acc: 0.5602\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.56516\n",
      "Epoch 3/5\n",
      "22974/22974 [==============================] - 33s 1ms/step - loss: 0.4912 - acc: 0.8221 - val_loss: 1.1658 - val_acc: 0.5410\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.56516\n",
      "Epoch 4/5\n",
      "22974/22974 [==============================] - 32s 1ms/step - loss: 0.2957 - acc: 0.8960 - val_loss: 1.5823 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.56516\n",
      "Epoch 5/5\n",
      "22974/22974 [==============================] - 32s 1ms/step - loss: 0.1950 - acc: 0.9311 - val_loss: 1.8731 - val_acc: 0.5141\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.56516\n"
     ]
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "bilstmCheckpoint = ModelCheckpoint(bilstmWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "bilstmHistory = bilstmModel.fit(x_train,y_train,epochs=5,batch_size=128,validation_data=(x_val, y_val), callbacks=[bilstmCheckpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "bilstmModel.load_weights(bilstmWeightsFilePath)\n",
    "\n",
    "bilstm_y_test_pred = bilstmModel.predict(x_test)\n",
    "bilstm_y_test_pred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in bilstm_y_test_pred])\n",
    "bilstm_y_test_pred_text = onehot_encoder.inverse_transform(bilstm_y_test_pred)\n",
    "y_test_text = onehot_encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   38  115   24    0]\n",
      " [   0  423 2585  537    0]\n",
      " [   0  389 4494 1311    0]\n",
      " [   0   18 1092 1222    0]\n",
      " [   0    0   21  110    0]]\n",
      "Average fscore: 0.2505999266948812\n",
      "testAccuracy 0.495920510542047\n",
      "Recalls for each class: [0.0, 0.11932299012693935, 0.7255408459799806, 0.5240137221269296, 0.0]\n",
      "Precisions for each class [0.0, 0.4873271889400922, 0.5409895269050199, 0.381398252184769, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(y_test_text, bilstm_y_test_pred_text, labels=[-2,-1,0,1,2])\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"testAccuracy {testAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bidirectional LSTM makes only a small improvement on the LSTM. It has a validation accuracy of 56.5% but a test accuracy of 49.6%. The NBOW remains the strongest model despite it using only the average word vector, meaning it ignores the sequences of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eenlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
