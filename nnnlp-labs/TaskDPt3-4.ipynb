{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten, Conv2D, Bidirectional\n",
    "from keras.utils import plot_model, vis_utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import string\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Task 4 improving the models imports\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has been preprocessed by removing all the \" characters: sed -i 's/\"//g' *.txt\n",
    "# as this caused issues reading the data as a csv file. \n",
    "# Also had to remove a blank line from subtask A 2016 test data \n",
    "\n",
    "# Load the data\n",
    "fileGlob = glob.glob('./task3Data/*.txt')\n",
    "\n",
    "traindf = pd.concat([pd.read_csv(f, sep='\\t', header=None, keep_default_na=False) for f in fileGlob], ignore_index = True)\n",
    "traindf.columns = ['id','topic','label','raw']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\m'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\,'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\o'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\l'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def preprocess(tweet, stop_words, target):\n",
    "    # Handle utf8 unicode problems    \n",
    "    tweet = tweet.encode('utf8').decode('unicode_escape', 'ignore') \n",
    "    tweet = contractions.fix(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    if target.lower() in tweet:\n",
    "        tweet = tweet.replace(target,\"<TARGETTOKEN>\")\n",
    "        \n",
    "    tweetLine = word_tokenize(tweet)\n",
    "    # remove all tokens that are not alphabetic or stopwords, also lower the words\n",
    "    tweetLine = [word for word in tweetLine if word not in stop_words and word not in string.punctuation]\n",
    "    return tweetLine\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "traindf['text'] = traindf.apply(lambda row: preprocess(row['raw'], stop_words, row['topic']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "['TARGETTOKEN', 'systems', 'technical', 'university', 'come', 'visit', 'TARGETTOKEN', 'global', 'training', 'providers', 'TARGETTOKEN', 'stu15', 'TARGETTOKEN', 'training', 'starts', '01/sep', 'http', '//t.co/yntxyrlyod']\n",
      "39\n",
      "['work', 'friday', 'night', 'lt', 'lt', 'lt', 'lt', 'lt', 'TARGETTOKEN', 'bound', 'morning', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt', 'gt']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to ensure tweets are tweet length\n",
    "maxi = 0\n",
    "for text in traindf.text:\n",
    "    length = len(' '.join(text))\n",
    "    if length > maxi:\n",
    "        maxi = length\n",
    "        sanityCheck = text\n",
    "print(maxi)\n",
    "print(sanityCheck)\n",
    "\n",
    "maxi = 0\n",
    "for text in traindf.text:\n",
    "    length = len(text)\n",
    "    if length > maxi:\n",
    "        maxi = length\n",
    "        sanityCheck = text\n",
    "print(maxi)\n",
    "print(sanityCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't stop thinking about the fact that I'm going to be in the presence of Snoop Dogg on Sunday\n",
      "['stop', 'thinking', 'fact', 'going', 'presence', 'TARGETTOKEN', 'sunday']\n",
      "2\n",
      "snoop dogg\n",
      "['stop', 'thinking', 'fact', 'going', 'presence', 'TARGETTOKEN', 'sunday']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Further sanity checks to see what preprocessing is doing\n",
    "pd.options.display.max_colwidth = 10000\n",
    "sample = traindf.loc[traindf.id == 641648318754516992]\n",
    "\n",
    "print(sample.raw.item())\n",
    "print(sample.text.item())\n",
    "print(sample.label.item())\n",
    "print(sample.topic.item())\n",
    "sampleLine = preprocess(sample.raw.item(),stop_words, sample.topic.item())\n",
    "print(sampleLine)\n",
    "print(sample.topic.item() in sample.raw.item().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index-word relationship\n",
    "word2idx = {'<PAD>': 0, '<UNK>' : 1,'TARGETTOKEN' : 2 }\n",
    "idx2word ={}\n",
    "sents_as_ids = []\n",
    "for line in traindf.text:\n",
    "    sentId = []\n",
    "    for word in line:\n",
    "        if word in word2idx:\n",
    "            sentId.append(word2idx[word])\n",
    "            continue\n",
    "        count = len(word2idx)\n",
    "        word2idx[word] = count\n",
    "        idx2word[count] = word\n",
    "        sentId.append(count)\n",
    "    sents_as_ids.append(sentId)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTextToNumSeq(text, word2idx,MAXIMUM_LENGTH):\n",
    "    numSeq = []\n",
    "    for word in text:\n",
    "        if word in word2idx:\n",
    "            numSeq.append(word2idx[word])\n",
    "        else:\n",
    "            # If unseen put in unknown\n",
    "            numSeq.append(1) \n",
    "                \n",
    "    numSeq = pad_sequences([numSeq],MAXIMUM_LENGTH )\n",
    "    return numSeq\n",
    "\n",
    "MAXIMUM_LENGTH = 50 # Motivated because max sequence of words i had was 32\n",
    "\n",
    "traindf['numSeq'] = traindf.apply(lambda row: convertTextToNumSeq(row['text'], word2idx, MAXIMUM_LENGTH),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    3248\n",
      " 1    3230\n",
      "-1     850\n",
      " 2     255\n",
      "-2      75\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation, stratify will balance classes across the sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(traindf.numSeq, traindf.label, stratify=traindf.label, random_state =2)\n",
    "labelDist = y_val.value_counts()\n",
    "print(labelDist)\n",
    "x_train = np.array([x for y in x_train for x in y]).reshape(len(x_train),MAXIMUM_LENGTH)\n",
    "x_val = np.array([x for y in x_val for x in y]).reshape(len(x_val),MAXIMUM_LENGTH)\n",
    "\n",
    "labelCount = len(labelDist)\n",
    "#Y data is categorical therefore must be converted to a vector\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(np.array(y_train).reshape(len(y_train),1))\n",
    "y_val = onehot_encoder.transform(np.array(y_val).reshape(len(y_val),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 6,080,905\n",
      "Trainable params: 6,080,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model and a place to save it's weights to \n",
    "VOCAB_SIZE = 60000\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(labelCount, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "weightsFilePath=\"task3Weights.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/5\n",
      "22974/22974 [==============================] - 23s 1ms/step - loss: 1.1273 - acc: 0.4791 - val_loss: 1.0093 - val_acc: 0.5517\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55171, saving model to task3Weights.best.hdf5\n",
      "Epoch 2/5\n",
      "22974/22974 [==============================] - 19s 815us/step - loss: 0.8055 - acc: 0.6785 - val_loss: 1.0409 - val_acc: 0.5492\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.55171\n",
      "Epoch 3/5\n",
      "22974/22974 [==============================] - 19s 811us/step - loss: 0.4534 - acc: 0.8364 - val_loss: 1.2281 - val_acc: 0.5363\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55171\n",
      "Epoch 4/5\n",
      "22974/22974 [==============================] - 19s 814us/step - loss: 0.2519 - acc: 0.9106 - val_loss: 1.4999 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.55171\n",
      "Epoch 5/5\n",
      "22974/22974 [==============================] - 19s 815us/step - loss: 0.1601 - acc: 0.9451 - val_loss: 1.7650 - val_acc: 0.5153\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.55171\n"
     ]
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "checkpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(x_train,y_train,epochs=5,batch_size=128,validation_data=(x_val, y_val), callbacks=[checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the history of this model\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracies')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "model.load_weights(weightsFilePath)\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in y_pred])\n",
    "y_pred_text = onehot_encoder.inverse_transform(y_pred)\n",
    "y_val_text = onehot_encoder.inverse_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageFScore(cm):\n",
    "    (noClasses,_) = cm.shape\n",
    "    fsum = 0\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for i in range(noClasses):\n",
    "        correct = cm[i][i]\n",
    "        # if row or col total is zero set to 1 to avoid nans\n",
    "        rowTotal = max(sum(cm[i]),1)\n",
    "        colTotal = max(sum(cm[:,i]),1)\n",
    "        recall = correct / rowTotal\n",
    "        recalls.append(recall)\n",
    "        precision = correct / colTotal\n",
    "        precisions.append(precision)\n",
    "        \n",
    "        # Get denominator, if 0 set to 1 to avoid nans\n",
    "        denominator = precision + recall if precision + recall > 0 else 1\n",
    "        f1 = 2*precision*recall / denominator\n",
    "        fsum += f1\n",
    "    return fsum/noClasses, recalls, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matric\n",
    "cm = confusion_matrix(y_val_text, y_pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    2   64    9    0]\n",
      " [   0   15  750   85    0]\n",
      " [   0    9 2556  683    0]\n",
      " [   0    1 1575 1654    0]\n",
      " [   0    0   71  184    0]]\n"
     ]
    }
   ],
   "source": [
    "# Rows are the actual, columns are the predicted. strongly negative,  negative, neutral, positve, strongly positive\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fscore: 0.2437495946359803\n",
      "valAccuracy 0.5517106294071559\n",
      "Recalls for each class: [0.0, 0.01764705882352941, 0.7869458128078818, 0.5120743034055728, 0.0]\n",
      "Precisions for each class [0.0, 0.5555555555555556, 0.5095693779904307, 0.6325047801147228, 0.0]\n"
     ]
    }
   ],
   "source": [
    "valAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"valAccuracy {valAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\o'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "testdf = pd.read_csv('./SemEval2017-task4-test/SemEval2017-task4-test.subtask-CE.english.txt', sep='\\t', header=None, keep_default_na=False)\n",
    "testdf.columns = ['id','topic','label','raw']\n",
    "\n",
    "testdf['text'] = testdf.apply(lambda row: preprocess(row['raw'], stop_words, row['topic']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into sequence of numbers\n",
    "testdf['numSeq'] = testdf.apply(lambda row: convertTextToNumSeq(row['text'], word2idx, MAXIMUM_LENGTH),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    6194\n",
      "-1    3545\n",
      " 1    2332\n",
      "-2     177\n",
      " 2     131\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_test = testdf['numSeq']\n",
    "y_test = testdf['label']\n",
    "\n",
    "# Prelim analysis to indicate class imbalance\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Onehot encode the y data\n",
    "y_test = onehot_encoder.transform(np.array(y_test).reshape(len(y_test),1))\n",
    "x_test = np.array([x for y in x_test for x in y]).reshape(len(x_test),MAXIMUM_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and prepare data for confusion matrix\n",
    "y_testpred = model.predict(x_test)\n",
    "y_testpred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in y_testpred])\n",
    "y_testpred_text = onehot_encoder.inverse_transform(y_testpred)\n",
    "y_test_text = onehot_encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   15  156    6    0]\n",
      " [   0   90 3259  196    0]\n",
      " [   0   61 5537  596    0]\n",
      " [   0    1 1547  784    0]\n",
      " [   0    0   38   93    0]]\n",
      "Average fscore: 0.22033834022734497\n",
      "testAccuracy 0.517893206236368\n",
      "Recalls for each class: [0.0, 0.02538787023977433, 0.893929609299322, 0.3361921097770154, 0.0]\n",
      "Precisions for each class [0.0, 0.5389221556886228, 0.5254816361393186, 0.46805970149253734, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(y_test_text, y_testpred_text, labels=[-2,-1,0,1,2])\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"testAccuracy {testAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy was 51.8%, the drop compared to other subtasks is expected as there are more classes for this task, therefore the challenge is harder. Again the average fscore is low because of the poor precision and recall for the extreme cases of strongly negative and strongly positive. The test and training data have a large class imbalance which makes this problem harder. Future work could improve the model by addressing the class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Improving the classifier\n",
    "In this section I have tried different models and also different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different embeddings\n",
    "# Spacy embeddings. \n",
    "\n",
    "def spacyEmbeddings(wordList, nlp):\n",
    "    spacyList = []\n",
    "    for word in wordList:\n",
    "        spacyList.append(nlp(word).vector)\n",
    "    return spacyList\n",
    "\n",
    "# Pads the spacy vec to consistent length\n",
    "def padSpacy(spacyList, MAXIMUM_LENGTH):\n",
    "    vecSize = len(spacyList[0])\n",
    "    listLength = len(spacyList)\n",
    "    zeroVec = np.zeros(vecSize)\n",
    "    for i in range(MAXIMUM_LENGTH - listLength):\n",
    "        spacyList.append(zeroVec)\n",
    "    return np.array(spacyList)\n",
    "\n",
    "# Load in spacy embeddings and convert all lines to use them\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "traindf['spacy'] = traindf.apply(lambda row: spacyEmbeddings(row['text'], nlp),axis=1)\n",
    "\n",
    "# Get average vecotor for NBOW model. N.B. Doing this before the padding to avoid diluting signal\n",
    "traindf['spacyAvg'] = traindf.apply(lambda row: np.average(row['spacy'], axis=0),axis=1)\n",
    "\n",
    "# Trying to use spacy 300 dim vector as a sequence created objects too big for my laptop's memory, \n",
    "# therefore couldn't use them as individual embeddings and instead had to average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    3248\n",
      " 1    3230\n",
      "-1     850\n",
      " 2     255\n",
      "-2      75\n",
      "Name: label, dtype: int64\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Perform split of spacy data into test and val\n",
    "spacy_x_train, spacy_x_val, spacy_y_train, spacy_y_val = train_test_split(traindf.spacyAvg, traindf.label, stratify=traindf.label, random_state =2)\n",
    "labelDist = spacy_y_val.value_counts()\n",
    "print(labelDist)\n",
    "spacy_x_train = np.array([x for y in spacy_x_train for x in y]).reshape(len(spacy_x_train),300)\n",
    "spacy_x_val = np.array([x for y in spacy_x_val for x in y]).reshape(len(spacy_x_val),300)\n",
    "print(spacy_x_train[0].shape)\n",
    "\n",
    "labelCount = len(labelDist)\n",
    "#Y data is categorical therefore must be converted to a vector\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "spacy_y_train = onehot_encoder.fit_transform(np.array(spacy_y_train).reshape(len(spacy_y_train),1))\n",
    "spacy_y_val = onehot_encoder.transform(np.array(spacy_y_val).reshape(len(spacy_y_val),1))\n",
    "\n",
    "spacy_y_test = testdf['label']\n",
    "spacy_y_test = onehot_encoder.transform(np.array(spacy_y_test).reshape(len(spacy_y_test),1))\n",
    "testdf['spacy'] = testdf.apply(lambda row: spacyEmbeddings(row['text'], nlp),axis=1)\n",
    "\n",
    "# Get average vecotor for NBOW model. N.B. Doing this before the padding to avoid diluting signal\n",
    "testdf['spacyAvg'] = testdf.apply(lambda row: np.average(row['spacy'], axis=0),axis=1)\n",
    "\n",
    "spacy_x_test = testdf['spacyAvg']\n",
    "spacy_x_test = np.array([x for y in spacy_x_test for x in y]).reshape(len(spacy_x_test),300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 52,677\n",
      "Trainable params: 52,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This is a NBOW model as it takes the average spacy vector for the sentence and then uses that as an input\n",
    "spacyModel = Sequential()\n",
    "spacyModel.add(Dense(64, activation='relu',input_shape=(300,)))\n",
    "spacyModel.add(Dropout(0.2))\n",
    "spacyModel.add(Dense(128, activation='relu'))\n",
    "spacyModel.add(Dropout(0.2))\n",
    "spacyModel.add(Dense(128, activation='relu'))\n",
    "spacyModel.add(Dropout(0.2))\n",
    "spacyModel.add(Dense(64, activation='relu'))\n",
    "spacyModel.add(Dense(5, activation='softmax'))\n",
    "spacyModel.summary()\n",
    "\n",
    "spacyModel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "spacyWeightsFilePath=\"task3Spacy.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/30\n",
      "22974/22974 [==============================] - 1s 57us/step - loss: 1.0768 - acc: 0.5138 - val_loss: 0.9687 - val_acc: 0.5688\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56882, saving model to task3Spacy.best.hdf5\n",
      "Epoch 2/30\n",
      "22974/22974 [==============================] - 1s 31us/step - loss: 0.9697 - acc: 0.5638 - val_loss: 0.9446 - val_acc: 0.5831\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.56882 to 0.58305, saving model to task3Spacy.best.hdf5\n",
      "Epoch 3/30\n",
      "22974/22974 [==============================] - 1s 30us/step - loss: 0.9439 - acc: 0.5741 - val_loss: 0.9314 - val_acc: 0.5837\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58305 to 0.58370, saving model to task3Spacy.best.hdf5\n",
      "Epoch 4/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.9323 - acc: 0.5776 - val_loss: 0.9295 - val_acc: 0.5783\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.58370\n",
      "Epoch 5/30\n",
      "22974/22974 [==============================] - 1s 34us/step - loss: 0.9181 - acc: 0.5883 - val_loss: 0.9336 - val_acc: 0.5738\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.58370\n",
      "Epoch 6/30\n",
      "22974/22974 [==============================] - 1s 31us/step - loss: 0.9077 - acc: 0.5920 - val_loss: 0.9241 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.58370\n",
      "Epoch 7/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.8982 - acc: 0.5983 - val_loss: 0.9300 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.58370\n",
      "Epoch 8/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.8872 - acc: 0.6013 - val_loss: 0.9328 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.58370\n",
      "Epoch 9/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.8784 - acc: 0.6067 - val_loss: 0.9227 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.58370\n",
      "Epoch 10/30\n",
      "22974/22974 [==============================] - 1s 35us/step - loss: 0.8677 - acc: 0.6048 - val_loss: 0.9283 - val_acc: 0.5778\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.58370\n",
      "Epoch 11/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.8586 - acc: 0.6163 - val_loss: 0.9329 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.58370\n",
      "Epoch 12/30\n",
      "22974/22974 [==============================] - 1s 38us/step - loss: 0.8491 - acc: 0.6207 - val_loss: 0.9390 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.58370\n",
      "Epoch 13/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.8381 - acc: 0.6217 - val_loss: 0.9387 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.58370\n",
      "Epoch 14/30\n",
      "22974/22974 [==============================] - 1s 34us/step - loss: 0.8306 - acc: 0.6262 - val_loss: 0.9377 - val_acc: 0.5797\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.58370\n",
      "Epoch 15/30\n",
      "22974/22974 [==============================] - 1s 35us/step - loss: 0.8193 - acc: 0.6330 - val_loss: 0.9466 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.58370\n",
      "Epoch 16/30\n",
      "22974/22974 [==============================] - 1s 34us/step - loss: 0.8078 - acc: 0.6391 - val_loss: 0.9566 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.58370\n",
      "Epoch 17/30\n",
      "22974/22974 [==============================] - 1s 35us/step - loss: 0.8009 - acc: 0.6434 - val_loss: 0.9516 - val_acc: 0.5680\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.58370\n",
      "Epoch 18/30\n",
      "22974/22974 [==============================] - 1s 36us/step - loss: 0.7926 - acc: 0.6482 - val_loss: 0.9600 - val_acc: 0.5678\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.58370\n",
      "Epoch 19/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7867 - acc: 0.6497 - val_loss: 0.9696 - val_acc: 0.5717\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.58370\n",
      "Epoch 20/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7762 - acc: 0.6527 - val_loss: 0.9650 - val_acc: 0.5686\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.58370\n",
      "Epoch 21/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.7696 - acc: 0.6580 - val_loss: 0.9691 - val_acc: 0.5746\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.58370\n",
      "Epoch 22/30\n",
      "22974/22974 [==============================] - 1s 39us/step - loss: 0.7600 - acc: 0.6634 - val_loss: 0.9795 - val_acc: 0.5672\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.58370\n",
      "Epoch 23/30\n",
      "22974/22974 [==============================] - 1s 36us/step - loss: 0.7520 - acc: 0.6680 - val_loss: 0.9976 - val_acc: 0.5615\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.58370\n",
      "Epoch 24/30\n",
      "22974/22974 [==============================] - 1s 38us/step - loss: 0.7471 - acc: 0.6725 - val_loss: 0.9922 - val_acc: 0.5678\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.58370\n",
      "Epoch 25/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.7426 - acc: 0.6700 - val_loss: 0.9863 - val_acc: 0.5678\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.58370\n",
      "Epoch 26/30\n",
      "22974/22974 [==============================] - 1s 36us/step - loss: 0.7318 - acc: 0.6785 - val_loss: 1.0047 - val_acc: 0.5648\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.58370\n",
      "Epoch 27/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7268 - acc: 0.6804 - val_loss: 1.0184 - val_acc: 0.5672\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.58370\n",
      "Epoch 28/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.7191 - acc: 0.6838 - val_loss: 1.0149 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.58370\n",
      "Epoch 29/30\n",
      "22974/22974 [==============================] - 1s 32us/step - loss: 0.7141 - acc: 0.6895 - val_loss: 1.0218 - val_acc: 0.5644\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.58370\n",
      "Epoch 30/30\n",
      "22974/22974 [==============================] - 1s 33us/step - loss: 0.7114 - acc: 0.6898 - val_loss: 1.0430 - val_acc: 0.5628\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.58370\n"
     ]
    }
   ],
   "source": [
    "spacyCheckpoint = ModelCheckpoint(spacyWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "spaceyHistory = spacyModel.fit(spacy_x_train, spacy_y_train, epochs=30,batch_size=128,validation_data=(spacy_x_val, spacy_y_val), callbacks=[spacyCheckpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "spacyModel.load_weights(spacyWeightsFilePath)\n",
    "\n",
    "# Get predictions and prepare data for confusion matrix\n",
    "spacy_y_testpred = spacyModel.predict(spacy_x_test)\n",
    "spacy_y_testpred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in spacy_y_testpred])\n",
    "spacy_y_testpred_text = onehot_encoder.inverse_transform(spacy_y_testpred)\n",
    "spacy_y_test_text = onehot_encoder.inverse_transform(spacy_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  124   49    4    0]\n",
      " [   0 1409 1956  180    0]\n",
      " [   0  765 4788  641    0]\n",
      " [   0   78 1153 1101    0]\n",
      " [   0    2   12  117    0]]\n",
      "Average fscore: 0.3311480349766879\n",
      "testAccuracy 0.5895468131513046\n",
      "Recalls for each class: [0.0, 0.39746121297602255, 0.7730061349693251, 0.47212692967409947, 0.0]\n",
      "Precisions for each class [0.0, 0.5925147182506307, 0.6016587082181453, 0.5389133627019089, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(spacy_y_test_text, spacy_y_testpred_text, labels=[-2,-1,0,1,2])\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"testAccuracy {testAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy NBOW method gets 58.4% validation accuracy and 59.0% test accuracy. This performance is similar to the LSTM at validation but outperforms the LSTM for test accuracy and test fscore. I believe the test performance improvement is largely due to the large number of dropout layers, which help defend against overfitting. This has made the Spacy NBOW model more robust than the LSTM. The Spacy NBOW model also takes much less time to train so might be more useful for real world applications that may require fast training/processing. Multiple architectures were experimented with before settling on this architecture, including much deeper networks, much wider networks, networks without dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Alternative model2: CNN\n",
    "In this I try a CNN model to see if that can improve the accuracy using learnt embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 6,159,045\n",
      "Trainable params: 159,045\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Perform transfer learning to transfer weights learnt from original model to accelerate learning\n",
    "model.load_weights(weightsFilePath)\n",
    "embeddingLayer = model.get_layer(index=0)\n",
    "embeddingLayer.trainable=False # Massively Reduce number of trainable weights which may help reduce overfitting\n",
    "\n",
    "# Create CNN model\n",
    "cnnModel = Sequential()\n",
    "cnnModel.add(embeddingLayer)\n",
    "cnnModel.add(Conv1D(64, 3))\n",
    "cnnModel.add(Dropout(0.5))\n",
    "cnnModel.add(MaxPooling1D(3))\n",
    "cnnModel.add(Flatten()) # Required to get vector rather than matrix/tensor out\n",
    "cnnModel.add(Dense(128, activation='relu'))\n",
    "cnnModel.add(Dropout(0.5))\n",
    "cnnModel.add(Dense(64, activation='relu'))\n",
    "cnnModel.add(Dense(5, activation='softmax'))\n",
    "cnnModel.summary()\n",
    "\n",
    "cnnModel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "cnnWeightsFilePath=\"task3CNN.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/20\n",
      "22974/22974 [==============================] - 4s 167us/step - loss: 0.9418 - acc: 0.6108 - val_loss: 1.0104 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55602, saving model to task3CNN.best.hdf5\n",
      "Epoch 2/20\n",
      "22974/22974 [==============================] - 2s 83us/step - loss: 0.7889 - acc: 0.6961 - val_loss: 1.0380 - val_acc: 0.5521\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.55602\n",
      "Epoch 3/20\n",
      "22974/22974 [==============================] - 2s 81us/step - loss: 0.7384 - acc: 0.7191 - val_loss: 1.0626 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55602\n",
      "Epoch 4/20\n",
      "22974/22974 [==============================] - 2s 83us/step - loss: 0.7022 - acc: 0.7364 - val_loss: 1.0740 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.55602\n",
      "Epoch 5/20\n",
      "22974/22974 [==============================] - 2s 80us/step - loss: 0.6807 - acc: 0.7442 - val_loss: 1.1011 - val_acc: 0.5380\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.55602\n",
      "Epoch 6/20\n",
      "22974/22974 [==============================] - 2s 80us/step - loss: 0.6664 - acc: 0.7470 - val_loss: 1.1556 - val_acc: 0.5398\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.55602\n",
      "Epoch 7/20\n",
      "22974/22974 [==============================] - 2s 82us/step - loss: 0.6547 - acc: 0.7526 - val_loss: 1.1568 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.55602\n",
      "Epoch 8/20\n",
      "22974/22974 [==============================] - 2s 80us/step - loss: 0.6445 - acc: 0.7580 - val_loss: 1.2100 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.55602\n",
      "Epoch 9/20\n",
      "22974/22974 [==============================] - 2s 84us/step - loss: 0.6264 - acc: 0.7633 - val_loss: 1.1987 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.55602\n",
      "Epoch 10/20\n",
      "22974/22974 [==============================] - 2s 86us/step - loss: 0.6253 - acc: 0.7627 - val_loss: 1.2372 - val_acc: 0.5393\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.55602\n",
      "Epoch 11/20\n",
      "22974/22974 [==============================] - 2s 84us/step - loss: 0.6156 - acc: 0.7675 - val_loss: 1.2160 - val_acc: 0.5453\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.55602\n",
      "Epoch 12/20\n",
      "22974/22974 [==============================] - 2s 81us/step - loss: 0.6065 - acc: 0.7707 - val_loss: 1.2163 - val_acc: 0.5368\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.55602\n",
      "Epoch 13/20\n",
      "22974/22974 [==============================] - 2s 82us/step - loss: 0.5981 - acc: 0.7731 - val_loss: 1.2646 - val_acc: 0.5351\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.55602\n",
      "Epoch 14/20\n",
      "22974/22974 [==============================] - 2s 81us/step - loss: 0.5956 - acc: 0.7760 - val_loss: 1.2385 - val_acc: 0.5383\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.55602\n",
      "Epoch 15/20\n",
      "22974/22974 [==============================] - 2s 84us/step - loss: 0.5897 - acc: 0.7768 - val_loss: 1.2506 - val_acc: 0.5380\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.55602\n",
      "Epoch 16/20\n",
      "22974/22974 [==============================] - 2s 79us/step - loss: 0.5852 - acc: 0.7760 - val_loss: 1.2477 - val_acc: 0.5338\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.55602\n",
      "Epoch 17/20\n",
      "22974/22974 [==============================] - 2s 87us/step - loss: 0.5837 - acc: 0.7753 - val_loss: 1.2989 - val_acc: 0.5306\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.55602\n",
      "Epoch 18/20\n",
      "22974/22974 [==============================] - 2s 81us/step - loss: 0.5794 - acc: 0.7790 - val_loss: 1.2893 - val_acc: 0.5309\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.55602\n",
      "Epoch 19/20\n",
      "22974/22974 [==============================] - 2s 79us/step - loss: 0.5722 - acc: 0.7830 - val_loss: 1.2986 - val_acc: 0.5319\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.55602\n",
      "Epoch 20/20\n",
      "22974/22974 [==============================] - 2s 80us/step - loss: 0.5699 - acc: 0.7810 - val_loss: 1.2719 - val_acc: 0.5303\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.55602\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint and train model \n",
    "cnnCheckpoint = ModelCheckpoint(cnnWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "cnnHistory = cnnModel.fit(x_train, y_train, epochs=20,batch_size=128,validation_data=(x_val, y_val), callbacks=[cnnCheckpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main issue with the CNN has been overfitting, Adding dropout layers and maxpooling has not helped. Architectures that used multiple conv layers to reduce the trainable dimensions also did not help. There is a ~20% diff between training and val accuracies from the start and that only increases as training continues. Experiments with different architectures and with non-trainable vs trainable embeddings did not yield signifcant improvements. The best validation accuracy was 55.6%. This is similar to the LSTM and is much faster to train, the overfitting remains its main challenge. As the val accuracy for this model was lower than NBOW I did not apply it to the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Model 3: Bidirectional LSTM\n",
    "For my third attempt at improving the performance i tried changing the model to the bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 6,235,781\n",
      "Trainable params: 6,235,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create bilstm\n",
    "bilstmModel = Sequential()\n",
    "bilstmModel.add(Embedding(VOCAB_SIZE, EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
    "bilstmModel.add(Bidirectional(LSTM(128)))\n",
    "bilstmModel.add(Dropout(0.5))\n",
    "bilstmModel.add(Dense(labelCount, activation='softmax'))\n",
    "bilstmModel.summary()\n",
    "\n",
    "bilstmModel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "bilstmWeightsFilePath=\"task3bilstm.best.hdf5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22974 samples, validate on 7658 samples\n",
      "Epoch 1/5\n",
      "22974/22974 [==============================] - 35s 2ms/step - loss: 1.1227 - acc: 0.4736 - val_loss: 0.9969 - val_acc: 0.5632\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56320, saving model to task3bilstm.best.hdf5\n",
      "Epoch 2/5\n",
      "22974/22974 [==============================] - 32s 1ms/step - loss: 0.8284 - acc: 0.6649 - val_loss: 1.0103 - val_acc: 0.5652\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.56320 to 0.56516, saving model to task3bilstm.best.hdf5\n",
      "Epoch 3/5\n",
      "22974/22974 [==============================] - 30s 1ms/step - loss: 0.5017 - acc: 0.8168 - val_loss: 1.1813 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.56516\n",
      "Epoch 4/5\n",
      "22974/22974 [==============================] - 31s 1ms/step - loss: 0.2887 - acc: 0.8988 - val_loss: 1.4935 - val_acc: 0.5276\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.56516\n",
      "Epoch 5/5\n",
      "22974/22974 [==============================] - 30s 1ms/step - loss: 0.1897 - acc: 0.9354 - val_loss: 1.7255 - val_acc: 0.5214\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.56516\n"
     ]
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "bilstmCheckpoint = ModelCheckpoint(bilstmWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "bilstmHistory = bilstmModel.fit(x_train,y_train,epochs=5,batch_size=128,validation_data=(x_val, y_val), callbacks=[bilstmCheckpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "bilstmModel.load_weights(bilstmWeightsFilePath)\n",
    "\n",
    "bilstm_y_test_pred = bilstmModel.predict(x_test)\n",
    "bilstm_y_test_pred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in bilstm_y_test_pred])\n",
    "bilstm_y_test_pred_text = onehot_encoder.inverse_transform(bilstm_y_test_pred)\n",
    "y_test_text = onehot_encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   88   76   13    0]\n",
      " [   0  771 2371  402    1]\n",
      " [   0  490 4450 1254    0]\n",
      " [   0   80 1071 1179    2]\n",
      " [   0    7   18  105    1]]\n",
      "Average fscore: 0.27964083520996447\n",
      "testAccuracy 0.5170853865417239\n",
      "Recalls for each class: [0.0, 0.21748942172073343, 0.7184371972876977, 0.5055746140651801, 0.007633587786259542]\n",
      "Precisions for each class [0.0, 0.536908077994429, 0.5572251440020035, 0.39925499492041994, 0.25]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(y_test_text, bilstm_y_test_pred_text, labels=[-2,-1,0,1,2])\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2] +cm[3][3] + cm[4][4])/sum(sum(cm))\n",
    "avgfscore, recalls, precisions = averageFScore(cm)\n",
    "print(f\"Average fscore: {avgfscore}\")\n",
    "print(f\"testAccuracy {testAccuracy}\")\n",
    "print(f\"Recalls for each class: {recalls}\")\n",
    "print(f\"Precisions for each class {precisions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bidirectional LSTM makes only a small improvement on the LSTM. It has a validation accuracy of 52.1% and a test accuracy of 51.8%. The NBOW remains the strongest model despite it using only the average word vector, meaning it ignores the sequences of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eenlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
