{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNNLPLab3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "eenlp",
      "language": "python",
      "name": "myenv"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattthelee/nlp-labs/blob/master/nnnlp-labs/NNNLPLab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hExKCzh6doIW"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 3 - Neural Network Classifier Using Simple Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HixoFOoCIJ7V"
      },
      "cell_type": "markdown",
      "source": [
        "In this session, we demonstrate how to solve a text classification task using simple \n",
        "feedforward neural network classifier. We will use IMDB Large Movie Review Dataset to train a binary classification model, able to predict whether a review is positive or negative. First, our network takes one-hot word vectors as input, averages them to make one vector and trains a \n",
        "fully-connected layer to predict the output. In the second part, we replace the one-hot vectors with the word embeddings and add a layer to see how much that improves the performance.\n",
        "\n",
        "We are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. But it is not straightforward to define models where layers connect to more than just the previous and next layers. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m8fpBfhBpupy",
        "outputId": "9819cc2b-e6fa-4f0d-b271-fa608a935cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "import os\n",
        "!mkdir drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 110851 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            " awspersonalkey.gpg\n",
            " awspersonalkey.txt\n",
            "'Colab Notebooks'\n",
            "'Data Science Books.zip'\n",
            " document.tex\n",
            " glove.6B.300d.txt\n",
            " id_rsa.pub\n",
            "'India_China Backpacking.desktop'\n",
            "'Italy, Greece ,Turkey.desktop'\n",
            " leempubgpgkey.txt\n",
            "'Mags meeting 13_10_15.odt'\n",
            "'Marlo papers'\n",
            " mobilekey.pgp\n",
            " newlaptopkey2.pem.gpg\n",
            " personalStatementImperial13thjanV4.odt\n",
            " personalStatementImperial13thjanV4.odt.odt\n",
            "\"personalStatementImperial laura's comments.odt\"\n",
            "\"personalStatementImperial laura's comments.odt.odt\"\n",
            " personalStatementImperial.odt.odt\n",
            "'personalStatementImperial.odt.odt (b26b911f)'\n",
            "'Quantum Black - DevOps Engineer.docx'\n",
            "'Rome, Greece cycling.desktop'\n",
            "'SRCNN Presentation - Group O.pdf'\n",
            "'SRCNN Presentation - Group O.pdf (ffb6f50e)'\n",
            "'SRCNN Presentation - Group O.pptx'\n",
            "'Untitled document.odt'\n",
            "'Untitled document.odt (07de5fa4)'\n",
            "'Untitled document.odt (9ecf24a6)'\n",
            "'Untitled document.odt (e3127427)'\n",
            "'Untitled spreadsheet.ods'\n",
            "'Updatedby MumpersonalStatementImperial (002).odt.odt'\n",
            " xcel-chart.csv\n",
            " xcel-chart.csv.ods\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Gp9J7CE-dQe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cqvPQvgvPv1W"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EundMtGPpCdf"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 50000 labeled movie reviews. These are split into 25,000 reviews for training and 25,000 reviews for testing. The  dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The data is preprocessed. For text classification, it is ususal to limit the size of the vocabulary to stop the dataset from becoming too sparse, creating possible overfitting. We keep the top 10,000 most frequently occurring words in the training data.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NyuSzkafqNca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a61101f-8a8f-4386-b283-e7f75f12646f"
      },
      "cell_type": "code",
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "17473536/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "6U4iCV9-rmay"
      },
      "cell_type": "markdown",
      "source": [
        "We now can start playing around with the data, let’s first see the length:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h-gjWRAuqg5s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcd7f612-5f03-4d84-8036-ed4d6b10c3c6"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(X_train), len(y_train)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MTRZrpcyr-4x"
      },
      "cell_type": "markdown",
      "source": [
        "The  reviews have been converted to integers and each integer represents a  word in a dictionary. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "79Ev72Kgq4XL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3723
        },
        "outputId": "222b51d6-a4f7-4579-bee0-c3a6e374f07a"
      },
      "cell_type": "code",
      "source": [
        " X_train[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Tvuu4KhStqei"
      },
      "cell_type": "markdown",
      "source": [
        "We can convert integers back to words by querying a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gMCH1OoDrSNR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "90705293-264e-4747-948e-046647188c48"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n",
            "1654784/1641221 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5IreFXgruZot"
      },
      "cell_type": "markdown",
      "source": [
        "Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown tokens. Index 0 will be used for padding."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "abIb7Fe5u3GQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<UNUSED>\"] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9TnnSuspvC5b"
      },
      "cell_type": "markdown",
      "source": [
        "To reverse key and values in a dictionary:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nKOiVVXQu-_I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZmTJEm8xvUvW"
      },
      "cell_type": "markdown",
      "source": [
        "To view a word:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SqN5jgVKvJJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c63caa27-0d24-4160-b954-ace383775b1f"
      },
      "cell_type": "code",
      "source": [
        "reverse_word_index[25]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q6QjrzgVvrYn"
      },
      "cell_type": "markdown",
      "source": [
        "And to recreate the whole sentence from our training data we define decode_review:\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wvrKeMgxvWlv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Sxg4YA_NvdRg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06eee753-d74e-40a4-c1b9-170ef1be7b85"
      },
      "cell_type": "code",
      "source": [
        "decode_review(X_train[10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "c8gIzXncfaJK"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating One-hot word vectors"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "B9W4yb3rv_E0"
      },
      "cell_type": "markdown",
      "source": [
        "It is  common to use one-hot representation as input in Natural Language Processing tasks. In Keras, the Embedding layer takes an index as an input and convert it to one-hot vector with the length of the vocabulary size. Then multiplies these vectors by a normal weight matrix. But there is no way to only get a one-hot vector as the output of a layer in Keras. To solve this we use Lambda() layer and a function that creates the one-hot layer. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RPO_pK9zH4C5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def OneHot(input_dim=None, input_length=None):\n",
        "    \n",
        "    if input_dim is None or input_length is None:\n",
        "        raise TypeError(\"input_dim or input_length is not set\")\n",
        "\n",
        "    \n",
        "    def _one_hot(x, num_classes):\n",
        "        return K.one_hot(K.cast(x, 'uint8'),\n",
        "                          num_classes=num_classes)\n",
        "\n",
        "    return Lambda(_one_hot,\n",
        "                  arguments={'num_classes': input_dim},\n",
        "                  input_shape=(input_length,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "364d3MAw0ez9"
      },
      "cell_type": "markdown",
      "source": [
        "input_dim refers to the length of the one-hot vector and input_length refers to the length of the input sequence. Since the input to K.one_hot should be an integer tensor, we cast x to one (Keras passes around float tensors by default).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VHz76GNA2M4r"
      },
      "cell_type": "markdown",
      "source": [
        " Each text sequence has in most cases different length of words. Here, we fill sequences with a pad token (0) to fit the size. This special tokens is then masked not to be accounted in averaging, loss calculation etc. We set the maximum length to 256."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9G_o7PsvgSFt"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing input data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jiFn7sd_wF5j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "X_train_enc = keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "X_test_enc = keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kcjFH1wKF_7d"
      },
      "cell_type": "markdown",
      "source": [
        "And to view a padded review:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zwH4dcfW_a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "79b72285-a159-42a9-a6a8-0070fbf0917c"
      },
      "cell_type": "code",
      "source": [
        "print(X_train_enc[1])\n",
        "print(len(X_train_enc[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
            "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
            "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
            "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
            "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
            "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
            "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
            "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
            " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
            "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
            "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
            " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
            "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
            "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F1zcxFwNGepA"
      },
      "cell_type": "markdown",
      "source": [
        "Now we want to build the neural network model. We  are going to have a hidden layer with 16 hidden units. \n",
        "\n",
        "First, we want to transform each index to an embedded vector and then average all vectors to a single one. It has been showed that unweighted average of word vectors outperforms many complicated networks that model semantic and syntactic compositionality. As an example you can take a look at this: (http://anthology.aclweb.org/P/P15/P15-1162.pdf)\n",
        "\n",
        "To average we need to ignore padded zeros:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Yi04MLIvJOGZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "whgIIB5ggjna"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network model using one-hot vectors"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jlOLnlnSJgrU"
      },
      "cell_type": "markdown",
      "source": [
        "The first layer is an one-hot layer. The second layer is to compute average on all word vectors in a sentence without considering padding. The  output vector is piped through a fully-connected layer. The last layer is connected with a single output node with the sigmoid activation function. The final value is a float between 0 and 1. \n",
        "The vocabulary count of the movie reviews (10000) is used as the input shape. At the end we visualize the model summary."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_Pn83gBbxiK7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "55410c14-1158-40bc-860d-d6f75889326d"
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "model = Sequential()\n",
        "model.add(OneHot(VOCAB_SIZE,MAX_SEQUENCE_LENGTH))\n",
        "#model.add(GlobalAveragePooling1DMasked())\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 256, 10000)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_Mz96xpCgvTj"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F3HbW_IKLqwT"
      },
      "cell_type": "markdown",
      "source": [
        "To compile the model we need a loss function and an optimizer. We use binary_crossentropy loss function which is just a special case of categorical cross entropy. We also use Adam optimizer that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can read more about it here:\n",
        "(https://arxiv.org/abs/1412.6980v8\n",
        ")\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qh1PWTNMxjUw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "E1jwQQqCN5Ia"
      },
      "cell_type": "markdown",
      "source": [
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f5lAqzQlxjSM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "y_val = np.array(y_train[:10000])\n",
        "partial_y_train = np.array(y_train[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "E8Kpo5G3OJEY"
      },
      "cell_type": "markdown",
      "source": [
        "Then we start to train the model for 40 epochs in mini-batches of 512 samples and monitor the model's loss and accuracy on the validation set."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "99_z39KAxjPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        },
        "outputId": "953fd655-6761-4bbd-a66a-0c10f6d51f1e"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 10s 666us/step - loss: 0.6924 - acc: 0.5159 - val_loss: 0.6916 - val_acc: 0.4998\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6902 - acc: 0.5866 - val_loss: 0.6892 - val_acc: 0.6296\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6876 - acc: 0.6603 - val_loss: 0.6865 - val_acc: 0.6651\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6845 - acc: 0.6683 - val_loss: 0.6836 - val_acc: 0.6509\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6810 - acc: 0.6741 - val_loss: 0.6801 - val_acc: 0.6601\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6773 - acc: 0.6583 - val_loss: 0.6760 - val_acc: 0.6721\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6729 - acc: 0.6669 - val_loss: 0.6719 - val_acc: 0.6734\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 8s 527us/step - loss: 0.6684 - acc: 0.6807 - val_loss: 0.6675 - val_acc: 0.6765\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6634 - acc: 0.6847 - val_loss: 0.6633 - val_acc: 0.6776\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6584 - acc: 0.6881 - val_loss: 0.6582 - val_acc: 0.6836\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.6531 - acc: 0.6904 - val_loss: 0.6532 - val_acc: 0.6855\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6479 - acc: 0.6923 - val_loss: 0.6481 - val_acc: 0.6833\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.6426 - acc: 0.6946 - val_loss: 0.6430 - val_acc: 0.6863\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6373 - acc: 0.6972 - val_loss: 0.6381 - val_acc: 0.6980\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.6318 - acc: 0.7010 - val_loss: 0.6330 - val_acc: 0.6990\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6265 - acc: 0.7049 - val_loss: 0.6280 - val_acc: 0.7019\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.6213 - acc: 0.7058 - val_loss: 0.6236 - val_acc: 0.7022\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.6161 - acc: 0.7087 - val_loss: 0.6183 - val_acc: 0.7030\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.6112 - acc: 0.7099 - val_loss: 0.6137 - val_acc: 0.7032\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6064 - acc: 0.7125 - val_loss: 0.6090 - val_acc: 0.7075\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.6014 - acc: 0.7171 - val_loss: 0.6046 - val_acc: 0.7117\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5968 - acc: 0.7184 - val_loss: 0.6004 - val_acc: 0.7084\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5925 - acc: 0.7183 - val_loss: 0.5961 - val_acc: 0.7138\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5879 - acc: 0.7251 - val_loss: 0.5920 - val_acc: 0.7146\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5838 - acc: 0.7230 - val_loss: 0.5880 - val_acc: 0.7204\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5797 - acc: 0.7252 - val_loss: 0.5845 - val_acc: 0.7165\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5759 - acc: 0.7267 - val_loss: 0.5810 - val_acc: 0.7235\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5721 - acc: 0.7307 - val_loss: 0.5773 - val_acc: 0.7252\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5686 - acc: 0.7321 - val_loss: 0.5738 - val_acc: 0.7257\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5651 - acc: 0.7337 - val_loss: 0.5709 - val_acc: 0.7235\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5622 - acc: 0.7315 - val_loss: 0.5675 - val_acc: 0.7294\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5588 - acc: 0.7365 - val_loss: 0.5650 - val_acc: 0.7318\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5559 - acc: 0.7373 - val_loss: 0.5618 - val_acc: 0.7326\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5530 - acc: 0.7396 - val_loss: 0.5593 - val_acc: 0.7316\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5506 - acc: 0.7403 - val_loss: 0.5567 - val_acc: 0.7355\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5483 - acc: 0.7406 - val_loss: 0.5544 - val_acc: 0.7366\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5459 - acc: 0.7417 - val_loss: 0.5526 - val_acc: 0.7365\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5432 - acc: 0.7437 - val_loss: 0.5502 - val_acc: 0.7376\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 8s 525us/step - loss: 0.5411 - acc: 0.7439 - val_loss: 0.5484 - val_acc: 0.7389\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 8s 526us/step - loss: 0.5390 - acc: 0.7451 - val_loss: 0.5461 - val_acc: 0.7413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i_9a_rybhG5J"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EYLH8kOgOo9W"
      },
      "cell_type": "markdown",
      "source": [
        "To evaulate the model on test data:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CFMt2Q7b3taP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c27fc9c9-16f9-42c2-8df0-56606b05f7ca"
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 9s 362us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9RrKiPHcAmQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5507639-cbbb-463d-a070-05afcdbbbaf0"
      },
      "cell_type": "code",
      "source": [
        "print(results)\n",
        "# loss, accuracay "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5458652648353577, 0.7398]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pW7IpHxMO6qp"
      },
      "cell_type": "markdown",
      "source": [
        "Our first model accuracy using one-hot vectors is \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OwZk_yoWhPJB"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting the accuracy graph"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JIDPH1J7PMzN"
      },
      "cell_type": "markdown",
      "source": [
        "To plot a graph of accuracy and loss over time we can use Matplotlib:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LS9k2vvSAqB7",
        "outputId": "bba38812-90d0-4876-c3d2-a7344d472614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1IUQQggzIUNdCCGEMBP/AydWl9rqb55aAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "a7OwOQw4h8RX"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network model using word embeddings"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "l-QzOMO_P4jc"
      },
      "cell_type": "markdown",
      "source": [
        "Now instead of one-hot vectors, we want to use embedding. We change our first layer in model1 to an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MFrCsL-NBFVL",
        "outputId": "4c0858af-9d38-437d-ae78-d2acbb28f71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1683
        }
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE= 10000\n",
        "\n",
        "# put your code here\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(VOCAB_SIZE,64,input_length = MAX_SEQUENCE_LENGTH))\n",
        "#model2.add(OneHot(VOCAB_SIZE,MAX_SEQUENCE_LENGTH))\n",
        "#model2.add(GlobalAveragePooling1DMasked())\n",
        "model2.add(GlobalAveragePooling1D())\n",
        "model2.add(Dense(16, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history2 = model2.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model2.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 256, 64)           640000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                1040      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 641,057\n",
            "Trainable params: 641,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 1s 43us/step - loss: 0.6897 - acc: 0.6312 - val_loss: 0.6839 - val_acc: 0.7231\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.6725 - acc: 0.7307 - val_loss: 0.6590 - val_acc: 0.7442\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.6346 - acc: 0.7731 - val_loss: 0.6126 - val_acc: 0.7738\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.5737 - acc: 0.8097 - val_loss: 0.5484 - val_acc: 0.8094\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 20us/step - loss: 0.5004 - acc: 0.8372 - val_loss: 0.4814 - val_acc: 0.8276\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.4306 - acc: 0.8615 - val_loss: 0.4257 - val_acc: 0.8473\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.3740 - acc: 0.8751 - val_loss: 0.3850 - val_acc: 0.8555\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.3319 - acc: 0.8866 - val_loss: 0.3576 - val_acc: 0.8631\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.2979 - acc: 0.8982 - val_loss: 0.3349 - val_acc: 0.8703\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.2710 - acc: 0.9061 - val_loss: 0.3192 - val_acc: 0.8754\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.2485 - acc: 0.9136 - val_loss: 0.3083 - val_acc: 0.8790\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.2293 - acc: 0.9209 - val_loss: 0.3016 - val_acc: 0.8780\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.2130 - acc: 0.9270 - val_loss: 0.2945 - val_acc: 0.8821\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1979 - acc: 0.9330 - val_loss: 0.2900 - val_acc: 0.8826\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1848 - acc: 0.9397 - val_loss: 0.2877 - val_acc: 0.8839\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1729 - acc: 0.9457 - val_loss: 0.2862 - val_acc: 0.8847\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1623 - acc: 0.9487 - val_loss: 0.2858 - val_acc: 0.8839\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1527 - acc: 0.9539 - val_loss: 0.2862 - val_acc: 0.8848\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1426 - acc: 0.9578 - val_loss: 0.2873 - val_acc: 0.8852\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 22us/step - loss: 0.1346 - acc: 0.9609 - val_loss: 0.2887 - val_acc: 0.8848\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1267 - acc: 0.9634 - val_loss: 0.2919 - val_acc: 0.8851\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1194 - acc: 0.9668 - val_loss: 0.2948 - val_acc: 0.8839\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1126 - acc: 0.9683 - val_loss: 0.2979 - val_acc: 0.8829\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1064 - acc: 0.9717 - val_loss: 0.3029 - val_acc: 0.8819\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.1004 - acc: 0.9724 - val_loss: 0.3058 - val_acc: 0.8825\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 20us/step - loss: 0.0942 - acc: 0.9767 - val_loss: 0.3106 - val_acc: 0.8817\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0890 - acc: 0.9782 - val_loss: 0.3155 - val_acc: 0.8803\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0841 - acc: 0.9797 - val_loss: 0.3210 - val_acc: 0.8801\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0801 - acc: 0.9818 - val_loss: 0.3263 - val_acc: 0.8793\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0753 - acc: 0.9838 - val_loss: 0.3317 - val_acc: 0.8786\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0711 - acc: 0.9850 - val_loss: 0.3380 - val_acc: 0.8777\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0671 - acc: 0.9859 - val_loss: 0.3441 - val_acc: 0.8782\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0640 - acc: 0.9867 - val_loss: 0.3504 - val_acc: 0.8775\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0603 - acc: 0.9885 - val_loss: 0.3570 - val_acc: 0.8763\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0571 - acc: 0.9899 - val_loss: 0.3633 - val_acc: 0.8765\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0536 - acc: 0.9906 - val_loss: 0.3700 - val_acc: 0.8763\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0518 - acc: 0.9899 - val_loss: 0.3767 - val_acc: 0.8756\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0481 - acc: 0.9919 - val_loss: 0.3837 - val_acc: 0.8752\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0457 - acc: 0.9929 - val_loss: 0.3908 - val_acc: 0.8754\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 21us/step - loss: 0.0424 - acc: 0.9933 - val_loss: 0.3988 - val_acc: 0.8717\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "[0.4232503311538696, 0.8614]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I4zIPJDcTPq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "374914fa-28c2-49b6-a698-381a9a902037"
      },
      "cell_type": "code",
      "source": [
        "results = model2.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 46us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "waS96edDTRyL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e89bceea-9a28-4977-9744-7faca73b791f"
      },
      "cell_type": "code",
      "source": [
        "print (results)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.4232503311538696, 0.8614]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XB7aveVzTC5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "704fab73-18bf-4c2e-b40a-58bd87b5e1a1"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history2.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3XlYlPX+//HnPTMMioCCArmWWeSW\nlt+ySEtFcMt+HVsxS0tPluZJO2kqJ9PyaLbYUatTZrapJS1wKk9JmtpqWtriVhanzEwRFFFkneX3\nx8gosg3IwDC8HtfFxdz33Pc9n8/cyvv+7IbT6XQiIiIi9Z6prhMgIiIiNUNBXURExE8oqIuIiPgJ\nBXURERE/oaAuIiLiJxTURURE/ISCuvi9mTNnMmjQIAYNGkSXLl3o16+fezsnJ6dK1xo0aBCZmZkV\nHjN//nzeeOONM0lyjbv99ttJTk6ukWtdcMEFHDhwgDVr1jB9+vQz+rw333zT/dqT71ZEKmap6wSI\neNvDDz/sfh0bG8vjjz/OJZdcUq1rrV69utJj7r///mpdu76Jj48nPj6+2udnZGTw4osvctNNNwGe\nfbciUjGV1KXBu+222/jXv/7F4MGD2bp1K5mZmYwZM4ZBgwYRGxvLyy+/7D62uJS6adMmbr75ZubP\nn8/gwYOJjY1l8+bNAEybNo1///vfgOshYuXKldxwww307t2befPmua/1/PPPExMTw/XXX8+KFSuI\njY0tM31vvfUWgwcPZsCAAYwYMYJ9+/YBkJyczL333ktiYiIDBw5kyJAh/PzzzwDs3buXG2+8kbi4\nOO6//37sdnup637yySdcc801JfZde+21fPrppxV+B8WSk5O5/fbbK/28jz/+mGuuuYaBAwdy3XXX\nsWvXLgASEhL4888/GTRoEIWFhe7vFuC1115jyJAhDBo0iHHjxnH48GH3d7to0SLuuOMO+vXrxx13\n3EFeXl6ptOXl5TFp0iQGDhxIbGwsjz32mPu9vXv3MmLECOLj47n++uvZsWNHhftjY2P55ptv3OcX\nb//xxx/07t2buXPncuutt1aYV4AXXniB/v37M3DgQB599FHsdju9evVi27Zt7mOWL1/O+PHjS+VH\nxFMK6iLA9u3b+e9//0uPHj147rnnaNOmDatXr+bVV19l/vz57N+/v9Q5O3fupHv37nz44Yfccsst\nPPfcc2Ve++uvvyYpKYl33nmH5cuXc+DAAX7++WdefPFF3n33XV5//fVyS6mHDh3ikUce4eWXX+aj\njz6iXbt27gcGgE8//ZRbbrmF1NRULrvsMl599VUAnnzySWJiYli7di2jRo1i69atpa4dExPDgQMH\n2Lt3L+AKagcOHOCKK67w+DsoVt7n2Ww2pk2bxuzZs0lNTS0RYOfOnUvLli1ZvXo1VqvVfa3vvvuO\npUuXsmzZMlavXk2rVq2YP3+++/3Vq1fzr3/9izVr1nD48GHWrFlTKj1vvPEGx48fZ/Xq1aSkpJCc\nnOwOzDNmzODqq69mzZo1jBs3jgceeKDC/RU5cuQInTp1Yvny5RXm9ZtvvuHtt9/m3Xff5f3332fL\nli189NFHDB48mFWrVrmvt2bNGq6++upKP1ekPArqIkCfPn0wmVz/HR588EFmzJgBQNu2bYmIiOCP\nP/4odU6TJk2Ii4sDoEuXLvz5559lXvuaa67BbDYTFRVF8+bN2b9/P19//TU9e/YkMjKSwMBArr/+\n+jLPbd68OVu2bOGss84C4JJLLnEHYYAOHTrQtWtXADp37uwOvN988w1DhgwBoFu3bpx77rmlrm21\nWunXrx/r1q0DYO3atcTFxWGxWDz+DoqV93kWi4Uvv/ySiy66qMz0l2XDhg0MHDiQ5s2bA3DjjTfy\nxRdfuN/v06cPzZo1w2KxEB0dXebDxujRo/n3v/+NYRg0bdqU888/nz/++IOCggI2bdrE0KFDAejf\nvz9vvvlmufsrU1RU5G6CqCivn376KX369CE4OBir1cqyZcsYMGAAV199NR988AEOh4MjR46wfft2\n+vXrV+nnipRHbeoiQNOmTd2vt23b5i6ZmkwmMjIycDgcpc4JCQlxvzaZTGUeAxAcHOx+bTabsdvt\nHD16tMRnRkVFlXmu3W5n0aJFrFu3DrvdzvHjx2nfvn2ZaSi+NkB2dnaJzw0NDS3z+gMHDuS1115j\n1KhRrF271l316+l3UKyiz1u2bBkpKSkUFhZSWFiIYRjlXgfg8OHDREZGlrjWoUOHKs3zqX777Tfm\nzZvH//73P0wmEwcOHOC6667jyJEjOBwO9zUMw6BJkyakp6eXub8yZrO5RL7Ly2tWVlaJPDVu3BiA\niy++mICAADZv3syBAwfo3bs3QUFBlX6uSHlUUhc5zZQpUxg4cCCpqamsXr2asLCwGv+M4OBgcnNz\n3dsHDx4s87gPPviAdevWsXz5clJTU7n33ns9un5oaGiJnv3FbdKnu/LKK/nxxx/57bff+O2337j8\n8suBqn8H5X3e1q1bWbJkCc899xypqan885//rDTtLVq04MiRI+7tI0eO0KJFi0rPO9UjjzzC+eef\nz4cffsjq1avp2LEjAGFhYRiGQVZWFgBOp5M9e/aUu9/pdJZ6YMvOzi7zMyvKa1hYmPva4AryxdtX\nX301q1evZvXq1e7aDpHqUlAXOc2hQ4fo2rUrhmGQkpJCXl5eiQBcE7p168amTZs4fPgwhYWF/Oc/\n/yk3La1btyY8PJysrCw+/PBDjh8/Xun1L7roIndb89atW/n999/LPM5qtdK7d2+eeOIJ+vfvj9ls\ndn9uVb6D8j7v8OHDNG/enFatWpGXl0dKSgq5ubk4nU4sFgu5ubnYbLYS1+rbty9r1qxxB72VK1fS\np0+fSvN8qkOHDtGpUyfMZjNffPEFe/bsITc3F6vVSq9evUhJSQHgs88+Y+zYseXuNwyDiIgIfvzx\nR8D1kFVQUFDmZ1aU19jYWNatW0d2djY2m4177rmHzz//HIChQ4eydu1avv322yrnU+R0Cuoip5k4\ncSL33HMP11xzDbm5udx8883MmDGj3MBYHd26dWPYsGEMGzaMkSNHltuOOnToUI4cOUJ8fDz3338/\nkyZN4sCBAyV60ZdlypQprF+/nri4OFasWMEVV1xR7rEDBw5k7dq1DB482L2vqt9BeZ935ZVXEhkZ\nSVxcHKNHj2bUqFGEhIRw7733csEFF9C0aVN69epVoj9Ct27dGDt2LCNGjGDQoEEcO3aM++67r8L8\nnm7cuHE89thjDB06lM2bNzNhwgSefvpptmzZwpw5c1i/fj39+/dnwYIFPPnkkwDl7h8/fjyvvPIK\nQ4cOJS0tjfPOO6/Mz6worxdddBFjxozhL3/5C1dffTWdO3d2t99fcMEFNGvWjN69e9OoUaMq5VPk\ndIbWUxepG06n093mumHDBhYsWFBuiV3825133smtt96qkrqcMZXURerA4cOHufzyy9m3bx9Op5MP\nP/zQ3WtaGpYtW7awb98+rrzyyrpOivgB9X4XqQPh4eFMmjSJ22+/HcMwOPfccz0aFy3+Zfr06Wzd\nupUnnnjCPaRS5Eyo+l1ERMRP6NFQRETETyioi4iI+Il636aekXHMo+PCwoLIyqrZscZ1SfnxbcqP\nb1N+fJvyU7GIiJBy32swJXWLxVzXSahRyo9vU358m/Lj25Sf6mswQV1ERMTfKaiLiIj4CQV1ERER\nP+HVoL57927i4uJYvnx5qfe+/PJLbrjhBm6++WaeffZZ9/65c+dy8803k5CQwA8//ODN5ImIiPgV\nr/V+z83NZfbs2cTExJT5/j//+U+WLl1KVFQUt956KwMHDuTw4cPs2bOHpKQk0tLSSExMJCkpyVtJ\nFBER8SteK6lbrVaWLFlCZGRkqff27t1L06ZNadmyJSaTiT59+rBx40Y2btxIXFwcAB06dCA7O7vE\nGs0iIiJSPq+V1C0WCxZL2ZfPyMggPDzcvR0eHs7evXvJysqiS5cuJfZnZGQQHBxc7ueEhQV5PFyg\norF99ZHy49uUH9+m/Pg25ad6fHryGU+mpfd0QH9ERIjHE9Wcqaef/hc//bSLw4cPkZ+fT6tWrQkN\nbcrcuU9Ueu4HH7xPkybB9OlT9vraCxfO58YbE+jevWOt5ac21Ob9qQ3Kj29Tfnyb8lP59cpTJ0E9\nMjKSzMxM93Z6ejqRkZEEBASU2H/w4EEiIiK8np6UFAsLFljZvdtEdLSDSZMKGTbMVu3r/e1v9wGu\nAP2//6UxYcIkj88dMuSaCt+fOPH+aqdLRERqz8nYAtHRQWccWzxRJ0G9TZs25OTk8Mcff3DWWWex\nfv16nnzySbKysnj66adJSEhgx44dREZGVlj1XhNSUizcdVdj9/auXeYT23k1/uVv3foNK1cuJzc3\nlwkT7uPbb7ewYcPHOBwOYmJ6MXr0WJYuXUyzZs1o374DyclvYhgm9uz5lb59+zN69FgmTBjL3//+\nACtXfs7Bg4f4/fc97Nv3B/feez8xMb1YvvwV1q79iFatWmOz2UhIGEGPHpe40/D115t48cXnCQgI\nICQkhEcemUdAQAALFjzJzp3bMZvNTJkynXPPPa/MfSIi/qgqhTtPjq3N2HIqrwX17du389hjj7Fv\n3z4sFgupqanExsbSpk0b4uPjmTVrFvff7yp1DhkyhPbt29O+fXu6dOlCQkIChmEwc+ZMbyXPbcEC\na5n7Fy60euWLT0v7hTfeSMZqtfLtt1v4979fxGQycdNN13LzzbeUOHbnzh28/vo7OBwObrzxGkaP\nHlvi/YMH03nyyUV89dWXvPvuO3Tp0pXk5Ld44413OH78OAkJ15GQMKLEOceOHWPmzH/SqlVrZs9+\niE2bNhIYGMjBg+m88MIrfPfdVj7+eA2HDh0qtU9BXUR8QU3XrlYlAHt6bG3HlmJeC+pdu3Zl2bJl\n5b5/6aWXljlcbfLkyd5KUpl27y57AEB5+8/Ueeedj9XqutmNGjViwoSxmM1mjhw5wtGjR0sce8EF\nHWnUqFG51+rW7SLA1ZzhqvnYy7nndiAwsBGBgY3o1KlLqXOaNWvGY4/9E7vdzp9/7uP//u9SsrIO\nc+GF3QG46KIeXHRRD1aseLXUPhERb/KkurqqJWBPHgCqEoA9Pba2Y0uxBj+jXHS0o0r7z1RAQAAA\nBw7sJylpBfPnP80zz7zAWWedVepYs7niXv2nvu90OnE6wWQ6eUsNo/Q5jz46m/vue4BnnnmB3r2v\nAsBkMuN0lsxvWftERKojJcVCnz5BtGwZTJ8+QaSklC5PFgfrXbvM2O0ng/Xpx1YUVCu+plHuNasS\ngD09trZjS7EGH9QnTSosc//EiWXvrylHjhwhLCyMoKAgfvrpRw4cOEBRUdEZXbNly5b8739p2Gw2\nsrKy+PHHXaWOOX48h6ioszh27Bhbt26hqKiITp06s3XrNwDs3v0j8+c/VuY+EZFingTq4uM8Caye\nBuuqBGBPr1mVAOzpsXUVWxp8UB82zMbixXl07mzHYnHSubOdxYu925EB4Pzzo2ncOIhx40bz8ccf\nce21151x4AwPb058/CDuvHMkCxc+SefOXUqV9q+77kbGjRvD44/PYcSIkSxf/gpt2rTj7LPbM378\nX1mw4En+8pfrueiiHqX2iYj/q3qpuvxADTUfrKsSgD29ZlUCsKfHlowt1FpsMZyeDAb3YZ6O/Wso\n4x4/+OB94uMHYTabGTkygaeeeprIyKg6SGHVNJT7U18pP76tsvx42rHs9PbqYqcHoz59gti1q3Tz\nYOfOdjZsKDl3SMuWwdjtpdsCLRYnf/55csZQT6/paRqrms6UFAsLF578jiZOrLj3u6fHQgMYpy7e\nc+jQIcaOHUVAgJUBAwbVi4AuItVT0x3LvNEJLDraUWZgLau6uqxgXVYJGPI8CqqeXrP4up6Woqty\nbG1TSb2eUn58m/Lj23w5P/5Uqq5KOouPdQVrM9HR9kpLwJ6oaqnaG1RSFxFpgPytVA1VK1kXl4Bd\nQdCzKcAr48ulam9o8B3lRERqgycd0KoyXMsbHcuq1wms8g7Gw4bZ2LAhlz//zGHDhtwGFWRrm4K6\niMgZqMne4lUtVZflTIZWVSVYK1D7JgV1EZHT1NUYbJWq5UwpqHvBXXfdUWril+eff4Y33lhe5vFb\nt37Dgw8+AMC0aX8v9f477ySxdOnicj/vl19+5vff9wAwc+Z0Cgryq5t0kQavLsdgV79UXfE4aAXq\nhkNB3Qvi4weybt2aEvs2bFhHXNyASs+dN++pKn/eJ5+sY+/e3wF4+OFHCQwsf754EalYXbZrV7dU\nXVSEgrUA6v3uFf37D2DcuDGMH38vAD/+uIuIiAgiIiLLXPr0VFdf3Z///vdjvvlmM4sWzSc8vDnN\nm7dwL6U6Z84sMjIOUlRUwMiRf+Wss1ry7rvJfPLJOsLCwnjooem89loSOTnHePTRRygqKsJkMjFt\n2gwMw2DOnFm0atWaX375mejoC5g2bUaJz//oow95++0kzGYT55zTgalT/4HNZuOf/5xJevp+rNZA\nHnzwYcLCwkvti4iIrLXvWKQ6PBnX7Qu9xRWcpbr8PqjPmhXI++9bMJnA4WhSI9e85hobs2YVlPt+\nWFg4rVq1ZufO7XTu3JV169YQHz8IKHvp06CgoFLXWLz4GWbMmM3550czefK9tGrVmmPHjtKz5+UM\nHjyU/PwjjB8/gZdeWs5ll8XQt29/Onfu6j7/xRefZ+jQa+nffwDr16/lpZdeYMyYu/jpp108/PBc\nwsLCGTZsCMeOHSMk5OSYx7y8PObPf5qQkBDuuedO0tJ+YefO7TRv3pxZs+awdm0qn3/+KRaLpdS+\nYcNuqJHvV6Qqqjuuu7zhYp4GavDOhCkiZ8Lvg3pdiY8fxMcfr6Fz56588cWnPPfcS0DZS5+WFdT3\n79/P+edHA66lTwsKCggJCWXXrh28914yVmsAR49ml/v5P/20i7vvngBAjx6X8MorLwLQunVbmjdv\nAUCLFhEcP55TIqiHhoYyfbprnfs9e34lO/sIP/30I5dccikAcXEDAXjyyXml9onUNm+M6/b2GGwR\nb/L7oD5rVgGzZhWcmMzgeK19bp8+/XjttZeIjx9I27btCA0NBVxLnz7xxALOOac9Tz1V/gIupy6h\nWjzp35o1qzl69CjPPvsiAQF2hg27roIUGO7ziopsGIbreqcv8HLqhIJFRUU89dTjvPLK6zRv3oIH\nHph04hwTDkfJiQfL2idSk2p6HWxPq9WrWqpWsBZfoo5yXhIU1IQOHc7ntddedle9Q9lLn5alRYsI\nfv/9N5xOJ99+uwVwLdfasmUrTCYTa9ascZ9rGAZ2u73E+acunfrdd1vo2LFTpWnOzT2O2WymefMW\npKcf4Mcfd2Gz2ejYsTNbt34NwBdffMZrr71U5j4RT/j6uG5Qb3GpvxTUvSg+fhBff72J3r2vcu8r\na+nTQ4cyS507dux4HnxwKlOn3udelKVv31i+/PIzJk4cR+PGjYmMjOTll5fQvfvFLFjwBN98s9l9\n/l//ejerV3/AvffezQcfrGLMmLsqTW/Tps249NLL+OtfR/Lyy0u45ZbbWLToKfr3H0BeXh4TJozl\nzTffYPDgocTFDSy1T6Qy9WFct0h9pgVd6inlx7cpP2XzdCEQTxcWqcpiIcXH1/SCIb5A/958W20u\n6KKSuoicMU9nYNO4bhHv8vuOciLiXVXpga5x3SLepZK6iJSruARusVAjK4t5a75yEXFRSV1EyuRp\nCbwqPdA1rlvEu1RSF2lgPG3/9kYPdNBwMRFvUlAXaUCqsgKZN1YWExHvUlAXaUCq0v7trR7oIuI9\nCuoifsKTavWqtH9XdW1vVamL1D0FdRE/4Gm1elWnSj1ZAkclcJF6wKtBfe7cudx8880kJCTwww8/\nlHhv7dq1XH/99QwfPpzly5cDsGnTJi6//HJuu+02brvtNmbPnu3N5In4DU+r1ava/q3JWkTqF68N\nadu8eTN79uwhKSmJtLQ0EhMTSUpKAsDhcDB79mxSUlJo1qwZd955J3FxcQD07NmTRYsWeStZIn7J\nWyuQiUj94rWS+saNG92BukOHDmRnZ5OT45qzOSsri9DQUMLDwzGZTFx++eV8+eWX3kqKSL3mSVu5\nViATEfBiUM/MzCQsLMy9HR4eTkZGhvv18ePH+e233ygqKmLTpk1kZrpWKvvll1+4++67GT58OF98\n8YW3kidSL3jaVq5hZSICtTij3KmLwRmGwbx580hMTCQkJIQ2bdoAcM455zBhwgQGDx7M3r17GTly\nJB999BFWa9nthQBhYUFYLKXnki5LRSvb1EfKj2+rKD8rV8LcubBzJ3TuDImJkJBQ+rhnnin7/Gef\nbczYsSe3x46F0FB49NGT15w+HRISSs+fXl0N6f7UR8qPb6ut/HgtqEdGRrpL3wAHDx4kIiLCvd2z\nZ09ef/11AObPn0/r1q2JiopiyJAhALRr144WLVqQnp5O27Zty/2crKzcct87lZby820NKT+nT7+6\nbRsMHw5Hj5buWb5zZzBQegnSnTudZGTklNjXv7/r51QnKsfOWEO6P/WR8uPb/GLp1V69epGamgrA\njh07iIyMJDg42P3+X//6Vw4dOkRubi7r168nJiaG9957j6VLlwKQkZHBoUOHiIqK8lYSReqENyaA\nEREBL5bUe/ToQZcuXUhISMAwDGbOnElycjIhISHEx8dz0003MXr0aAzDYOzYsYSHhxMbG8vkyZP5\n+OOPKSoqYtasWRVWvYvUR1WdAMbTJUhFRLzapj558uQS2x07dnS/HjBgAAMGDCjxfnBwMM8//7w3\nkyRS5zxdUxw0BE1EqkYzyonUIE/WH6/uBDAagiYildF66iI1xNP1x1X6FhFvUVAXqSEVdYA7PWAP\nG2ZTEBeRGqfqd5EaUpUOcCIi3qC/NiI1RMPPRKSuKaiLVMKTuddBU7WKSN1Tm7pIBTzt/Aand4Az\nEx1tVwc4EalVCuoiFahK5zc42QHONS2kZ1MYi4jUFFW/i1RAnd9EpD7RXyZpsGp6nXIRkbqmoC4N\nktYpFxF/pKAuDZKnK6UNG2Zj8eI8One2Y7E46dzZzuLFpTvJiYj4AnWUkwapKm3lmv1NROoLldSl\nQVJbuYj4IwV18TuedIBTW7mI+CNVv4tf0UppItKQKaiLX9FKaSLSkKn6XfyKJosRkYZMf+nEr6gD\nnIg0ZArqUi9opTQRkcqpTV18XvVXSlMHOBFpWBTUxedVd6U0EZGGRtXv4vPU+U1ExDP6qyg+T53f\nREQ8o6AuPk+d30REPKOgLnWquFe7xUK5vdq1UpqIiGfUUU7qTFV7tSuIi4hUTCV1qTOermkuIiKe\nUVCXOqNe7SIiNcurfz3nzp3LzTffTEJCAj/88EOJ99auXcv111/P8OHDWb58uUfniH9Rr3YRkZrl\ntaC+efNm9uzZQ1JSEnPmzGHOnDnu9xwOB7Nnz2bJkiWsWLGC9evXc+DAgQrPEf+jXu0iIjXLa0F9\n48aNxMXFAdChQweys7PJyckBICsri9DQUMLDwzGZTFx++eV8+eWXFZ4j9Yen87SX7NWOerWLiJwh\nr/V+z8zMpEuXLu7t8PBwMjIyCA4OJjw8nOPHj/Pbb7/RunVrNm3aRM+ePSs8pzxhYUFYLGaP0hQR\nEVL9DPkgX8zPypVw110nt4t7tIeGQkJC6ePHjnX9uJiBxqUPqqd88f6cCeXHtyk/vq228lNrQ9qc\nTqf7tWEYzJs3j8TEREJCQmjTpk2l55QnKyvXo8+PiAghI+OYZ4mtB3w1P488EoQrOJc0e7ad/v3L\nv1e+mp/qUn58m/Lj25Sfyq9XHq8F9cjISDIzM93bBw8eJCIiwr3ds2dPXn/9dQDmz59P69atKSgo\nqPAc8X3q0S4iUne89pe2V69epKamArBjxw4iIyNLVKP/9a9/5dChQ+Tm5rJ+/XpiYmIqPUd8n3q0\ni4jUHa+V1Hv06EGXLl1ISEjAMAxmzpxJcnIyISEhxMfHc9NNNzF69GgMw2Ds2LGEh4cTHh5e6hyp\nXyZNKiwxS1wx9WgXEfE+w+lJw7UP87SdQm00tSclxcLChVZ27zYRHe1g4sTCSnu0+3J+qkP58W3K\nj29Tfiq/Xnk097vUOM3TLiJSN9R7SURExE8oqIvHPJ1URkRE6ob+KotHqrJMqoiI1A0FdfFIRcuk\nKqj7DqcTioqgsBAKCgwKCjjxY5zYd/J1UREEBECjRmC1OgkMhMBA1+vifVar6/2iIjh+3PW7qMg4\n8bv4swxsNtdnms3QvLmTFi2cBAXV9bch0vAoqItHNKlMxfLzISPD4OBBg4MHTTgcsG9fAEePGhw9\napCTg/v1sWMGR4/CsWOuYBgcDCEhTkJDnSd+l95u0sRJbq5BdjZkZxvunyNHXNc8cuTke0VFhpdy\nWbVpLoOCXME9IqL4t4MWLZzun7Cwkj8hIWB4K+kiDYSCungkOtrBrl2lp3/1h0llnE5XUM7Ph7w8\no9zfx4/DwYOmU4L3ySCenV1WNGpU5ucFBBQHbNfrY8dc18jNrV5Es1qdNG3qpFkzJ2ef7aRJk9NL\n2q7frpL4ydcWi5OiopOl+cJCo0RJPj//5L7AQAtgw2JxnW+xgNXqSn9AACd+nNjtBocOGWRkGGRm\nun5++MHk0YOG2XwywDdrBuHhrjydHvxP/WnWzEmTJmU/DNjtkJcHx48b5OZCbu7J3xERYLeb3A9P\noaGuPInUd/pnLB7xp0ll8vPhiy/MrFljYe1aC7//Xv3ahubNHbRq5aB7dyeRka6fiAgHZ5/dCKcz\n70TAOBnEQ0JcAbesIGSzUW6JPifHICjIFcRCQ6FZs+LXTho39n4J1zXONq9a5zqdcPQoJwK966Eo\nI8NVu5CV5fo5csTg8GGDI0fg8GGDtDQDh8OzTFmtxSV9J/n5JwN3fn5l5zcpsRUU5LrGqbUlzZo5\nOessJ1FRDs46q/i1k7POchAcrJoF8T0K6uIRV7t5XpUnlfEV+/cbJ4K4mU8/tbhLxU2bOomJsdG4\nsatE26gRNG7sLLFdvC8oCCIjHe7g3aKFq5RaloiIRmRkVO27sVigWTNXwIZ6PSdUCYYBTZu6vuvz\nzrN7dI7DAceOuQJ8dnZxwD8pX0h2AAAgAElEQVT5EHDqT/H+I0cMGjWCFi1ctRVBQa57VvK363VA\nQCDp6YUnHqA48QDlepA6csTg998NCgsrjthBQc4Tgd5BVJTr34rF4sRsdvUtsFg48dqJxQImk2tf\nYKDroaG4eaX4IaL44S8oyHWsSHUoqIvH6mpSmfx82LLFzGefmfnjD5O72vXUKtjiqtrwcCfBwa7S\n4bffmlizxsKaNRa2bTvZdBAdbSc+3k58vI1LL7WXG5il7phMJx8EvPGAExERSEZGQYXH5OfDkSMG\n6ekGBw4YHDhg4sCB4m2Te/+vv5pxOmuuyG4Yrlqd0FAnwcGu5oUmTVyvg4M58bvka9dCl2Z3M0zT\nptXvo2C3u/LuaqKpsWxJLdEtE1JSLCxYcLIEPmlS3ZbAbTb44QcTn31m4bPPzGzebPagKvWk4nbf\n4tK41eqkb18bAwbYiIuzcc45/lMKFu9p1Ah3lXv37gBl1zIUFUFmpqvvgd0OdrurA6Tdzim/Dfd2\nfn7JmoHTawqKt48dM0hPN5GTg4edH0sONzCZnO4Ho+KfoCBXE4WrD4lBXt7JfiPF26d+VqNGJx8s\nynqYCA52PXwUd36MiDjZITI0VM0TdUFBvYHzhfHnTids3w7vvhvA55+b+eILC8eOnfxr0KmTnauu\nstO7t40LLnBw9GjZ1bCnts0ePw49etiJi7PTp48NLfYn3hIQAC1bnvqgWPMPjQUFkJPjGkXh+u36\nN168z+FozL59BaeNiMA9MqKsjphBQU53E1NwMEREONxNTYGBJz+z+HN+/931gOFprURAQMlgHxbm\n+l6KH25sNnA4DPdr1wOR6wHIanXlqbgJ4/SmjOImDqu1ZC1GRTUajRqd7CgaEOC/DxwK6g1cXY4/\n37nTxJtvBpCcbOHAASjuLX7OOQ7+8pcirrzSTq9ediIiTv8jqZK2NCzFIxeaN4ey/v1HREBGRsWd\nVgsKTlarBwZWL6g5nZCbWzLYZ2W5RjwUj3bIzDTIyDCd+G2QlmZi2zbPP8xsLg7+3gtPhuE8MSeD\n63s99bXZXPIBw/XwceqDiGu/YcBFF7kKDX362OnSxeETfSEU1Bu42h5/fvCgQUqKhaSkALZvd7Vz\nN2vmZMQI6Nkzj9697bRtq6AtUtOKg/mZMAzcbfwunv1fPX7c1T/BMDilI2HJDoTFr8E12uLgwWPu\nYHoyyLqaN4pfFxYWP2CcWnNRskaj+HXxsE3XUM3Sr3Ny4PBhEzZbcXqcJWoJGjc+tSOkqxnjk08s\nfPKJK4w2b+7gyitdtYpXXWWjXbu6+TumoN7A1cb48/x8SE218OabAaxbZ8ZuN7BYnAwaVMRNN9mI\nj7fRpk1IlXuLi0j9UPJBwDOnPgCU5Ds1d+npBp995hpR88knZv7znwD+8x9Xz9v27R1cdZWNq66y\nc/PNtZcmBfUGzlvjz51O2LzZzJtvWnj3XdfMauCqrrrppiL+8hcbLVqoRC4i9VdUlJMbbrBxww02\nnE745RcTn35q5pNPzHz+uYVXX7Xy6quwahUsXlw7aVJQb+Bqevy50wkbNpiZNy+Qb791PWK3bOng\n9tsLufFGV0c3ERF/Yxhw/vkOzj/fwZgxRdhsrmG1X3xhITb2DNs9qkBBXWps/PlXX5l59FErGze6\n/lkNHVrEqFFF9O5tL6MKTUTEf1kscOmlDi69tPDEvAi19Lm18zHiz374wcSjjwby8ceuf07x8Tam\nTSvgwgtVKhcRqU0K6lJtP/1k4rHHrKxa5eoY0ru3K5j37KlgLiJSF3xgVJ14Q0qKhT59gmjZMpg+\nfYJISam557dffzW4555GXHVVEKtWBfB//2fn7bdzSU7OU0AXEalDKqn7IW/NEpeebvDkk1ZWrAjA\nZjPo3NnO9OkFDBhg99vZmURE6hMFdT9U07PE5eTAc89ZefZZK7m5Bh06OJg6NZ//9/9sPjGDkoiI\nuCio+6GamiXOZoMVKwJ4/HErGRkmIiIcPPxwASNGFGn1JhERH6Q/zX7oTGeJczohNdXM7NmB/Pyz\nmaAgJ5MnFzB+fKEWRhER8WGqPPVDkyaVPRucJ7PEbd1q4i9/aczIkUGkpZm47bZCNm06zgMPKKCL\niPg6ldT9UHVmifv1V4O5cwN5913X8LRBg4p48MHCGp0DXkREvEtB3U95OktcTg489ZSVxYutFBUZ\nXHyxnZkzC7jiCnstpFJERGqSV4P63Llz+f777zEMg8TERLp16+Z+b8WKFbz33nuYTCa6du3KP/7x\nD5KTk1m4cCHt2rUD4IorrmDcuHHeTGKD5XTCO+9YePjhQNLTTbRt6+Chh1w92jU8TUSkfvJaUN+8\neTN79uwhKSmJtLQ0EhMTSUpKAiAnJ4elS5fy0UcfYbFYGD16NN999x0AQ4YMYerUqd5KlgDbt5tI\nTAzkq68sNGrkZMqUAiZMKKRx6cXaRESkHvFaUN+4cSNxcXEAdOjQgezsbHJycggODiYgIICAgABy\nc3MJCgoiLy+Ppk2beispckJWFsybF8irrwbgcBgMGVLEI48U0K6dlkAVEfEHXuv9npmZSVhYmHs7\nPDycjBPL1AQGBnLPPfcQFxdHv3796N69O+3btwdcJfwxY8YwatQodu7c6a3kNSh2O7z2WgAxMU14\n+WUrHTo4ePPNXF55JV8BXUTEj9RaRzmn82TwyMnJYfHixaxevZrg4GBGjRrFjz/+SPfu3QkPD6dv\n3758++23TJ06lffff7/C64aFBWGxeLauZ0REyBnlwdd4kp+NG+Fvf4MtWyA4GJ54Au6914zVGlQL\nKayahnh/6hPlx7cpP76ttvLjtaAeGRlJZmame/vgwYNEREQAkJaWRtu2bQkPDwfgkksuYfv27dxw\nww106NABgIsvvpjDhw9jt9sxV7AYd1ZWrkfpiYgIISPjWHWz43M8yc/jj1t58slAAG68sYiHHiog\nKspJdnZtpLBqGuL9qU+UH9+m/Pi2ms5PRQ8IXqt+79WrF6mpqQDs2LGDyMhIgk/MXtK6dWvS0tLI\nz88HYPv27ZxzzjksWbKEVatWAbB7927Cw8MrDOhSvvXrzTz5ZCBnn+3gvfdyefbZfKKiVNUuIuLP\nvFZS79GjB126dCEhIQHDMJg5cybJycmEhIQQHx/PmDFjGDlyJGazmYsvvphLLrmENm3aMGXKFFau\nXInNZmPOnDneSp5fO3TI4G9/a0RAgJOXXsrjwgs1gYyISENgOE9t7K6HPK3SaCjVOU4njB7diP/+\nN4AZMwr4298qnxrWFzSU+1NfKT++TfnxbX5R/S51Y+VKC//9bwBXXGFj/Pj6EdBFRKRmKKj7kV9/\nNUhMbERoqJNnnslH3RFERBqWSoN6WlpabaRDPJSSYqFPnyAsFujTJ4iUFFe3CJsN7rmnMcePGzz2\nWD5t2tTrVhUREamGSjvK3XvvvYSGhnLDDTcwZMgQGmsu0TqTkmLhrrtOfv+7dplPbOeRlmbim2/M\nXHddEddfX/lCLiIi4n8qDer//e9/2b17Nx9++CG33XYbnTp14sYbbyyxOIvUjgULrGXuf/RRK3v3\nmmjd2sG8efm1nCoREfEVHrWpR0dHM3HiRKZNm0ZaWhrjx49nxIgR/Pbbb15Onpxq9+6yb9dvv5lw\nOODpp/Np1qyWEyUiIj6j0pL6vn37SElJYdWqVZx33nncfffdXHnllWzbto0pU6bw1ltv1UY6BYiO\ndrBrV1m93wzGjy+kd2+tgS4i0pBVWlK/7bbbMJlMvPrqqzzzzDNcddVVGIZBt27dVAVfyyZNKnuI\nWps2DqZNK6jl1IiIiK+pNKi/9957nHPOOURFRQHwxhtvcPz4cQBmzJjh3dRJCcOG2Vi8OI/One2Y\nzWA2OwkIcPL663kEBtZ16kREpK5VGtSnT59eYmGW/Px8HnjgAa8mSso3bJiN9etziY8Hu93g4YcL\n6NhR08CKiIgHQf3IkSOMHDnSvX3HHXdw9OhRryZKKvbSSwGsXg39+tkYPbqorpMjIiI+otKgXlRU\nVGICmu3bt1NUpEBSV3buNPHww4E0bw4LF+Zj0pyAIiJyQqW936dPn8748eM5duwYdrud8PBwHn/8\n8dpIm5zm2DEYM6Yx+fkGK1fCWWdp1jgRETmp0qDevXt3UlNTycrKwjAMmjVrxtatW2sjbXIKpxP+\n/vdGpKWZGDeukGuvtZKRUdepEhERX1JpUM/JyeHdd98lKysLcFXHv/POO3z++edeT5yc9NJLAbz7\nbgA9e9p48MECoOzZ5UREpOGqtEV20qRJ/PTTTyQnJ3P8+HHWr1/PrFmzaiFpUmzrVhMPPRRI8+YO\nXnghn4CAuk6RiIj4okqDekFBAY888gitW7dm6tSpvPbaa3z44Ye1kTYBsrLgzjsbY7PBc8/l06qV\n2tFFRKRsHvV+z83NxeFwkJWVRbNmzdi7d29tpK3BczhgwoTG7N1r4v77C+nbV9PAiohI+SptU7/2\n2mt58803ufHGGxkyZAjh4eGcffbZtZG2Bu+ZZ6ysWWOhTx8b999f9hSxIiIixSoN6gkJCRiGAUBM\nTAyHDh2iU6dOXk9YQ/fll2bmzrXSsqWD557Lx1zWOi4iIiKnqLT6/dTZ5KKioujcubM7yIt3pKcb\njB3bCMOAF17Ip0ULtaOLiEjlKi2pd+rUiYULF3LxxRcTcEq365iYGK8mrKGy22HcuEYcPGhi1qx8\nLrtM7egiIuKZSoP6rl27APjmm2/c+wzDUFCvQSkpFhYssLJ7t4lmzZwcOmRi8OAixo3TdLwiIuK5\nSoP6smXLaiMdDVZKioW77mrs3j50yNW0MWCADbVyiIhIVVQa1G+55ZYy29BXrFjhlQQ1NAsWlD0z\n3JIlVkaMsNVyakREpD6rNKhPmjTJ/bqoqIivvvqKoKAgryaqIdm9u+y+iuXtFxERKU+lQb1nz54l\ntnv16sWdd97ptQQ1NNHRDnbtKj1eLTraUQepERGR+qzSoH767HH79+/n119/9VqCGppJkwpLtKkX\nmzhRk82IiEjVVBrUR40a5X5tGAbBwcFMmDDBq4lqSIYNszF3roM9ewzMZrjgAgcTJxYybJja00VE\npGoqDerr1q3D4XBgMrnaeIuKikqMV6/I3Llz+f777zEMg8TERLp16+Z+b8WKFbz33nuYTCa6du3K\nP/7xD4qKipg2bRp//vknZrOZRx99lLZt21Yza/XDtm0m9uwxMXCgjWXL8uo6OSIiUo9V2hsrNTWV\n8ePHu7dHjBjB6tWrK73w5s2b2bNnD0lJScyZM4c5c+a438vJyWHp0qWsWLGCN954g7S0NL777jtW\nrVpFaGgob7zxBnfffTfz58+vZrbqj9dfdz0g3XKLxqSLiMiZqTSov/zyyzzxxBPu7ZdeeomXX365\n0gtv3LiRuLg4ADp06EB2djY5OTkABAQEEBAQQG5uLjabjby8PJo2bcrGjRuJj48H4IorrmDr1q3V\nylR9kZcHb78dQGSkg7g4VbeLiMiZqTSoO51OQkJC3NvBwcEezf2emZlJWFiYezs8PJyMjAwAAgMD\nueeee4iLi6Nfv350796d9u3bk5mZSXh4uCthJhOGYVBY6L8dxj74wEJ2tkFCQhEetmiIiIiUq9I2\n9a5duzJp0iR69uyJ0+nks88+o2vXrlX+IKfz5KIkOTk5LF68mNWrVxMcHMyoUaP48ccfKzynPGFh\nQVgsni1hFhERUvlBtejNN12/J0wIJCIisMrn+1p+zpTy49uUH9+m/Pi22spPpUH9wQcf5L333uOH\nH37AMAz+3//7fwwaNKjSC0dGRpKZmenePnjwIBEREQCkpaXRtm1bd6n8kksuYfv27URGRpKRkUHH\njh0pKirC6XRitZY941qxrKzcStMCri80I+OYR8fWhl9/NVi/PpgrrrDRrFkeJyoxPOZr+TlTyo9v\nU358m/Lj22o6PxU9IFRa/Z6Xl0dAQAAzZszgwQcfJDs7m7y8yntp9+rVi9TUVAB27NhBZGQkwcHB\nALRu3Zq0tDTy8/MB2L59O+eccw69evVyd8Jbv349l112WeW5q6feeMNV3z5ihDrIiYhIzai0pD51\n6lQuvfRS93Z+fj4PPPAAzz77bIXn9ejRgy5dupCQkIBhGMycOZPk5GRCQkKIj49nzJgxjBw5ErPZ\nzMUXX8wll1yC3W7nyy+/ZPjw4VitVubNm3fmOfRBNpsrqIeGOhk6VB3kRESkZlQa1I8cOcLIkSPd\n23fccQfr1q3z6OKTJ08usd2xY0f364SEBBISEkq8Xzw23d+tW2cmPd3EHXcU0rj0ZHIiIiLVUmn1\ne1FREWlpae7tbdu2UVSkKuMzsXy5q+r91lv1PYqISM2ptKQ+ffp0xo8fz7Fjx3A4HISFhfH444/X\nRtr8Unq6wZo1Frp1s3PhhVq0RUREak6lQb179+6kpqayf/9+Nm3aREpKCuPGjePzzz+vjfT5naSk\nAOx2QzPIiYhIjas0qH/33XckJyfzwQcf4HA4mD17NgMGDKiNtPkdpxNWrAigUSMn11+voC4iIjWr\n3Db1JUuWMGTIEO677z7Cw8N55513aNeuHVdffbXHC7pISRs3mvn1VxPXXGOjadO6To2IiPibckvq\nCxYs4LzzzuOhhx7i8ssvB/BoelgpX3EHOY1NFxERbyg3qG/YsIGUlBRmzpyJw+Fg2LBh6vV+BrKz\nYdUqC+3bO4iJsdd1ckRExA+VW/0eERHB2LFjSU1NZe7cufz+++/s27ePu+++m08++aQ20+gX3nkn\ngPx8gxEjilCFh4iIeEOl49QBLr30UubNm8dnn31G3759K51NTkpbsSIAs9nJzTertkNERLzDo6Be\nLDg4mISEBN4sXl5MPPLDDya2bTMTH28jKqryledERESqo0pBXapnxQrNICciIt6noO5leXmu9vSz\nznIQG6sOciIi4j0K6l6UkmIhJqYJR48aFBXB++9XOtePiIhItSnKeElKioW77jq5BNuhQ6YT23kM\nG6blVkVEpOappO4lCxZYy9y/cGHZ+0VERM6UgrqX7N5d9ldb3n4REZEzpQjjJeecU/ayqtHRWm5V\nRES8Q0HdS1q1Kns8+sSJhbWcEhERaSgU1L0gPd1g0yYzEREOOnWyY7E46dzZzuLF6iQnIiLeo97v\nXrB4cQCFhQZTpxYwcqQmnBERkdqhknoNy86GV16xEhnp4KabFNBFRKT2KKjXsFdesZKTY3DXXUU0\nalTXqRERkYZEQb0G5eXBCy8EEBLiZNQodYgTEZHapaBeg5KSAsjIMHHHHYWEhtZ1akREpKFRUK8h\nNhs8+6yVwEAnd96ptnQREal9Cuo15P33LezZYyIhoUhrpouISJ1QUK8BTicsWmTFZHIyfrza0kVE\npG4oqNeA9evN7Nhh5tprbbRvr1K6iIjUDa9OPjN37ly+//57DMMgMTGRbt26AZCens7kyZPdx+3d\nu5f777+foqIiFi5cSLt27QC44oorGDdunDeTWCMWLXKtvDZhgkrpIiJSd7wW1Ddv3syePXtISkoi\nLS2NxMREkpKSAIiKimLZsmUA2Gw2brvtNmJjY0lNTWXIkCFMnTrVW8mqcV9/beLLLy3Extq48EIt\n1iIiInXHa9XvGzduJC4uDoAOHTqQnZ1NTk5OqeNSUlIYOHAgTZo08VZSvOrpp12l9HvvVSldRETq\nlteCemZmJmFhYe7t8PBwMjIySh331ltvccMNN7i3N2/ezJgxYxg1ahQ7d+70VvJqxE8/mVi9OoD/\n+z87MTH2uk6OiIg0cLW2oIvTWboD2bfffsu5555LcHAwAN27dyc8PJy+ffvy7bffMnXqVN5///0K\nrxsWFoTFYvYoDRERIVVPeAWmTHH9njHDTGRkzV7bEzWdn7qm/Pg25ce3KT++rbby47WgHhkZSWZm\npnv74MGDRERElDhmw4YNxMTEuLc7dOhAhw4dALj44os5fPgwdrsds7n8oJ2VletReiIiQsjIOFaV\nLFTojz8MVqxoQnS0g8svz6WMSgivqun81DXlx7cpP75N+fFtNZ2fih4QvFb93qtXL1JTUwHYsWMH\nkZGR7hJ5sW3bttGxY0f39pIlS1i1ahUAu3fvJjw8vMKAXpcWL7ZisxlMmFCISQMDRUTEB3itpN6j\nRw+6dOlCQkIChmEwc+ZMkpOTCQkJIT4+HoCMjAyaN2/uPueaa65hypQprFy5EpvNxpw5c7yVvDNy\n+DAsWxZAq1YOrrvOVtfJERERAbzcpn7qWHSgRKkcKNVeftZZZ7mHuvmyOXMCyc01mD69AKu1rlMj\nIiLioorjKvr0UzPLllnp1MnOHXdo4RYREfEdCupVkJMDf/97I0wmJwsX5quULiIiPkVBvQrmzg3k\n999N3HNPIRddpNnjRETEtyioe+irr8y8+KKV886zM2WKZo8TERHfo6Dugbw8mDSpEYbhZMGCfBo1\nqusUiYiIlKag7oHHHw/kf/8zMXZsET17qtpdRER8k4J6JbZuNfHccwGcfbaDadMK6jo5IiIi5VJQ\nr0BBAUyc2AiHw2DBgnyKF5JLSbHQp08QLVsG06dPECkptTaFvoiISLkUjSrwr39Z+eknM7ffXkiv\nXq5V2FJSLNx1V2P3Mbt2mU9s5zFsmGaXExGRuqOSejm2bTOxcKGVNm0cPPTQyWr3BQvKHpy+cKEG\nrYuISN1SUC9DUZGr2t1uN3jyyXxOXYdm9+6yv7Ly9ouIiNQWRaIyPPOMle3bzQwfXkRsrL3Ee9HR\nZfd+L2+/iIhIbVFQP82PP5qYP99KVJSDRx7JL/X+pEllTzwzcaImpBERkbqloH4Km801yUxhocET\nT+TTtGnpY4YNs7F4cR6dO9uxWJx07mxn8WJ1khMRkbqn3u+nWLo0gK1bzVx3XRGDBtnLPW7YMJuC\nuIiI+ByV1E/x008m2rRxMGeOJpkREZH6RyX1U8yfX0BhYQGBgXWdEhERkapTSf0UhoECuoiI1FsK\n6iIiIn5CQV1ERMRPKKiLiIj4CQV1ERERP6GgLiIi4icU1EVERPyEgrqIiIifUFAXERHxEwrqIiIi\nfkJBXURExE8oqIuIiPgJry7oMnfuXL7//nsMwyAxMZFu3boBkJ6ezuTJk93H7d27l/vvv59BgwYx\nbdo0/vzzT8xmM48++iht27b1ZhJFRET8hteC+ubNm9mzZw9JSUmkpaWRmJhIUlISAFFRUSxbtgwA\nm83GbbfdRmxsLKtWrSI0NJT58+fz+eefM3/+fBYsWOCtJIqIiPgVr1W/b9y4kbi4OAA6dOhAdnY2\nOTk5pY5LSUlh4MCBNGnShI0bNxIfHw/AFVdcwdatW72VPBEREb/jtaCemZlJWFiYezs8PJyMjIxS\nx7311lvccMMN7nPCw8NdCTOZMAyDwsJCbyVRRETEr3i1Tf1UTqez1L5vv/2Wc889l+DgYI/POV1Y\nWBAWi9mjNEREhHh0XH2h/Pg25ce3KT++TfmpHq8F9cjISDIzM93bBw8eJCIiosQxGzZsICYmpsQ5\nGRkZdOzYkaKiIpxOJ1artcLPycrK9Sg9EREhZGQcq0IOfJvy49uUH9+m/Pg25afy65XHa9XvvXr1\nIjU1FYAdO3YQGRlZqkS+bds2OnbsWOKc1atXA7B+/Xouu+wybyVPRETE73itpN6jRw+6dOlCQkIC\nhmEwc+ZMkpOTCQkJcXeGy8jIoHnz5u5zhgwZwpdffsnw4cOxWq3MmzfPW8kTERHxO15tUz91LDpQ\nolQO8P7775fYLh6bLiIiIlWnGeVERET8hIK6iIiIn1BQFxER8RMK6iIiIn5CQV1ERMRPKKiLiIj4\nCQV1ERERP6GgLiIi4icU1EVERPyEgrqIiIifUFAXERHxEwrqIiIifkJBXURExE8oqIuIiPgJBXUR\nERE/oaAuIiLiJxTURURE/ISCuoiIiJ9QUBcREfETCuoiIiJ+QkFdRETETyioi4iI+AkFdRERET+h\noC4iIuInFNRFRET8hIK6iIiIn1BQFxER8RMK6iIiIn7C4s2Lz507l++//x7DMEhMTKRbt27u9/bv\n38/f//53ioqK6Ny5M4888gibNm1i4sSJnH/++QBER0czY8YMbyZRRETEb3gtqG/evJk9e/aQlJRE\nWloaiYmJJCUlud+fN28eo0ePJj4+nocffpg///wTgJ49e7Jo0SJvJUtERMRvea36fePGjcTFxQHQ\noUMHsrOzycnJAcDhcLBlyxZiY2MBmDlzJq1atfJWUkRERBoErwX1zMxMwsLC3Nvh4eFkZGQAcPjw\nYZo0acKjjz7K8OHDmT9/vvu4X375hbvvvpvhw4fzxRdfeCt5IiIifserbeqncjqdJV6np6czcuRI\nWrduzdixY9mwYQOdOnViwoQJDB48mL179zJy5Eg++ugjrFZrudcNCwvCYjF7lIaIiJAzzocvUX58\nm/Lj25Qf36b8VI/XgnpkZCSZmZnu7YMHDxIREQFAWFgYrVq1ol27dgDExMTw888/07dvX4YMGQJA\nu3btaNGiBenp6bRt27bcz8nKyvUoPRERIWRkHKtudnyO8uPblB/fpvz4NuWn8uuVx2vV77169SI1\nNRWAHTt2EBkZSXBwMAAWi4W2bdvy22+/ud9v37497733HkuXLgUgIyODQ4cOERUV5a0kioiI+BWv\nldR79OhBly5dSEhIwDAMZs6cSXJyMiEhIcTHx5OYmMi0adNwOp1ER0cTGxtLbm4ukydP5uOPP6ao\nqIhZs2ZVWPUuIiIiJxnOUxu76yFPqzRUnePblB/fpvz4NuXHt/lF9buIiIjULgV1ERERP6GgLiIi\n4icU1EVERPyEgrqIiIifUFAXERHxEwrqJ6SkWOjTJ4iWLYPp0yeIlJRam0FXRESkRihy4Qrod93V\n2L29a5f5xHYew4bZ6i5hIiIiVaCSOrBgQdmz1i1cqNnsRESk/lBQB3bvLvtrKG+/iIiIL1LUAqKj\nHVXaLyIi4osU1IFJk6Za5tIAAAi3SURBVArL3D9xYtn7RUREfJGCOjBsmI3Fi/Po3NmOxeKkc2c7\nixerk5yIiNQv6v1+wrBhNgVxERGp11RSFxER8RMK6iIiIn5CQV1ERMRPKKiLiIj4CQV1ERERP6Gg\nLiIi4icU1EVERPyEgrqIiIifUFAXERHxE4bT6XTWdSJERETkzKmkLiIi4icU1EVERPyEgrqIiIif\nUFAXERHxEwrqIiIifkJBXURExE9Y6joB3jZ37ly+//57DMMgMTGRbt261XWSqm3Tpk1MnDiR888/\nH4Do6GhmzJhRx6mqnt27dzN+/Hhuv/12br31Vvbv388DDzyA3W4nIiKCJ554AqvVWtfJ9Njp+Zk2\nbRo7duygWbNmAIwZM4a+ffvWbSKr4PHHH2fLli3YbDbuuusuLrzwwnp9f07Pz7p16+rt/cnLy2Pa\ntGkcOnSIgoICxo8fT8eOHevt/SkrP6mpqfX2/hTLz89n6NChjB8/npiYmFq7P34d1Ddv3syePXtI\nSkoiLS2NxMREkpKS6jpZZ6Rnz54sWrSorpNxRnJzc5k9ezYxMTHufYsWLeKWW25h8ODBPPXUU7z9\n9tvccsstdZhKz5WVH4C///3v9OvXr45SVX1fffUVP//8M0lJSWRlZTFs2DBiYmLq7f0pKz+XX355\nvb0/69evp2vXrtx5553s27eP0aNH06NHj3p7f8rKz8UXX1xv70+x5557jqZNmwK1+/fNr6vfN27c\nSFxcHAAdOnQgOzubnJycOk6VWK1WlixZQmRkpHvfpk2b6N+/PwD9+vVj48aNdZW8KisrP/XZpZde\nysKFCwEIDQ0lLy+vXt+fsvJjt9vrOFXVN2TIEO68804A9u/fT1RUVL2+P2Xlp75LS0vjl19+cdcu\n1Ob98eugnpmZSVhYmHs7PDycjIyMOkzRmfvll1+4++67GT58OF988UVdJ6daLBYLjRo1KrEvLy/P\nXR3VvHnzenWfysoPwPLlyxk5ciT33Xcfhw8froOUVY/ZbCYoKAiAt99+m6uuuqpe35+y8mM2m+vt\n/SmWkJDA5MmTSUxMrNf3p9ip+YH6+/8H4LHHHmPatGnu7dq8P35d/X66+j4j7jnnnMOECRMYPHgw\ne/fuZeTIkXz00Uf1pu3MU/X9PgFce+21NGvWjE6dOvHCCy/wzDPP8NBDD9V1sqpk7dq1vP3227z0\n0ksMGDDAvb++3p9T87N9+/Z6f39WrlzJrl27mDJlSol7Ul/vz6n5SUxMrLf35z//+Q8XXXQRbdu2\nLfN9b98fvy6pR0ZGkpmZ6d4+ePAgERERdZiiMxMVFcWQIUMwDIN27drRokUL0tPT6zpZNSIoKIj8\n/HwA0tPT631VdkxMDJ06dQIgNjaW3bt313GKquazzz7j+eefZ8mSJYSEhNT7+3N6furz/dm+fTv7\n9+8HoFOnTtjtdpo0aVJv709Z+YmOjq6392fDhg18/PHH3HTTTbz11lv8+9//rtX/P34d1Hv16kVq\naioAO3bsIDIykuDg4DpOVfW99957LF26FP5/e/cT0vQfx3H8Ofc1aeAlDcN5MSxDRNLAg0yGngJv\nioegDpEXxRFo6kK35cVtJSF6UtLL/FNCRJeCIAgSTVBkonWVNDrIgtYfFq7xO4xfJK0f9rMa++71\nOH4Gn33evA/v7/vz2b4fYHd3l0gkYorzJ4C6urpvuXry5An19fVpXtHhuFwutre3geR52r//WMgE\nHz584ObNm4yPj3/79XEm5ydVPJmcn5WVFaampoDkEePnz58zOj+p4vF6vRmbn5GREe7fv8/8/Dyt\nra10dHT81fyY/pa24eFhVlZWsFgs+Hw+zpw5k+4l/W8fP37k2rVrRKNR9vb26OzsxOl0pntZv2xj\nY4NgMMibN28wDIOioiKGh4dxu918+fKF4uJi/H4/ubm56V7qgaSK5+LFi0xMTHD06FFsNht+v5+C\ngoJ0L/VA7t27x9jYGKWlpd/GAoEAAwMDGZmfVPE0NzczPT2dkfmJxWL09/fz9u1bYrEYnZ2dVFZW\n0tfXl5H5SRWPzWbj1q1bGZmf742NjWG323E4HH8tP6Yv6iIiItnC1NvvIiIi2URFXURExCRU1EVE\nRExCRV1ERMQkVNRFRERMIqveKCciSTs7O5w/f57q6up9406nk7a2tkPPv7y8zMjICHNzc4eeS0QO\nTkVdJEsdO3aMUCiU7mWIyG+koi4i+1RUVNDR0cHy8jKfPn0iEAhw+vRpwuEwgUAAwzCwWCx4vV7K\nysrY2trC4/GQSCTIy8vD7/cDkEgk8Pl8vHr1iiNHjjA+Pg5Ad3c30WiUeDxOQ0MD7e3t6QxXxFR0\npi4i+3z9+pVTp04RCoW4cOECo6OjAPT29nL9+nVCoRCXL19mcHAQAJ/Px5UrV5iZmaGlpYXHjx8D\nyesnXS4X8/PzGIbBwsICi4uLxONxZmdnuXv3LjabjUQikbZYRcxGnbpIlnr37h2XLl3aN9bT0wOA\nw+EAoKamhsnJSaLRKJFIhKqqKgBqa2vp6uoCYH19ndraWgCampqA5Jn6yZMnKSwsBODEiRNEo1Ea\nGxsZHR3l6tWrOJ1OWltbyclRbyHyu6ioi2Sp/zpT//7t0RaLBYvF8tPPgZTdttVq/WGsoKCAhw8f\nsra2xtOnT2lpaeHBgwcp76MXkV+nR2QR+cGLFy8AWF1dpby8nPz8fI4fP044HAZgaWmJs2fPAslu\n/vnz5wA8evSI27dv/3TehYUFnj17xrlz5+jt7cVmsxGJRP5wNCLZQ526SJZKtf1eUlICwMuXL5mb\nm+P9+/cEg0EAgsEggUAAq9VKTk4ON27cAMDj8eDxeJidncUwDIaGhnj9+nXK7ywtLcXtdnPnzh2s\nVisOhwO73f7nghTJMrqlTUT2KS8vZ3NzE8PQM79IptH2u4iIiEmoUxcRETEJdeoiIiImoaIuIiJi\nEirqIiIiJqGiLiIiYhIq6iIiIiahoi4iImIS/wC7TKruNebjgQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7FBpTc_rXGvQ"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracy of model2 is 87%. Using Embedding layer instead of one-hot layer improved the performance."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "--020hfG6rN2"
      },
      "cell_type": "markdown",
      "source": [
        "### Using pre-trained word embeddings"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "J4gBeOyi4gkM"
      },
      "cell_type": "markdown",
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f_PypdqG9Iis",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZcIZ3dq59bCh"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gembn7VM3ex8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.initializers import Constant\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HGxciLK4-xOr"
      },
      "cell_type": "markdown",
      "source": [
        "We freeze the weights. To create the model:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PZCPUM0W_Drc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wordToIndex, indexToWord, wordToGlove = readGloveFile(\"drive/glove.6B.300d.txt\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ym24vP3GILpR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "preTrainedEmbeddingLayer = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AUm4FaOoImf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "24699359-2a06-4a5e-df74-ab3c156796b1"
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "model3 = Sequential()\n",
        "model3.add(preTrainedEmbeddingLayer(VOCAB_SIZE,64))\n",
        "model3.add(GlobalAveragePooling1D())\n",
        "model3.add(Dense(16, activation='relu'))\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "model3.summary()\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-550acbbda5df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreTrainedEmbeddingLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __call__() takes exactly 2 arguments (3 given)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "M-bZ5SCHiIMl"
      },
      "cell_type": "markdown",
      "source": [
        "### Adding another hidden layer to the network"
      ]
    },
    {
      "metadata": {
        "id": "jszxin21IKAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZbZ6UBDfbjea"
      },
      "cell_type": "markdown",
      "source": [
        "In model4, we only add another dense layer to see if that improves the performance."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Vw0le1YjDdCa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "model4 = Sequential()\n",
        "model4.add(preTrainedEmbeddingLayer(VOCAB_SIZE,64,input_length = MAX_SEQUENCE_LENGTH))\n",
        "model4.add(GlobalAveragePooling1D())\n",
        "model4.add(Dense(16, activation='relu'))\n",
        "model4.add(Dense(16, activation='relu'))\n",
        "model4.add(Dense(1, activation='sigmoid'))\n",
        "model4.summary()\n",
        "\n",
        "model4.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LrZlKyr-Igch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QtsdVeW7UgCu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history3.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Kx--Ytk3ZbLo"
      },
      "cell_type": "markdown",
      "source": [
        "The accuracy of model3 with an additional layer is 85%. Adding more layers can help you to extract more features. But we can do that upto a certain extent. After some point, instead of extracting features, we tend to overfit the data. Overfitting can lead to errors in some or the other form like false positives. It is not easy to choose the number of units in a hidden layer or the number of hidden layers in a neural network. For many applications, one hidden layer is enough. As a general rule, the number of units in that hidden layer is between the number of inputs and the number of outputs.\n",
        " The best way to decide on the number of units and hidden layers is to try various parameters. Train several neural networks with different numbers of hidden layers and neurons, and monitor the performance of them. You will have to experiment using a series of different architectures. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gn2GSV4ioyO2"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XYC6DykEox2w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GsCJ01StlgCx"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial is substantially based on this document:\n",
        "https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
        "\n",
        "To read more about Sequential APIs you can go to: https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "The one-hot word vector layer is taken from:\n",
        "https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jL0UovfaE9GE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}