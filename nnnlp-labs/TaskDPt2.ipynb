{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.utils import plot_model, vis_utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import string\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has been preprocessed by removing all the \" characters: sed -i 's/\"//g' *.txt\n",
    "# as this caused issues reading the data as a csv file. \n",
    "# Also had to remove a blank line from subtask A 2016 test data \n",
    "# TODO instead change the quote char in the read_csv call\n",
    "\n",
    "# Load the data\n",
    "fileGlob = glob.glob('./task2Data/*A.arabic.txt')\n",
    "\n",
    "traindf = pd.concat([pd.read_csv(f, sep='\\t', header=None, keep_default_na=False) for f in fileGlob], ignore_index = True)\n",
    "traindf.columns = ['id','label','raw']\n",
    "#traindf = traindf.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "Convert the text into vector my using a pre-trained system. \n",
    "Convert text into a vector by using a NN to train the embeddings. \n",
    "\n",
    "Looks like i need to handle the weird character replacement that's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do some preprocessing on the data to remove stop words, punctuation and probably stem the words too. \n",
    "# Need to handle the /u002c and other unicdoe character artifacts that are happening. \n",
    "# If removing punctuation then may want to simply remove them \n",
    "# but if expanding contractinons will need to convert them first. - a library exists to do this\n",
    "# Need to check to see what's happening with emojis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet, stop_words):\n",
    "    # Handle utf8 unicode problems\n",
    "    #print(tweet)\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    tweet = tweet.encode('utf8').decode('unicode_escape', 'ignore') \n",
    "    #tweet = contractions.fix(tweet)\n",
    "    tweetLine = word_tokenize(tweet)\n",
    "    # remove all tokens that are not alphabetic or stopwords, also lower the words\n",
    "    tweetLine = [word.lower() for word in tweetLine if word not in stop_words and word not in string.punctuation]\n",
    "    return tweetLine\n",
    "\n",
    "stop_words = stopwords.words('arabic')\n",
    "\n",
    "traindf['text'] = traindf.apply(lambda row: preprocess(row['raw'], stop_words),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2678\n",
      "['shihanh010', 'Ã¸Â¨Ã¹\\x88Ã¸Â´', 'Ã¹\\x88Ã¸Â§Ã¹\\x84Ã¸Â§', 'Ã¸Â§Ã¹\\x88Ã¸Â¨Ã¸Â§Ã¹', 'Ã¸Â§', 'face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy', ':face_with_tears_of_joy']\n",
      "140\n",
      "['Ã¯Âº\\x87Ã¯Âº', 'Â«', 'Ã¯Âº\\x8d', 'Ã¯', 'Â»', 'Â§Ã¯', 'Â»', '\\x84Ã¯', 'Â»', '\\x96', 'Ã¯Âº\\x8dÃ¯', 'Â»', '\\x9fÃ¯ÂºÂ°Ã¯', 'Â»', '\\x8bÃ¯', 'Â»', 'Â´Ã¯', 'Â»', 'Â¢', 'Ã¯', 'Â»', '\\x93Ã¯', 'Â»', 'Â¼', 'Ã¯Âº\\x97Ã¯Âº', 'Ã¯Âº\\x92Ã¯', 'Â»', 'ÂªÃ¹\\x8f', 'Ã¯', 'Â»', '\\x93Ã¯ÂºÂ¨Ã¯', 'Â»', 'Â´Ã¯ÂºÂ®Ã¹\\x8c', 'Ã¯', 'Â»', 'Â£Ã¯', 'Â»', 'Â¦', 'Ã¯Âº\\x87Ã¯Âº\\x9fÃ¯Âº\\x8eÃ¯Âº\\x91Ã¯Âº\\x98Ã¯', 'Â»', 'Âª', 'Ã¯Âº\\x8dÃ¯', 'Â»', '\\x9fÃ¯', 'Â»', '\\x8cÃ¯ÂºÂ´Ã¯', 'Â»', 'Â´Ã¯ÂºÂ®Ã¯', 'Â»', 'Â±Ã¯', 'Â»', '\\xadÃ¯', 'Â»', 'Â»', 'Ã¯Âº\\x97Ã¯ÂºÂ®Ã¯', 'Â»', '\\x9bÃ¯', 'Â»', 'Â¦', 'Ã¯', 'Â»', '\\x8bÃ¯', 'Â»', 'Ã¯', 'Â»', 'Â°', 'Ã¯Âº\\x87Ã¯', 'Â»', 'Â³Ã¯ÂºÂ®Ã¯Âº\\x8dÃ¯', 'Â»', 'Â¥', 'Ã¯', 'Â»', 'Â³Ã¯', 'Â»', 'Â®Ã¯', 'Â»', 'Â£Ã¯Âº\\x8eÃ¹\\x8b', 'Ã¯', 'Â»', '\\xadÃ¯', 'Â»', 'Â»', 'Ã¯Âº\\x97Ã¯', 'Â»', 'Â®Ã¯', 'Â»', 'Â§Ã¯', 'Â»', 'Â²', 'Ã¯Âº\\x91Ã¯', 'Â»', 'Ã¯', 'Â»', 'Â´Ã¯ÂºÂ®', 'Ã¯Âº\\x83Ã¯', 'Â»', '\\xad', 'Ã¯Âº\\x9fÃ¯', 'Â»', 'Â®Ã¯', 'Â»', 'Â¥', 'Ã¯', 'Â»', '\\x9bÃ¯', 'Â»', 'Â´Ã¯ÂºÂ®Ã¯', 'Â»', 'Â±Ã¯Âº\\x83Ã¯Âº\\xadÃ¯', 'Â»', 'Â¯', 'Ã¯Âº\\x8dÃ¯', 'Â»', '\\x9fÃ¯', 'Â»', 'Â¤Ã¯ÂºÂ¨Ã¯', 'Â»', 'Ã¯', 'Â»', 'Â®Ã¯', 'Â»', '\\x89', 'Ã¯', 'Â»', '\\x93Ã¯', 'Â»', 'Â²', 'Ã¯Âº\\x8dÃ¯', 'Â»', '\\x9fÃ¯Âº\\x98Ã¯', 'Â»', 'Ã¯', 'Â»', '\\x94Ã¯Âº\\x8eÃ¯ÂºÂ¯', 'Ã¯', 'Â»', 'Â³Ã¯', 'Â»', 'Â¬Ã¯ÂºÂ¬']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to ensure tweets are tweet length\n",
    "maxi = 0\n",
    "for text in traindf.text:\n",
    "    length = len(' '.join(text))\n",
    "    if length > maxi:\n",
    "        maxi = length\n",
    "        sanityCheck = text\n",
    "print(maxi)\n",
    "print(sanityCheck)\n",
    "\n",
    "maxi = 0\n",
    "for text in traindf.text:\n",
    "    length = len(text)\n",
    "    if length > maxi:\n",
    "        maxi = length\n",
    "        sanityCheck = text\n",
    "print(maxi)\n",
    "print(sanityCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021    @shihanh010 Ø¨ÙˆØ´ ÙˆØ§Ù„Ø§ Ø§ÙˆØ¨Ø§Ù…Ø§ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
      "Name: raw, dtype: object\n",
      "1021    [shihanh010, Ã¸Â¨Ã¹ÂˆÃ¸Â´, Ã¹ÂˆÃ¸Â§Ã¹Â„Ã¸Â§, Ã¸Â§Ã¹ÂˆÃ¸Â¨Ã¸Â§Ã¹, Ã¸Â§, face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, :face_with_tears_of_joy, ...]\n",
      "Name: text, dtype: object\n",
      "1021    neutral\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 10000\n",
    "sample = traindf.loc[traindf.id == 785620354970574848]\n",
    "\n",
    "print(sample.raw)\n",
    "print(sample.text)\n",
    "print(sample.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1021', '@', 'shihanh010', 'Ã˜Â¨Ã™\\x88Ã˜Â´', 'Ã™\\x88Ã˜Â§Ã™\\x84Ã˜Â§', 'Ã˜Â§Ã™\\x88Ã˜Â¨Ã˜Â§Ã™', 'Ã˜Â§', ':', 'face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':', ':face_with_tears_of_joy', ':']\n"
     ]
    }
   ],
   "source": [
    "tweet = sample.raw.to_string()\n",
    "tweet = emoji.demojize(tweet)\n",
    "tweet = tweet.encode('utf8').decode('unicode_escape', 'ignore') \n",
    "tweetLine = word_tokenize(tweet)\n",
    "print(tweetLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index-word relationship\n",
    "word2idx = {'<PAD>': 0, '<UNK>' : 1, }\n",
    "idx2word ={}\n",
    "sents_as_ids = []\n",
    "for line in traindf.text:\n",
    "    sentId = []\n",
    "    for word in line:\n",
    "        if word in word2idx:\n",
    "            sentId.append(word2idx[word])\n",
    "            continue\n",
    "        count = len(word2idx)\n",
    "        word2idx[word] = count\n",
    "        idx2word[count] = word\n",
    "        sentId.append(count)\n",
    "    sents_as_ids.append(sentId)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTextToNumSeq(text, word2idx,MAXIMUM_LENGTH):\n",
    "    numSeq = []\n",
    "    for word in text:\n",
    "        if word in word2idx:\n",
    "            numSeq.append(word2idx[word])\n",
    "        else:\n",
    "            # If unseen put in unknown\n",
    "            numSeq.append(1) \n",
    "                \n",
    "    numSeq = pad_sequences([numSeq],MAXIMUM_LENGTH )\n",
    "    return numSeq\n",
    "\n",
    "MAXIMUM_LENGTH = 50 # Motivated because max sequence of words i had was 32\n",
    "\n",
    "traindf['numSeq'] = traindf.apply(lambda row: convertTextToNumSeq(row['text'], word2idx, MAXIMUM_LENGTH),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     368\n",
      "negative    285\n",
      "positive    186\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(traindf.numSeq, traindf.label, stratify=traindf.label, random_state =2)\n",
    "print(y_val.value_counts())\n",
    "x_train = np.array([x for y in x_train for x in y]).reshape(len(x_train),MAXIMUM_LENGTH)\n",
    "x_val = np.array([x for y in x_val for x in y]).reshape(len(x_val),MAXIMUM_LENGTH)\n",
    "\n",
    "#Y data is categorical therefore must be converted to a vector\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "y_train = onehot_encoder.fit_transform(np.array(y_train).reshape(len(y_train),1))\n",
    "y_val = onehot_encoder.transform(np.array(y_val).reshape(len(y_val),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           3000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 3,080,703\n",
      "Trainable params: 3,080,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2516 samples, validate on 839 samples\n",
      "Epoch 1/5\n",
      "2516/2516 [==============================] - 3s 1ms/step - loss: 1.0723 - acc: 0.4356 - val_loss: 1.0561 - val_acc: 0.4410\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.44100, saving model to task2Weights.best.hdf5\n",
      "Epoch 2/5\n",
      "2516/2516 [==============================] - 2s 876us/step - loss: 0.9980 - acc: 0.5505 - val_loss: 1.0348 - val_acc: 0.4720\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.44100 to 0.47199, saving model to task2Weights.best.hdf5\n",
      "Epoch 3/5\n",
      "2516/2516 [==============================] - 2s 892us/step - loss: 0.7206 - acc: 0.6737 - val_loss: 1.0496 - val_acc: 0.5578\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.47199 to 0.55781, saving model to task2Weights.best.hdf5\n",
      "Epoch 4/5\n",
      "2516/2516 [==============================] - 2s 884us/step - loss: 0.3077 - acc: 0.9173 - val_loss: 1.2029 - val_acc: 0.5316\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.55781\n",
      "Epoch 5/5\n",
      "2516/2516 [==============================] - 2s 888us/step - loss: 0.1375 - acc: 0.9801 - val_loss: 1.2949 - val_acc: 0.4887\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.55781\n"
     ]
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "weightsFilePath=\"task2Weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit(x_train,y_train,epochs=5,batch_size=128,validation_data=(x_val, y_val),callbacks=[checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracies')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With stop word removal, demojizing, string.punctuation word remova, vocab size 30000, padding at 50, get 0.56 val accuracy \n",
    "# decresing vocab size to 5000 causes bad results and strange effects\n",
    "# adding dropout 0.1 between embedding and lstm made it worse by 2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the model with the best val accuracy\n",
    "model.load_weights(weightsFilePath)\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in y_pred])\n",
    "y_pred_text = onehot_encoder.inverse_transform(y_pred)\n",
    "y_val_text = onehot_encoder.inverse_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val_text, y_pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[160 124   1]\n",
      " [ 69 294   5]\n",
      " [ 25 147  14]]\n",
      "0.5614035087719298 0.7989130434782609 0.07526881720430108\n"
     ]
    }
   ],
   "source": [
    "# Rows are the actual, columns are the predicted.  negative, neutral, positve\n",
    "print(cm)\n",
    "recall1 = cm[0][0] / (cm[0][0] + cm[0][1] + cm[0][2])\n",
    "recall2 = cm[1][1] / (cm[1][0] + cm[1][1] + cm[1][2])\n",
    "recall3 = cm[2][2] / (cm[2][0] + cm[2][1] + cm[2][2])\n",
    "print(recall1,recall2,recall3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5578069129916567"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cm[0][0] + cm[1][1] + cm[2][2])/sum(sum(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "testdf = pd.read_csv('./SemEval2017-task4-test/SemEval2017-task4-test.subtask-A.arabic.txt', sep='\\t', header=None, keep_default_na=False)\n",
    "testdf.columns = ['id','label','raw']\n",
    "\n",
    "testdf['text'] = testdf.apply(lambda row: preprocess(row['raw'], stop_words),axis=1)\n",
    "testdf['numSeq'] = testdf.apply(lambda row: convertTextToNumSeq(row['text'], word2idx, MAXIMUM_LENGTH),axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     2363\n",
      "negative    2221\n",
      "positive    1514\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_test = testdf['numSeq']\n",
    "y_test = testdf['label']\n",
    "\n",
    "# Prelim analysis to indicate class imbalance\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Onehot encode the y data\n",
    "y_test = onehot_encoder.transform(np.array(y_test).reshape(len(y_test),1))\n",
    "x_test = np.array([x for y in x_test for x in y]).reshape(len(x_test),MAXIMUM_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and prepare data for confusion matrix\n",
    "y_testpred = model.predict(x_test)\n",
    "y_testpred = np.array([[1 if i == max(sc) else 0 for i in sc] for sc in y_testpred])\n",
    "y_testpred_text = onehot_encoder.inverse_transform(y_testpred)\n",
    "y_test_text = onehot_encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageFScore(cm):\n",
    "    (noClasses,_) = cm.shape\n",
    "    fsum = 0\n",
    "    for i in range(noClasses):\n",
    "        correct = cm[i][i]\n",
    "        rowTotal = sum(cm[i])\n",
    "        colTotal = sum(cm[:,i])\n",
    "        recall = correct / rowTotal\n",
    "        precision = correct / colTotal\n",
    "        denominator = precision + recall if precision + recall > 0 else 1\n",
    "        f1 = 2*precision*recall / denominator\n",
    "        fsum += f1\n",
    "    return fsum/noClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 926 1288    7]\n",
      " [ 461 1891   11]\n",
      " [ 275 1226   13]]\n",
      "Test Accuracy: 0.46408658576582484\n",
      "Negative, neutral, positive recall: 0.41692931112111664,0.8002539145154465,0.008586525759577279\n",
      "Average fscore: 0.3508618122549789\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix and get some key information from it. \n",
    "cm = confusion_matrix(y_test_text, y_testpred_text, labels=['negative','neutral','positive'])\n",
    "\n",
    "print(cm)\n",
    "testAccuracy = (cm[0][0] + cm[1][1] + cm[2][2])/sum(sum(cm))\n",
    "print(f\"Test Accuracy: {testAccuracy}\")\n",
    "recall1 = cm[0][0] / (cm[0][0] + cm[0][1] + cm[0][2])\n",
    "recall2 = cm[1][1] / (cm[1][0] + cm[1][1] + cm[1][2])\n",
    "recall3 = cm[2][2] / (cm[2][0] + cm[2][1] + cm[2][2])\n",
    "print(f\"Negative, neutral, positive recall: {recall1},{recall2},{recall3}\")\n",
    "print(f\"Average fscore: {averageFScore(cm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix shows prediction in columns of negative, neutral and positive. Groundtruth are in rows of negative, neutral and positive. \n",
    "Test accuracy is 46.4%. We can see from the confusion matrix that the classifier particularly struggles to classify when the text is neutral, its recall of positive samples is 0.85%. Therefore to help improve this model we could try concentrating on features that help to classify positive text. The fact that the model is worse than guessing when the data is positive is concerning and suggests there is a lot of room for improvement. We would expect that the neutral class is the hardest to classify though as it's most similar to the two other classes. Both the training and test data have a class imbalance. Future work could try addressing this class imbalance. I think large difference between the neutral and the positive recalls shows the model struggles to distiguish between them and has placed its decision boundary to get more of the neutral cases right. The positive class si the smallest in both the test and the training data, which may explain why the model has done this. If we wanted to identify neutral tweets then this model would be very useful but most applications would want to classify the negative or the positive tweets, so the future work may want to incentivise the model to improve recall on negative and positive classes, either by balancing the data or creating a custom loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Need to check whether i should be running the test data against a model that has been trained on all the training data. If so: Also need to work out how many epochs that model would have to be trained for.\n",
    "\n",
    "If not: Then i need to run the test data against the training model that has been trained for the number of epochs that gave the highest validation accuracy. \n",
    "\n",
    "Should also try using a CNN on the arabic data rather than LSTM because the CNN should require much less data to optimise than the CNN. The lack of arabic data is a real challenge for this model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eenlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
