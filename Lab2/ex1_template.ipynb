{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Must be using Python 3. This notebook was created on 3.6.5\")\n",
    "\n",
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        # We want to skip the first line as it is the column title\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "            preprocessedData.append((Id, preProcess(Text), Label))\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    \n",
    "    id = int(reviewLine[0])\n",
    "    text = str(reviewLine[8])\n",
    "    label = str(reviewLine[1])\n",
    "    return (id, text, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/leem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    #Initialisation steps:\n",
    "    new_text = []\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # remove html tags\n",
    "    text = BeautifulSoup(text,\"lxml\").get_text()\n",
    "    # replace i'd with i would and other similar contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # split by whitespace and remove non-alphanumeric characters like punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        # Loop through words and remove capital letters\n",
    "        new_token = token.lower()\n",
    "        \n",
    "        # If token is a stop word we don't want to include it\n",
    "        if new_token in stop_words:\n",
    "            continue;\n",
    "        \n",
    "        # Use the porter algorithm to stem the word e.g. rationalise -> rational\n",
    "        new_text.append(porter.stem(new_token))\n",
    "    \n",
    "    # Should return a list of tokens\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVec = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        # For each token, we want to increment the global feature count\n",
    "        if token in featureDict:\n",
    "            featureDict[token] += 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "        # We want to add the token to the dictionary to create a simple vector\n",
    "        if token in featureVec:\n",
    "            featureVec[token] += 1\n",
    "        else:\n",
    "            featureVec[token] = 1\n",
    "\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    #print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(max_iter=1000,C=globalCostCoeff))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    # Shuffle the data so we will split the data randomly\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    # Perform preprocessing and vectorisation on the data\n",
    "    data = prepData(dataset)\n",
    "    \n",
    "    # Initialise an empty confusion matrix\n",
    "    totalConfusionMatrix = np.zeros((2,2))\n",
    "    \n",
    "    # Get the length of the testData slice \n",
    "    testDataLen = int(len(data)/folds)\n",
    "    \n",
    "    # For each slice of the data\n",
    "    for i in range(0,len(data),testDataLen):\n",
    "        testEnd = i+testDataLen\n",
    "        \n",
    "        #Slice up the data into test and training\n",
    "        testingData = data[i:testEnd]\n",
    "        trainingData = data[:i] + data[testEnd:]\n",
    "        \n",
    "        #print(\"Split:\", i, \"Len train data:\",len(trainingData),\"Len test data:\",len(testingData),\"Scores below\")\n",
    "        \n",
    "        # Train the model then get its predictions \n",
    "        classifier = trainClassifier(trainingData)\n",
    "        yPred = [predictVector(x[0], classifier) for x in testingData]\n",
    "        \n",
    "        # Get the true labels\n",
    "        yTrue = [x[1] for x in testingData]\n",
    "        \n",
    "        # Get a confusion matrix to describe the performance of this fold\n",
    "        confusionMatrix = confusion_matrix(yTrue,yPred)\n",
    "        \n",
    "        # Add to the overall confusion matrix. This will allow us to get average performance metrics later\n",
    "        totalConfusionMatrix = np.add(confusionMatrix, totalConfusionMatrix)\n",
    "        \n",
    "    print(totalConfusionMatrix)\n",
    "    \n",
    "    # Get the average performance metrics from overall confusion matrix\n",
    "    averagePrecision = totalConfusionMatrix[0][0] / (totalConfusionMatrix[0][0] + totalConfusionMatrix[0][1])\n",
    "    averageRecall = totalConfusionMatrix[0][0] / (totalConfusionMatrix[0][0] + totalConfusionMatrix[1][0])\n",
    "    averageF1Score = 2*averagePrecision*averageRecall / (averagePrecision + averageRecall)\n",
    "    averageAccuracy = (totalConfusionMatrix[0][0] + totalConfusionMatrix[1][1])/ float(np.sum(totalConfusionMatrix))\n",
    "    \n",
    "    # Return results in a tuple\n",
    "    cv_results = (averagePrecision,averageRecall,averageF1Score,averageAccuracy)\n",
    "    return cv_results\n",
    "\n",
    "def prepData(data):\n",
    "    newData = []\n",
    "    for (_, Text, Label) in data:\n",
    "        newData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "# Takes in a list of strings as review samples and returns list of predictions\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "# Takes in string as a review sample and returns a prediction\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))\n",
    "\n",
    "# Easier to pass the vector to this predict func\n",
    "def predictVector(textVec, classifier):\n",
    "    return classifier.classify(textVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "24768\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is a great source of information for those studying to take the medical administrative assistant exam, as it's incredibly knowledgeable and full of insightful resources. The author really knows her stuff! There are so many informative questions to help study from, and one of the best study guides I've ever looked into. Well done!\n",
      "After preprocessing\n",
      "['book', 'great', 'sourc', 'inform', 'studi', 'take', 'medic', 'administr', 'assist', 'exam', 'incred', 'knowledg', 'full', 'insight', 'resourc', 'author', 'realli', 'know', 'stuff', 'mani', 'inform', 'question', 'help', 'studi', 'one', 'best', 'studi', 'guid', 'ever', 'look', 'well', 'done']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "#Cell for sense-checking the preprocessing function\n",
    "example = rawData[randint(0,len(rawData))][1]\n",
    "print(example)\n",
    "print(\"After preprocessing\")\n",
    "print(preProcess(example))\n",
    "print(len(preProcess(example)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost coeff: 0.0001\n",
      "[[6773. 3727.]\n",
      " [4330. 6170.]]\n",
      "Precision: 0.6450476190476191 Recall: 0.6100153111771593 F1Score: 0.6270425403879091 Accuracy: 0.6163333333333333\n",
      "Cost coeff: 0.001\n",
      "[[7065. 3435.]\n",
      " [4191. 6309.]]\n",
      "Precision: 0.6728571428571428 Recall: 0.6276652452025586 F1Score: 0.6494760066188637 Accuracy: 0.6368571428571429\n",
      "Cost coeff: 0.01\n",
      "[[7014. 3486.]\n",
      " [4132. 6368.]]\n",
      "Precision: 0.668 Recall: 0.6292840480890005 F1Score: 0.6480643074933012 Accuracy: 0.6372380952380953\n",
      "Cost coeff: 0.1\n",
      "[[6943. 3557.]\n",
      " [4192. 6308.]]\n",
      "Precision: 0.6612380952380953 Recall: 0.6235294117647059 F1Score: 0.6418303674601341 Accuracy: 0.631\n",
      "Cost coeff: 1.0\n",
      "[[6714. 3786.]\n",
      " [4407. 6093.]]\n",
      "Precision: 0.6394285714285715 Recall: 0.6037226868087402 F1Score: 0.6210628555570973 Accuracy: 0.6098571428571429\n",
      "Cost coeff: 1.5\n",
      "[[6637. 3863.]\n",
      " [4430. 6070.]]\n",
      "Precision: 0.632095238095238 Recall: 0.5997108520827686 F1Score: 0.6154773496545649 Accuracy: 0.6050952380952381\n",
      "Best costCoeff: 0.001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bestF1 = 0\n",
    "# intialise list of candidate cost coefficients\n",
    "costCoeffs = [0.0001,0.001,0.01,0.1,1.0,1.5]\n",
    "\n",
    "for c in costCoeffs:\n",
    "    # For each candidate cost coefficient, print the confusion matrix and the performance metrics\n",
    "    globalCostCoeff = c\n",
    "    print('Cost coeff:', globalCostCoeff)\n",
    "    myResults = crossValidate(rawData,10)\n",
    "    print(f\"Precision: {myResults[0]} Recall: {myResults[1]} F1Score: {myResults[2]} Accuracy: {myResults[3]}\")\n",
    "    \n",
    "    # Determine the best cost coefficient based off of F1Score. \n",
    "    # Depending on the objectives of the business, a different metric might be better e.g. recall.\n",
    "    if myResults[2] > bestF1:\n",
    "        bestF1 = myResults[2]\n",
    "        bestC = c\n",
    "\n",
    "print(\"Best costCoeff:\",bestC)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 4:\n",
    "I first tried to improve the accuracy by performing better pre-processing on the data. \n",
    "With only whitespace separating occuring in pre-processing the model achieved:\n",
    "Precision: 0.62 Recall: 0.60 F1Score: 0.61 Accuracy: 0.61\n",
    "\n",
    "With html tag removal, word stemming, removal of stop words, expansion of contractions and lowering of word case the model achieved: \n",
    "Precision: 0.64 Recall: 0.60 F1Score: 0.62 Accuracy: 0.61\n",
    "\n",
    "The pre-processing resulted in a small increase in model precision but there is still a lot more to be done\n",
    "\n",
    "I then tried tuning the SVC model to improve the model. I first increased the max iterations from the default 1000 to 10000 to see if there was an issue with the model not converging. This had a negligible effect on model performance and so I did not vary that parameter.\n",
    "\n",
    "Changing the cost value caused a significant increase in model performance, to investigate this I then tried several values of the cost value to find the best. The best Cost coefficient was 0.01 which resulted in the following metrics:\n",
    "Precision: 0.67 Recall: 0.63 F1Score: 0.65 Accuracy: 0.64\n",
    "It should be noted that the C=0.01 and C=0.001 have very similar performance. This similarity, combined with the small random variations expected from cross-validation's data shuffle, means that repeated runs might choose C=0.001 instead of C=0.01.  \n",
    "\n",
    "This smaller cost value than the default 1.0, means that the model learns more slowly, correcting itself with smaller increments. That can increase the number of iterations required to reach the best result but the smaller steps also mean it is less likely to overshoot a cost minima. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8275. 2225.]\n",
      " [2138. 8362.]]\n",
      "Precision: 0.7880952380952381 Recall: 0.7946797272639969 F1Score: 0.7913737866398891 Accuracy: 0.7922380952380952\n"
     ]
    }
   ],
   "source": [
    "#Exercise 5:\n",
    "# I will include the stars in rating, the verified purchase and the product category in the model\n",
    "\n",
    "def exercise5Parse(reviewLine):\n",
    "    # Should return a tuple of id, the review text, the label, the star rating normalised, boolean verified purchase, product category\n",
    "    id = int(reviewLine[0])\n",
    "    text = str(reviewLine[8])\n",
    "    label = str(reviewLine[1])\n",
    "    #  normalising rating\n",
    "    rating = float(int(reviewLine[2])/5)\n",
    "    \n",
    "    # Convert to int to ease processing\n",
    "    verifiedPurchase = 1 if reviewLine[3] == 'Y' else 0\n",
    "    \n",
    "    # Will need to convert this to a feature later\n",
    "    productCategory = str(reviewLine[5])\n",
    "    \n",
    "    return (id, text, label, rating, verifiedPurchase, productCategory)\n",
    "\n",
    "def ex5PrepData(data):\n",
    "    newData = []\n",
    "    for (_, Text, Label, Rating, VerifiedPurchase, ProductCat) in data:\n",
    "        fullFeaturesDict = toFeatureVector(preProcess(Text))\n",
    "        \n",
    "        # Add other features using * to avoid confusion with words\n",
    "        fullFeaturesDict['*Rating'] = Rating\n",
    "        fullFeaturesDict['*VerifiedPurchase'] = VerifiedPurchase\n",
    "        \n",
    "        # Here i add the product category in the same way as with any of the words\n",
    "        fullFeaturesDict[ProductCat] = 1\n",
    "                                        \n",
    "        newData.append((fullFeaturesDict,Label))\n",
    "    return newData\n",
    "\n",
    "def ex5LoadData(path):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        # We want to skip the first line as it is the column title\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Id, Text, Label, Rating, VerifiedPurchase, ProductCat) = exercise5Parse(line)\n",
    "            ex5RawData.append((Id, Text, Label, Rating, VerifiedPurchase, ProductCat))\n",
    "            \n",
    "def ex5CrossValidate(dataset, folds):\n",
    "    # Shuffle the data so we will split the data randomly\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    # Perform preprocessing and vectorisation on the data\n",
    "    data = ex5PrepData(dataset)\n",
    "    \n",
    "    # Initialise an empty confusion matrix\n",
    "    totalConfusionMatrix = np.zeros((2,2))\n",
    "    \n",
    "    # Get the length of the testData slice \n",
    "    testDataLen = int(len(data)/folds)\n",
    "    \n",
    "    # For each slice of the data\n",
    "    for i in range(0,len(data),testDataLen):\n",
    "        testEnd = i+testDataLen\n",
    "        \n",
    "        #Slice up the data into test and training\n",
    "        testingData = data[i:testEnd]\n",
    "        trainingData = data[:i] + data[testEnd:]\n",
    "        \n",
    "        #print(\"Split:\", i, \"No train data:\",len(trainingData),\"No test data:\",len(testingData),\"Scores below\")\n",
    "        # Train the model then get its predictions \n",
    "        classifier = trainClassifier(trainingData)\n",
    "        yPred = [predictVector(x[0], classifier) for x in testingData]\n",
    "        \n",
    "        # Use a lambda function to get the true labels\n",
    "        yTrue = [x[1] for x in testingData]\n",
    "        \n",
    "        # Get a confusion matrix to describe the performance of this fold\n",
    "        confusionMatrix = confusion_matrix(yTrue,yPred)\n",
    "        \n",
    "        # Add to the overall confusion matrix. This will allow us to get average performance metrics later\n",
    "        totalConfusionMatrix = np.add(confusionMatrix, totalConfusionMatrix)\n",
    "        \n",
    "    print(totalConfusionMatrix)\n",
    "    \n",
    "    # Get the average performance metrics from overall confusion matrix\n",
    "    averagePrecision = totalConfusionMatrix[0][0] / (totalConfusionMatrix[0][0] + totalConfusionMatrix[0][1])\n",
    "    averageRecall = totalConfusionMatrix[0][0] / (totalConfusionMatrix[0][0] + totalConfusionMatrix[1][0])\n",
    "    averageF1Score = 2*averagePrecision*averageRecall / (averagePrecision + averageRecall)\n",
    "    averageAccuracy = (totalConfusionMatrix[0][0] + totalConfusionMatrix[1][1])/ float(np.sum(totalConfusionMatrix))\n",
    "    \n",
    "    # Return results in a tuple\n",
    "    cv_results = (averagePrecision,averageRecall,averageF1Score,averageAccuracy)\n",
    "    return cv_results            \n",
    "\n",
    "ex5RawData = []\n",
    "ex5LoadData(reviewPath)\n",
    "\n",
    "ex5MyResults = ex5CrossValidate(ex5RawData,10)\n",
    "print(f\"Precision: {ex5MyResults[0]} Recall: {ex5MyResults[1]} F1Score: {ex5MyResults[2]} Accuracy: {ex5MyResults[3]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including the three new features the performance of the model greatly increased to: \n",
    "Precision: 0.79 Recall: 0.79 F1Score: 0.79 Accuracy: 0.79\n",
    "\n",
    "This increase is expected as additional data should always help the model. In particular, I expect that the verified purchase feature will have made the largest improvement as intuitively I think fake reviews are unlikely to make a genuine purchase of the item. \n",
    "\n",
    "I expect the star rating could also have helped because fake reviews would often be in the extremes. i.e. a 1 star ofr 5 star rating. linear SVM will only be able to make partial use of this, i.e. it will either determine that more stars increase the likelihood of a fake review or it will determine that fewer stars increases the likelihood of a fake review. It will not be able to say that reviews at either extreme are fake. To do that, we should consider using a different model or a non-linear svm such as a kernal SVM. \n",
    "\n",
    "The product category could be helping because it may be more common for people to leave fake reviews in some product categories, in particular ones where there is strong competition between manufacturers. \n",
    "\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
