{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        # We want to skip the first line as it is the column title\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "            preprocessedData.append((Id, preProcess(Text), Label))\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    \n",
    "    id = int(reviewLine[0])\n",
    "    text = str(reviewLine[8])\n",
    "    label = str(reviewLine[1])\n",
    "    return (id, text, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/leem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    #Initialisation steps:\n",
    "    new_text = []\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # remove html tags\n",
    "    text = BeautifulSoup(text,\"lxml\").get_text()\n",
    "    # replace i'd with i would and other similar contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # split by whitespace and remove non-alphanumeric characters like punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        # Loop through words and remove capital letters\n",
    "        new_token = token.lower()\n",
    "        \n",
    "        # If token is a stop word we don't want to include it\n",
    "        if new_token in stop_words:\n",
    "            continue;\n",
    "        \n",
    "        # Use the porter algorithm to stem the word e.g. rationalise -> rational\n",
    "        new_text.append(porter.stem(new_token))\n",
    "    \n",
    "    # Should return a list of tokens\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVec = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        # For each token, we want to increment the global feature count\n",
    "        if token in featureDict:\n",
    "            featureDict[token] += 1\n",
    "        else:\n",
    "            featureDict[token] = 1\n",
    "        # We want to add the token to the dictionary to create a simple vector\n",
    "        if token in featureVec:\n",
    "            featureVec[token] += 1\n",
    "        else:\n",
    "            featureVec[token] = 1\n",
    "\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    #print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(max_iter=1000,C=globalCostCoeff))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    # Shuffle the data so we will split the data randomly\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    # Perform preprocessing and vectorisation on the data\n",
    "    data = prepData(dataset)\n",
    "    \n",
    "    # Initialise an empty confusion matrix\n",
    "    totalConfusionMatrix = np.zeros((2,2))\n",
    "    \n",
    "    # Get the length of the testData slice \n",
    "    testDataLen = int(len(data)/folds)\n",
    "    \n",
    "    # For each slice of the data\n",
    "    for i in range(0,len(data),testDataLen):\n",
    "        testEnd = i+testDataLen\n",
    "        \n",
    "        #Slice up the data into test and training\n",
    "        testingData = data[i:testEnd]\n",
    "        trainingData = data[:i] + data[testEnd:]\n",
    "        \n",
    "        #print(\"Split:\", i, \"No train data:\",len(trainingData),\"No test data:\",len(testingData),\"Scores below\")\n",
    "        # Train the model then get its predictions \n",
    "        classifier = trainClassifier(trainingData)\n",
    "        yPred = [predictVector(x[0], classifier) for x in testingData]\n",
    "        \n",
    "        # Use a lambda function to get the true labels\n",
    "        yTrue = [x[1] for x in testingData]\n",
    "        \n",
    "        # Get a confusion matrix to describe the performance of this fold\n",
    "        confusionMatrix = confusion_matrix(yTrue,yPred)\n",
    "        \n",
    "        # Add to the overall confusion matrix. This will allow us to get average performance metrics later\n",
    "        totalConfusionMatrix = np.add(confusionMatrix, totalConfusionMatrix)\n",
    "        \n",
    "    print(totalConfusionMatrix)\n",
    "    \n",
    "    # Get the average performance metrics from overall confusion matrix\n",
    "    averagePrecision = totalConfusionMatrix[0][0] / (totalConfusionMatrix[0][0] + totalConfusionMatrix[0][1])\n",
    "    averageRecall = totalConfusionMatrix[0][0] / (totalConfusionMatrix[0][0] + totalConfusionMatrix[1][0])\n",
    "    averageF1Score = 2*averagePrecision*averageRecall / (averagePrecision + averageRecall)\n",
    "    averageAccuracy = (totalConfusionMatrix[0][0] + totalConfusionMatrix[1][1])/ float(np.sum(totalConfusionMatrix))\n",
    "    \n",
    "    # Return results in a tuple\n",
    "    cv_results = (averagePrecision,averageRecall,averageF1Score,averageAccuracy)\n",
    "    return cv_results\n",
    "\n",
    "def prepData(data):\n",
    "    newData = []\n",
    "    for (_, Text, Label) in data:\n",
    "        newData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "# Takes in a list of strings as review samples and returns list of predictions\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "# Takes in string as a review sample and returns a prediction\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))\n",
    "\n",
    "# More efficient to use lambda function and simply pass the vector to this predict func\n",
    "def predictVector(textVec, classifier):\n",
    "    return classifier.classify(textVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "24768\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is a great source of information for those studying to take the medical administrative assistant exam, as it's incredibly knowledgeable and full of insightful resources. The author really knows her stuff! There are so many informative questions to help study from, and one of the best study guides I've ever looked into. Well done!\n",
      "After preprocessing\n",
      "['book', 'great', 'sourc', 'inform', 'studi', 'take', 'medic', 'administr', 'assist', 'exam', 'incred', 'knowledg', 'full', 'insight', 'resourc', 'author', 'realli', 'know', 'stuff', 'mani', 'inform', 'question', 'help', 'studi', 'one', 'best', 'studi', 'guid', 'ever', 'look', 'well', 'done']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "#Cell for sense-checking the preprocessing function\n",
    "example = rawData[randint(0,len(rawData))][1]\n",
    "print(example)\n",
    "print(\"After preprocessing\")\n",
    "print(preProcess(example))\n",
    "print(len(preProcess(example)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost coeff: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bestF1 = 0\n",
    "costCoeffs = [0.0001,0.001,0.01,0.1,1.0,1.5]\n",
    "\n",
    "for c in costCoeffs:\n",
    "    globalCostCoeff = c\n",
    "    print('Cost coeff:', globalCostCoeff)\n",
    "    myResults = crossValidate(rawData,10)\n",
    "    print(f\"Precision: {myResults[0]} Recall: {myResults[1]} F1Score: {myResults[2]} Accuracy: {myResults[3]}\")\n",
    "    if myResults[2] > bestF1:\n",
    "        bestF1 = myResults[2]\n",
    "        bestC = c\n",
    "\n",
    "print(\"Best costCoeff:\",bestC)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 4:\n",
    "I first tried to improve the accuracy by performing better pre-processing on the data. \n",
    "With only whitespace separating occuring in pre-processing the model achieved:\n",
    "Precision: 0.6201904761904762 Recall: 0.604025600593637 F1Score: 0.6120013157276444 Accuracy: 0.6068095238095238\n",
    "\n",
    "With html tag removal, word stemming, removal of stop words, expansion of contractions and lowering of word case the model achieved: \n",
    "Precision: 0.6384761904761905 Recall: 0.6045085662759243 F1Score: 0.6210282538212135 Accuracy: 0.6103809523809524\n",
    "\n",
    "The pre-processing resulted in a small increase in model performance\n",
    "\n",
    "I then tried tuning the SVC model to improve the model. I first increased the max iterations from the default 1000 to 10000 to see if there was an issue with the model not converging. This had a negligible effect on model performance and so I did not vary that parameter.\n",
    "\n",
    "Changing the cost value caused a significant increase in model performance, to investigate this i then tried several values of the cost value to find the best. The best Cost coefficient was 0.01 which resulted in the following metrics:\n",
    "Precision: 0.6727619047619048 Recall: 0.6321818507248971 F1Score: 0.651840915382486 Accuracy: 0.6406666666666667\n",
    "\n",
    "This smaller cost value than the default 1.0, means that the model learns more slowly, correcting itself with smaller increments. That can increase the number of iterations required to reach the best result but the smaller steps also mean it is less likely to overshoot a cost minima. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excercise 5:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
