{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import time\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "class NmtModel(object):\n",
    "    def __init__(self,source_dict,target_dict,use_attention):\n",
    "        self.num_layers = 2\n",
    "        self.hidden_size = 200\n",
    "        self.embedding_size = 100\n",
    "        self.hidden_dropout_rate=0.2\n",
    "        self.embedding_dropout_rate = 0.2\n",
    "        self.max_target_step = 30\n",
    "        self.vocab_target_size = len(target_dict.vocab)\n",
    "        self.vocab_source_size = len(source_dict.vocab)\n",
    "        self.target_dict = target_dict\n",
    "        self.source_dict = source_dict\n",
    "        self.SOS = target_dict.word2ids['<start>']\n",
    "        self.EOS = target_dict.word2ids['<end>']\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        self.source_words = tf.placeholder(tf.int32,[None,None],\"source_words\")\n",
    "        self.target_words = tf.placeholder(tf.int32,[None,None],\"target_words\")\n",
    "        self.source_sent_lens = tf.placeholder(tf.int32,[None],\"source_sent_lens\")\n",
    "        self.target_sent_lens = tf.placeholder(tf.int32,[None],\"target_sent_lens\")\n",
    "        self.is_training = tf.placeholder(tf.bool,[],\"is_training\")\n",
    "\n",
    "        self.predictions,self.loss = self.get_predictions_and_loss(self.source_words,self.target_words,self.source_sent_lens,self.target_sent_lens,self.is_training)\n",
    "\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss, trainable_params)\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, trainable_params))\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def get_predictions_and_loss(self, source_words,target_words, source_sent_lens,target_sent_lens,is_training):\n",
    "        self.embeddings_target = tf.get_variable(\"embeddings_target\", [self.vocab_target_size, self.embedding_size], dtype=tf.float32)\n",
    "        self.embeddings_source = tf.get_variable(\"embeddings_source\", [self.vocab_source_size, self.embedding_size], dtype=tf.float32)\n",
    "\n",
    "        batch_size = shape(target_words, 0)\n",
    "        max_target_sent_len = shape(target_words, 1)\n",
    "\n",
    "        embedding_keep_prob = 1 - (tf.to_float(is_training) * self.embedding_dropout_rate)\n",
    "        hidden_keep_prob = 1 - (tf.to_float(is_training) * self.hidden_dropout_rate)\n",
    "\n",
    "        source_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_source,source_words),embedding_keep_prob)\n",
    "        target_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_target,target_words),embedding_keep_prob)\n",
    "\n",
    "\n",
    "        encoder_outputs, encode_final_states = self.encoder(source_embs,source_sent_lens,hidden_keep_prob, embedding_keep_prob)\n",
    "\n",
    "        time_major_target_embs = tf.transpose(target_embs,[1,0,2])\n",
    "\n",
    "\n",
    "        def _decoder_scan(pre,inputs):\n",
    "            pre_logits, pre_pred, pre_states = pre\n",
    "            step_embeddings = inputs\n",
    "\n",
    "            pred_embeddings = tf.nn.embedding_lookup(self.embeddings_target,pre_pred)\n",
    "\n",
    "            step_embeddings = tf.cond(is_training,lambda :step_embeddings,lambda :pred_embeddings)\n",
    "            curr_logits, curr_states = self.step_decoder(step_embeddings,encoder_outputs,pre_states,hidden_keep_prob)\n",
    "            curr_pred = tf.argmax(curr_logits,1,output_type=tf.int32)\n",
    "\n",
    "            return curr_logits, curr_pred, curr_states\n",
    "\n",
    "        init_logits = tf.zeros([batch_size,self.vocab_target_size])\n",
    "        init_pred = tf.ones([batch_size],tf.int32) * self.SOS\n",
    "\n",
    "        time_major_logits, time_major_preds, _ = tf.scan(_decoder_scan,time_major_target_embs,initializer=(init_logits, init_pred,encode_final_states))\n",
    "        time_major_logits, time_major_preds = tf.stack(time_major_logits),tf.stack(time_major_preds)\n",
    "\n",
    "        logits = tf.transpose(time_major_logits,[1,0,2])\n",
    "        predictions = tf.transpose(time_major_preds,[1,0])\n",
    "\n",
    "        logits_mask = tf.sequence_mask(target_sent_lens-1,max_target_sent_len)\n",
    "        flatten_logits_mask = tf.reshape(logits_mask,[batch_size*max_target_sent_len])\n",
    "        flatten_logits = tf.boolean_mask(tf.reshape(logits,[batch_size*max_target_sent_len,self.vocab_target_size]),flatten_logits_mask)\n",
    "\n",
    "        gold_labels_mask = tf.concat([tf.zeros([batch_size,1],dtype=tf.bool),tf.sequence_mask(target_sent_lens-1,max_target_sent_len-1)],1)\n",
    "        flatten_gold_labels_mask = tf.reshape(gold_labels_mask,[batch_size*max_target_sent_len])\n",
    "        flatten_gold_labels = tf.boolean_mask(tf.reshape(target_words,[batch_size*max_target_sent_len]),flatten_gold_labels_mask)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=flatten_gold_labels,logits=flatten_logits))\n",
    "\n",
    "        return predictions, loss\n",
    "\n",
    "    def encoder(self,embeddings, sent_lens, hidden_keep_prob=1.0, embedding_keep_prob=1.0):\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            \"\"\"\n",
    "            Task 1 encoder\n",
    "\n",
    "            Start\n",
    "            \"\"\"\n",
    "\n",
    "            word_embeddings = tf.nn.dropout(embeddings,embedding_keep_prob)\n",
    "            word_lstm_first = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
    "                                                          state_keep_prob=hidden_keep_prob,\n",
    "                                                          variational_recurrent=True,\n",
    "                                                          dtype=tf.float32)\n",
    "            word_lstm_second = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
    "                                                          state_keep_prob=hidden_keep_prob,\n",
    "                                                          variational_recurrent=True,\n",
    "                                                          dtype=tf.float32)\n",
    "            lstm_cells = tf.nn.rnn_cell.MultiRNNCell([word_lstm_first, word_lstm_second])\n",
    "            output,state = tf.nn.dynamic_rnn(lstm_cells, word_embeddings,sequence_length=sent_lens, dtype=tf.float32)\n",
    "\n",
    "            \"\"\"\n",
    "            End Task 1\n",
    "            \"\"\"\n",
    "            return output, state\n",
    "\n",
    "    def step_decoder(self,step_embeddings,encoder_outputs, pre_states, hidden_keep_prob=1.0):\n",
    "        with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "            \"\"\"\n",
    "            Task 2 decoder without attention\n",
    "\n",
    "            Start\n",
    "            Secondly, since we donâ€™t process the whole sentences, instead we only process one step of\n",
    "            the LSTM. We can directly call the LSTM cell by feeding the cell the step_embeddings and\n",
    "            the pre_states, it will return the LSTM output of this step (step_decoder_output) and new\n",
    "            states.\n",
    "            \"\"\"\n",
    "            word_lstm_first = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
    "                                                          state_keep_prob=hidden_keep_prob,\n",
    "                                                          variational_recurrent=True,\n",
    "                                                          dtype=tf.float32)\n",
    "            word_lstm_second = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
    "                                                          state_keep_prob=hidden_keep_prob,\n",
    "                                                          variational_recurrent=True,\n",
    "                                                          dtype=tf.float32)\n",
    "            lstm_cells = tf.nn.rnn_cell.MultiRNNCell([word_lstm_first, word_lstm_second])\n",
    "            # Perform a run through on a single step of the sequence\n",
    "            step_decoder_output, curr_states = lstm_cells(step_embeddings,pre_states)\n",
    "\n",
    "            if not self.use_attention:\n",
    "\n",
    "\n",
    "                output_weights = tf.get_variable(\"output_weights\", [shape(step_decoder_output, 1), self.vocab_target_size])\n",
    "                output_bias = tf.get_variable(\"output_bias\", [self.vocab_target_size])\n",
    "                logits = tf.nn.xw_plus_b(step_decoder_output,output_weights,output_bias)\n",
    "\n",
    "                # End Task 2\n",
    "            else:\n",
    "                #Task 3 attention\n",
    "                # reshape the output to have an additional final dim\n",
    "                reshaped_output = tf.expand_dims(step_decoder_output, -1)\n",
    "\n",
    "                # Calculate the raw score by matrix mutliplication\n",
    "                raw_score = tf.matmul(encoder_outputs,reshaped_output)\n",
    "\n",
    "                # Apply softmax to score to get prob dist\n",
    "                softmax_score = tf.nn.softmax(raw_score,1)\n",
    "\n",
    "                # Create the attention weighted vector\n",
    "                encoded_vector = tf.reduce_sum(softmax_score * encoder_outputs,1)\n",
    "\n",
    "                concat_vec = tf.concat([step_decoder_output, encoded_vector],1)\n",
    "\n",
    "                output_weights = tf.get_variable(\"output_weights\", [shape(concat_vec, 1), self.vocab_target_size])\n",
    "                output_bias = tf.get_variable(\"output_bias\", [self.vocab_target_size])\n",
    "                logits = tf.nn.xw_plus_b(concat_vec,output_weights,output_bias)\n",
    "\n",
    "                #Ends Task 3\n",
    "\n",
    "        return logits, curr_states\n",
    "\n",
    "    def time_used(self, start_time):\n",
    "        curr_time = time.time()\n",
    "        used_time = curr_time-start_time\n",
    "        m = used_time // 60\n",
    "        s = used_time - 60 * m\n",
    "        return \"%d m %d s\" % (m, s)\n",
    "\n",
    "    def train(self,train_data,dev_data,test_data, epochs):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
    "            epoch_time = time.time()\n",
    "            losses = []\n",
    "            source_train,target_train = train_data\n",
    "            for i, (source,target) in enumerate(zip(source_train,target_train)):\n",
    "                source_words,source_sent_lens = source\n",
    "                target_words,target_sent_lens = target\n",
    "                fd = {self.source_words:source_words,self.target_words:target_words,\n",
    "                            self.source_sent_lens:source_sent_lens,self.target_sent_lens:target_sent_lens,\n",
    "                            self.is_training:True}\n",
    "\n",
    "                _, loss= self.sess.run([self.train_op, self.loss], feed_dict=fd)\n",
    "\n",
    "                losses.append(loss)\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(\"[{}]: loss:{:.2f}\".format(i+1, sum(losses[i + 1 - 100:]) / 100.0))\n",
    "            print(\"Average epoch loss:{}\".format(sum(losses) / len(losses)))\n",
    "            print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
    "            dev_time = time.time()\n",
    "            print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
    "            self.eval(dev_data)\n",
    "            print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
    "\n",
    "        print(\"Training finished!\")\n",
    "        print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
    "\n",
    "        print(\"Evaluating on test set:\")\n",
    "        test_time = time.time()\n",
    "        self.eval(test_data)\n",
    "        print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
    "\n",
    "    def get_target_sentences(self, sents,vocab,reference=False,isnumpy=False):\n",
    "        str_sents = []\n",
    "        for sent in sents:\n",
    "            str_sent = []\n",
    "            for t in sent:\n",
    "                if isnumpy:\n",
    "                    t = t.item()\n",
    "                if t == self.SOS:\n",
    "                    continue\n",
    "                if t == self.EOS:\n",
    "                    break\n",
    "\n",
    "                str_sent.append(vocab[t])\n",
    "            if reference:\n",
    "                str_sents.append([str_sent])\n",
    "            else:\n",
    "                str_sents.append(str_sent)\n",
    "        return str_sents\n",
    "\n",
    "    def eval(self, dataset):\n",
    "        source_batches, target_batches = dataset\n",
    "        references = []\n",
    "        candidates = []\n",
    "        vocab = self.target_dict.vocab\n",
    "        PAD = self.target_dict.PAD\n",
    "\n",
    "        for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
    "            source_words, source_sent_lens = source\n",
    "            target_words, target_sent_lens = target\n",
    "            infer_target_words = [[PAD for i in range(self.max_target_step)] for b in target_words]\n",
    "\n",
    "            fd = {self.source_words: source_words, self.target_words: infer_target_words,\n",
    "                        self.source_sent_lens: source_sent_lens,\n",
    "                        self.is_training: False}\n",
    "            predictions = self.sess.run(self.predictions,feed_dict=fd)\n",
    "\n",
    "            references.extend(self.get_target_sentences(target_words,vocab,reference=True))\n",
    "            candidates.extend(self.get_target_sentences(predictions,vocab,isnumpy=True))\n",
    "\n",
    "        score = corpus_bleu(references,candidates)\n",
    "        print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
    "\n",
    "def shape(x, n):\n",
    "    return x.get_shape()[n].value or tf.shape(x)[n]\n",
    "\n",
    "class LanguageDict():\n",
    "    def __init__(self, sents):\n",
    "        word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
    "\n",
    "        self.vocab = [t for t,c in word_counter.items() if c > 10]\n",
    "        self.vocab.append('<pad>')\n",
    "        self.vocab.append('<unk>')\n",
    "        self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
    "        self.UNK = self.word2ids['<unk>']\n",
    "        self.PAD = self.word2ids['<pad>']\n",
    "\n",
    "\n",
    "def load_dataset(path, max_num_examples=30000,batch_size=100,add_start_end = False):\n",
    "    lines = [line for line in open(path,'r')]\n",
    "    if max_num_examples > 0:\n",
    "        max_num_examples = min(len(lines), max_num_examples)\n",
    "        lines = lines[:max_num_examples]\n",
    "\n",
    "    sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in lines]\n",
    "    if add_start_end:\n",
    "        for sent in sents:\n",
    "            sent.append('<end>')\n",
    "            sent.insert(0,'<start>')\n",
    "\n",
    "    lang_dict = LanguageDict(sents)\n",
    "\n",
    "    sents = [[lang_dict.word2ids.get(tok,lang_dict.UNK) for tok in sent] for sent in sents]\n",
    "\n",
    "    batches = []\n",
    "    for i in range(len(sents) // batch_size):\n",
    "        batch = sents[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_len = [len(sent) for sent in batch]\n",
    "        max_batch_len = max(batch_len)\n",
    "        for sent in batch:\n",
    "            if len(sent) < max_batch_len:\n",
    "                sent.extend([lang_dict.PAD for _ in range(max_batch_len - len(sent))])\n",
    "        batches.append((batch, batch_len))\n",
    "\n",
    "\n",
    "    unit = len(batches)//10\n",
    "    train_batches = batches[:8*unit]\n",
    "    dev_batches = batches[8*unit:9*unit]\n",
    "    test_batches = batches[9*unit:]\n",
    "\n",
    "    return train_batches,dev_batches,test_batches,lang_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 240/30/30 train/dev/test batches\n"
     ]
    }
   ],
   "source": [
    "source_train, source_dev, source_test, source_dict = load_dataset(\"data.30.vi\",max_num_examples=max_example,batch_size=batch_size)\n",
    "target_train, target_dev, target_test, target_dict = load_dataset(\"data.30.en\", max_num_examples=max_example,batch_size=batch_size, add_start_end=True)\n",
    "print(\"read %d/%d/%d train/dev/test batches\" % (len(source_train),len(source_dev), len(source_test)))\n",
    "\n",
    "train_data = (source_train,target_train)\n",
    "dev_data = (source_dev,target_dev)\n",
    "test_data = (source_test,target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without attention\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 100\n",
    "max_example = 30000\n",
    "use_attention = False\n",
    "\n",
    "model = NmtModel(source_dict,target_dict,use_attention)\n",
    "model.build()\n",
    "model.train(train_data,dev_data,test_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 240/30/30 train/dev/test batches\n",
      "source vocab: 2034, target vocab:2506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1/10\n",
      "[100]: loss:4.99\n",
      "[200]: loss:4.33\n",
      "Average epoch loss:4.567721256613732\n",
      "Time used for epoch 1: 3 m 27 s\n",
      "Evaluating on dev set after epoch 1/10:\n",
      "Model BLEU score: 1.67\n",
      "Time used for evaluate on dev set: 0 m 9 s\n",
      "Starting training epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1638c39af351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNmtModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-05ad8c755e5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, dev_data, test_data, epochs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                             self.is_training:True}\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eenlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run with attention\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "use_attention = True\n",
    "\n",
    "model2 = NmtModel(source_dict,target_dict,use_attention)\n",
    "model2.build()\n",
    "model2.train(train_data,dev_data,test_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eenlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
