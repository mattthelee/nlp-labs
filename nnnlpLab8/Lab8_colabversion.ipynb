{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab8-colabversion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattthelee/nlp-labs/blob/master/nnnlpLab8/Lab8_colabversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "esHKaW7th3wj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import collections\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "class NmtModel(object):\n",
        "    def __init__(self,source_dict,target_dict,use_attention):\n",
        "        self.num_layers = 2\n",
        "        self.hidden_size = 200\n",
        "        self.embedding_size = 100\n",
        "        self.hidden_dropout_rate=0.2\n",
        "        self.embedding_dropout_rate = 0.2\n",
        "        self.max_target_step = 30\n",
        "        self.vocab_target_size = len(target_dict.vocab)\n",
        "        self.vocab_source_size = len(source_dict.vocab)\n",
        "        self.target_dict = target_dict\n",
        "        self.source_dict = source_dict\n",
        "        self.SOS = target_dict.word2ids['<start>']\n",
        "        self.EOS = target_dict.word2ids['<end>']\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        self.source_words = tf.placeholder(tf.int32,[None,None],\"source_words\")\n",
        "        self.target_words = tf.placeholder(tf.int32,[None,None],\"target_words\")\n",
        "        self.source_sent_lens = tf.placeholder(tf.int32,[None],\"source_sent_lens\")\n",
        "        self.target_sent_lens = tf.placeholder(tf.int32,[None],\"target_sent_lens\")\n",
        "        self.is_training = tf.placeholder(tf.bool,[],\"is_training\")\n",
        "\n",
        "        self.predictions,self.loss = self.get_predictions_and_loss(self.source_words,self.target_words,self.source_sent_lens,self.target_sent_lens,self.is_training)\n",
        "\n",
        "        trainable_params = tf.trainable_variables()\n",
        "        gradients = tf.gradients(self.loss, trainable_params)\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "        self.train_op = optimizer.apply_gradients(zip(gradients, trainable_params))\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "    def get_predictions_and_loss(self, source_words,target_words, source_sent_lens,target_sent_lens,is_training):\n",
        "        self.embeddings_target = tf.get_variable(\"embeddings_target\", [self.vocab_target_size, self.embedding_size], dtype=tf.float32)\n",
        "        self.embeddings_source = tf.get_variable(\"embeddings_source\", [self.vocab_source_size, self.embedding_size], dtype=tf.float32)\n",
        "\n",
        "        batch_size = shape(target_words, 0)\n",
        "        max_target_sent_len = shape(target_words, 1)\n",
        "\n",
        "        embedding_keep_prob = 1 - (tf.to_float(is_training) * self.embedding_dropout_rate)\n",
        "        hidden_keep_prob = 1 - (tf.to_float(is_training) * self.hidden_dropout_rate)\n",
        "\n",
        "        source_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_source,source_words),embedding_keep_prob)\n",
        "        target_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_target,target_words),embedding_keep_prob)\n",
        "\n",
        "\n",
        "        encoder_outputs, encode_final_states = self.encoder(source_embs,source_sent_lens,hidden_keep_prob, embedding_keep_prob)\n",
        "\n",
        "        time_major_target_embs = tf.transpose(target_embs,[1,0,2])\n",
        "\n",
        "\n",
        "        def _decoder_scan(pre,inputs):\n",
        "            pre_logits, pre_pred, pre_states = pre\n",
        "            step_embeddings = inputs\n",
        "\n",
        "            pred_embeddings = tf.nn.embedding_lookup(self.embeddings_target,pre_pred)\n",
        "\n",
        "            step_embeddings = tf.cond(is_training,lambda :step_embeddings,lambda :pred_embeddings)\n",
        "            curr_logits, curr_states = self.step_decoder(step_embeddings,encoder_outputs,pre_states,hidden_keep_prob)\n",
        "            curr_pred = tf.argmax(curr_logits,1,output_type=tf.int32)\n",
        "\n",
        "            return curr_logits, curr_pred, curr_states\n",
        "\n",
        "        init_logits = tf.zeros([batch_size,self.vocab_target_size])\n",
        "        init_pred = tf.ones([batch_size],tf.int32) * self.SOS\n",
        "\n",
        "        time_major_logits, time_major_preds, _ = tf.scan(_decoder_scan,time_major_target_embs,initializer=(init_logits, init_pred,encode_final_states))\n",
        "        time_major_logits, time_major_preds = tf.stack(time_major_logits),tf.stack(time_major_preds)\n",
        "\n",
        "        logits = tf.transpose(time_major_logits,[1,0,2])\n",
        "        predictions = tf.transpose(time_major_preds,[1,0])\n",
        "\n",
        "        logits_mask = tf.sequence_mask(target_sent_lens-1,max_target_sent_len)\n",
        "        flatten_logits_mask = tf.reshape(logits_mask,[batch_size*max_target_sent_len])\n",
        "        flatten_logits = tf.boolean_mask(tf.reshape(logits,[batch_size*max_target_sent_len,self.vocab_target_size]),flatten_logits_mask)\n",
        "\n",
        "        gold_labels_mask = tf.concat([tf.zeros([batch_size,1],dtype=tf.bool),tf.sequence_mask(target_sent_lens-1,max_target_sent_len-1)],1)\n",
        "        flatten_gold_labels_mask = tf.reshape(gold_labels_mask,[batch_size*max_target_sent_len])\n",
        "        flatten_gold_labels = tf.boolean_mask(tf.reshape(target_words,[batch_size*max_target_sent_len]),flatten_gold_labels_mask)\n",
        "\n",
        "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=flatten_gold_labels,logits=flatten_logits))\n",
        "\n",
        "        return predictions, loss\n",
        "\n",
        "    def encoder(self,embeddings, sent_lens, hidden_keep_prob=1.0, embedding_keep_prob=1.0):\n",
        "        with tf.variable_scope(\"encoder\"):\n",
        "            \"\"\"\n",
        "            Task 1 encoder\n",
        "\n",
        "            Start\n",
        "            \"\"\"\n",
        "\n",
        "            word_embeddings = tf.nn.dropout(embeddings,embedding_keep_prob)\n",
        "            word_lstm_first = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            word_lstm_second = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            lstm_cells = tf.nn.rnn_cell.MultiRNNCell([word_lstm_first, word_lstm_second])\n",
        "            output,state = tf.nn.dynamic_rnn(lstm_cells, word_embeddings,sequence_length=sent_lens, dtype=tf.float32)\n",
        "\n",
        "            \"\"\"\n",
        "            End Task 1\n",
        "            \"\"\"\n",
        "            return output, state\n",
        "\n",
        "    def step_decoder(self,step_embeddings,encoder_outputs, pre_states, hidden_keep_prob=1.0):\n",
        "        with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "            \"\"\"\n",
        "            Task 2 decoder without attention\n",
        "\n",
        "            Start\n",
        "            Secondly, since we donâ€™t process the whole sentences, instead we only process one step of\n",
        "            the LSTM. We can directly call the LSTM cell by feeding the cell the step_embeddings and\n",
        "            the pre_states, it will return the LSTM output of this step (step_decoder_output) and new\n",
        "            states.\n",
        "            \"\"\"\n",
        "            word_lstm_first = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            word_lstm_second = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            lstm_cells = tf.nn.rnn_cell.MultiRNNCell([word_lstm_first, word_lstm_second])\n",
        "            # Perform a run through on a single step of the sequence\n",
        "            step_decoder_output, curr_states = lstm_cells(step_embeddings,pre_states)\n",
        "\n",
        "            if not self.use_attention:\n",
        "\n",
        "\n",
        "                output_weights = tf.get_variable(\"output_weights\", [shape(step_decoder_output, 1), self.vocab_target_size])\n",
        "                output_bias = tf.get_variable(\"output_bias\", [self.vocab_target_size])\n",
        "                logits = tf.nn.xw_plus_b(step_decoder_output,output_weights,output_bias)\n",
        "\n",
        "                # End Task 2\n",
        "            else:\n",
        "                #Task 3 attention\n",
        "                # reshape the output to have an additional final dim\n",
        "                reshaped_output = tf.expand_dims(step_decoder_output, -1)\n",
        "\n",
        "                # Calculate the raw score by matrix mutliplication\n",
        "                raw_score = tf.matmul(encoder_outputs,reshaped_output)\n",
        "\n",
        "                # Apply softmax to score to get prob dist\n",
        "                softmax_score = tf.nn.softmax(raw_score,1)\n",
        "\n",
        "                # Create the attention weighted vector\n",
        "                encoded_vector = tf.reduce_sum(softmax_score * encoder_outputs,1)\n",
        "\n",
        "                concat_vec = tf.concat([step_decoder_output, encoded_vector],1)\n",
        "\n",
        "                output_weights = tf.get_variable(\"output_weights\", [shape(concat_vec, 1), self.vocab_target_size])\n",
        "                output_bias = tf.get_variable(\"output_bias\", [self.vocab_target_size])\n",
        "                logits = tf.nn.xw_plus_b(concat_vec,output_weights,output_bias)\n",
        "\n",
        "                #Ends Task 3\n",
        "\n",
        "        return logits, curr_states\n",
        "\n",
        "    def time_used(self, start_time):\n",
        "        curr_time = time.time()\n",
        "        used_time = curr_time-start_time\n",
        "        m = used_time // 60\n",
        "        s = used_time - 60 * m\n",
        "        return \"%d m %d s\" % (m, s)\n",
        "\n",
        "    def train(self,train_data,dev_data,test_data, epochs):\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "            epoch_time = time.time()\n",
        "            losses = []\n",
        "            source_train,target_train = train_data\n",
        "            for i, (source,target) in enumerate(zip(source_train,target_train)):\n",
        "                source_words,source_sent_lens = source\n",
        "                target_words,target_sent_lens = target\n",
        "                fd = {self.source_words:source_words,self.target_words:target_words,\n",
        "                            self.source_sent_lens:source_sent_lens,self.target_sent_lens:target_sent_lens,\n",
        "                            self.is_training:True}\n",
        "\n",
        "                _, loss= self.sess.run([self.train_op, self.loss], feed_dict=fd)\n",
        "\n",
        "                losses.append(loss)\n",
        "                if (i+1) % 100 == 0:\n",
        "                    print(\"[{}]: loss:{:.2f}\".format(i+1, sum(losses[i + 1 - 100:]) / 100.0))\n",
        "            print(\"Average epoch loss:{}\".format(sum(losses) / len(losses)))\n",
        "            print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "            dev_time = time.time()\n",
        "            print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "            self.eval(dev_data)\n",
        "            print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "        print(\"Training finished!\")\n",
        "        print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "        print(\"Evaluating on test set:\")\n",
        "        test_time = time.time()\n",
        "        self.eval(test_data)\n",
        "        print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "    def get_target_sentences(self, sents,vocab,reference=False,isnumpy=False):\n",
        "        str_sents = []\n",
        "        for sent in sents:\n",
        "            str_sent = []\n",
        "            for t in sent:\n",
        "                if isnumpy:\n",
        "                    t = t.item()\n",
        "                if t == self.SOS:\n",
        "                    continue\n",
        "                if t == self.EOS:\n",
        "                    break\n",
        "\n",
        "                str_sent.append(vocab[t])\n",
        "            if reference:\n",
        "                str_sents.append([str_sent])\n",
        "            else:\n",
        "                str_sents.append(str_sent)\n",
        "        return str_sents\n",
        "\n",
        "    def eval(self, dataset):\n",
        "        source_batches, target_batches = dataset\n",
        "        references = []\n",
        "        candidates = []\n",
        "        vocab = self.target_dict.vocab\n",
        "        PAD = self.target_dict.PAD\n",
        "\n",
        "        for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "            source_words, source_sent_lens = source\n",
        "            target_words, target_sent_lens = target\n",
        "            infer_target_words = [[PAD for i in range(self.max_target_step)] for b in target_words]\n",
        "\n",
        "            fd = {self.source_words: source_words, self.target_words: infer_target_words,\n",
        "                        self.source_sent_lens: source_sent_lens,\n",
        "                        self.is_training: False}\n",
        "            predictions = self.sess.run(self.predictions,feed_dict=fd)\n",
        "\n",
        "            references.extend(self.get_target_sentences(target_words,vocab,reference=True))\n",
        "            candidates.extend(self.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "        score = corpus_bleu(references,candidates)\n",
        "        print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "\n",
        "def shape(x, n):\n",
        "    return x.get_shape()[n].value or tf.shape(x)[n]\n",
        "\n",
        "class LanguageDict():\n",
        "    def __init__(self, sents):\n",
        "        word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "        self.vocab = [t for t,c in word_counter.items() if c > 10]\n",
        "        self.vocab.append('<pad>')\n",
        "        self.vocab.append('<unk>')\n",
        "        self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "        self.UNK = self.word2ids['<unk>']\n",
        "        self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "def load_dataset(path, max_num_examples=30000,batch_size=100,add_start_end = False):\n",
        "    lines = [line for line in open(path,'r')]\n",
        "    if max_num_examples > 0:\n",
        "        max_num_examples = min(len(lines), max_num_examples)\n",
        "        lines = lines[:max_num_examples]\n",
        "\n",
        "    sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in lines]\n",
        "    if add_start_end:\n",
        "        for sent in sents:\n",
        "            sent.append('<end>')\n",
        "            sent.insert(0,'<start>')\n",
        "\n",
        "    lang_dict = LanguageDict(sents)\n",
        "\n",
        "    sents = [[lang_dict.word2ids.get(tok,lang_dict.UNK) for tok in sent] for sent in sents]\n",
        "\n",
        "    batches = []\n",
        "    for i in range(len(sents) // batch_size):\n",
        "        batch = sents[i * batch_size:(i + 1) * batch_size]\n",
        "        batch_len = [len(sent) for sent in batch]\n",
        "        max_batch_len = max(batch_len)\n",
        "        for sent in batch:\n",
        "            if len(sent) < max_batch_len:\n",
        "                sent.extend([lang_dict.PAD for _ in range(max_batch_len - len(sent))])\n",
        "        batches.append((batch, batch_len))\n",
        "\n",
        "\n",
        "    unit = len(batches)//10\n",
        "    train_batches = batches[:8*unit]\n",
        "    dev_batches = batches[8*unit:9*unit]\n",
        "    test_batches = batches[9*unit:]\n",
        "\n",
        "    return train_batches,dev_batches,test_batches,lang_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NiOepHszh3wu",
        "colab_type": "code",
        "outputId": "e6bb8cb9-cf90-4404-9357-479e201d3271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "max_example = 30000\n",
        "batch_size = 100\n",
        "source_train, source_dev, source_test, source_dict = load_dataset(\"/content/gdrive/My Drive/data.30.vi\",max_num_examples=max_example,batch_size=batch_size)\n",
        "target_train, target_dev, target_test, target_dict = load_dataset(\"/content/gdrive/My Drive/data.30.en\", max_num_examples=max_example,batch_size=batch_size, add_start_end=True)\n",
        "print(\"read %d/%d/%d train/dev/test batches\" % (len(source_train),len(source_dev), len(source_test)))\n",
        "\n",
        "train_data = (source_train,target_train)\n",
        "dev_data = (source_dev,target_dev)\n",
        "test_data = (source_test,target_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "read 240/30/30 train/dev/test batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c4FC6uy1h3w0",
        "colab_type": "code",
        "outputId": "45367da8-1232-40cb-bc52-8d0dfb8499f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1941
        }
      },
      "cell_type": "code",
      "source": [
        "# Run without attention\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "use_attention = False\n",
        "\n",
        "model = NmtModel(source_dict,target_dict,use_attention)\n",
        "model.build()\n",
        "model.train(train_data,dev_data,test_data,10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source vocab: 2034, target vocab:2506\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-05ad8c755e5b>:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-05ad8c755e5b>:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-1-05ad8c755e5b>:105: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-1-05ad8c755e5b>:113: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-1-05ad8c755e5b>:114: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1242: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training epoch 1/10\n",
            "[100]: loss:5.46\n",
            "[200]: loss:4.81\n",
            "Average epoch loss:5.003604946533839\n",
            "Time used for epoch 1: 1 m 29 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 1.95\n",
            "Time used for evaluate on dev set: 0 m 6 s\n",
            "Starting training epoch 2/10\n",
            "[100]: loss:4.22\n",
            "[200]: loss:4.10\n",
            "Average epoch loss:4.123744910955429\n",
            "Time used for epoch 2: 1 m 28 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 1.66\n",
            "Time used for evaluate on dev set: 0 m 6 s\n",
            "Starting training epoch 3/10\n",
            "[100]: loss:3.96\n",
            "[200]: loss:3.88\n",
            "Average epoch loss:3.894476612408956\n",
            "Time used for epoch 3: 1 m 11 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 2.32\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 4/10\n",
            "[100]: loss:3.78\n",
            "[200]: loss:3.72\n",
            "Average epoch loss:3.727801298101743\n",
            "Time used for epoch 4: 0 m 53 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 2.63\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 5/10\n",
            "[100]: loss:3.65\n",
            "[200]: loss:3.61\n",
            "Average epoch loss:3.6099854240814846\n",
            "Time used for epoch 5: 0 m 56 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 2.88\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 6/10\n",
            "[100]: loss:3.55\n",
            "[200]: loss:3.52\n",
            "Average epoch loss:3.514710689584414\n",
            "Time used for epoch 6: 0 m 56 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 3.56\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 7/10\n",
            "[100]: loss:3.47\n",
            "[200]: loss:3.44\n",
            "Average epoch loss:3.4310897399981815\n",
            "Time used for epoch 7: 0 m 55 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 3.92\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 8/10\n",
            "[100]: loss:3.40\n",
            "[200]: loss:3.37\n",
            "Average epoch loss:3.3641176263491315\n",
            "Time used for epoch 8: 0 m 55 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 3.86\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 9/10\n",
            "[100]: loss:3.34\n",
            "[200]: loss:3.32\n",
            "Average epoch loss:3.3083722720543545\n",
            "Time used for epoch 9: 0 m 56 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 4.14\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 10/10\n",
            "[100]: loss:3.29\n",
            "[200]: loss:3.27\n",
            "Average epoch loss:3.2596866716941197\n",
            "Time used for epoch 10: 0 m 56 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 4.47\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Training finished!\n",
            "Time used for training: 11 m 16 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 4.75\n",
            "Time used for evaluate on test set: 0 m 3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0WVIlTzsby6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0b84b124-e4f6-43fe-adf1-79f7dd20cccf"
      },
      "cell_type": "code",
      "source": [
        "# Sample of the output from the attentionless model:\n",
        "\n",
        "source_batches, target_batches = test_data\n",
        "print(f\"Input sentence: {source_batches[0]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sentence: ([[1687, 1070, 341, 35, 73, 50, 222, 996, 430, 147, 112, 1350, 633, 1187, 457, 84, 106, 1859, 52, 358, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [236, 543, 27, 988, 791, 72, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [236, 188, 189, 139, 140, 130, 126, 419, 650, 236, 61, 396, 109, 220, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [454, 134, 861, 71, 427, 519, 504, 37, 542, 150, 855, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [332, 175, 71, 109, 4, 319, 410, 82, 565, 881, 73, 50, 39, 40, 46, 15, 73, 185, 37, 4, 1403, 0, 1, 19, 16, 1095, 1098, 33, 2032, 2032], [75, 319, 540, 108, 37, 188, 71, 62, 108, 109, 110, 111, 186, 117, 71, 250, 12, 182, 10, 442, 377, 4, 695, 236, 1318, 33, 2032, 2032, 2032, 2032], [10, 23, 73, 113, 745, 633, 4, 229, 595, 1089, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [289, 396, 71, 14, 109, 220, 311, 7, 745, 633, 185, 3, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [14, 872, 140, 4, 212, 179, 649, 14, 11, 195, 187, 62, 14, 1187, 108, 1450, 932, 806, 168, 1, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [14, 711, 275, 108, 481, 409, 280, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [3, 100, 71, 77, 681, 109, 220, 46, 621, 304, 62, 758, 968, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [1000, 149, 1391, 46, 481, 159, 100, 332, 745, 633, 119, 109, 194, 1289, 19, 108, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [1137, 633, 61, 37, 248, 23, 422, 10, 112, 384, 73, 114, 115, 1095, 1098, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [126, 228, 37, 4, 581, 389, 390, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [150, 71, 118, 318, 15, 188, 7, 13, 114, 115, 1095, 1098, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [111, 37, 4, 887, 1083, 220, 179, 274, 396, 83, 72, 105, 170, 13, 481, 2033, 396, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [490, 491, 316, 71, 13, 1031, 1032, 37, 639, 421, 633, 321, 12, 20, 114, 115, 1095, 1098, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [13, 1033, 1316, 463, 41, 1031, 1032, 37, 13, 481, 25, 19, 114, 115, 62, 13, 586, 490, 46, 381, 140, 48, 16, 223, 257, 33, 2032, 2032, 2032, 2032], [150, 316, 71, 20, 409, 280, 117, 12, 745, 633, 71, 1031, 1032, 1095, 1098, 180, 421, 37, 75, 429, 12, 13, 44, 45, 7, 114, 115, 1095, 1096, 33], [13, 1031, 1032, 387, 717, 130, 433, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [126, 37, 4, 93, 229, 2033, 2033, 447, 1046, 709, 62, 522, 1171, 41, 13, 84, 370, 0, 1, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [44, 45, 23, 46, 1606, 169, 1105, 1421, 31, 1735, 62, 108, 23, 324, 889, 82, 83, 16, 506, 110, 50, 13, 754, 1337, 496, 219, 52, 717, 33, 2032], [236, 61, 675, 37, 242, 243, 71, 48, 107, 71, 4, 142, 201, 71, 649, 650, 134, 592, 195, 187, 6, 84, 61, 467, 126, 749, 419, 2032, 2032, 2032], [20, 1417, 370, 223, 224, 23, 981, 649, 862, 109, 41, 4, 171, 172, 675, 833, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [248, 118, 10, 64, 65, 426, 408, 37, 20, 1031, 1032, 217, 218, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [107, 150, 130, 126, 13, 1316, 463, 106, 37, 13, 159, 1335, 71, 222, 528, 71, 13, 106, 902, 71, 13, 147, 217, 71, 2033, 2032, 2032, 2032, 2032, 2032], [62, 13, 586, 490, 639, 421, 12, 13, 1291, 112, 114, 204, 13, 537, 811, 71, 537, 811, 2008, 109, 1797, 25, 163, 537, 25, 41, 159, 1335, 2033, 2032], [62, 118, 10, 119, 725, 116, 12, 159, 1335, 1547, 31, 1025, 1589, 324, 352, 146, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [107, 150, 71, 1031, 1032, 217, 218, 2033, 1447, 140, 13, 153, 154, 19, 20, 1291, 112, 114, 204, 13, 537, 811, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [41, 389, 390, 125, 61, 71, 14, 109, 220, 207, 30, 4, 581, 754, 1337, 84, 106, 113, 4, 170, 13, 586, 490, 522, 1171, 33, 2032, 2032, 2032, 2032], [56, 46, 754, 1337, 186, 117, 419, 56, 109, 1003, 243, 1219, 697, 134, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [135, 56, 46, 490, 1023, 113, 55, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 16, 98, 1509, 105, 220, 529, 758, 1269, 140, 650, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [286, 229, 54, 71, 20, 592, 37, 222, 758, 1269, 186, 194, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 547, 126, 37, 4, 427, 428, 675, 565, 881, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 108, 109, 112, 114, 1230, 621, 991, 113, 241, 1895, 1896, 19, 114, 115, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [126, 37, 4, 1851, 234, 64, 105, 220, 316, 4, 114, 115, 668, 328, 331, 159, 185, 186, 117, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [107, 150, 71, 118, 10, 302, 430, 113, 4, 147, 217, 1417, 370, 109, 899, 1923, 1181, 1291, 112, 114, 7, 529, 217, 218, 50, 70, 2033, 33, 2032, 2032], [126, 37, 20, 159, 1335, 352, 146, 649, 83, 72, 4, 52, 53, 71, 62, 310, 170, 118, 10, 195, 187, 46, 37, 2033, 33, 2032, 2032, 2032, 2032, 2032], [126, 37, 1031, 1032, 2033, 118, 10, 23, 98, 183, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 275, 140, 71, 108, 46, 1027, 1073, 185, 3, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [75, 12, 14, 154, 263, 678, 998, 72, 71, 394, 311, 7, 4, 507, 40, 666, 384, 890, 992, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 118, 318, 23, 30, 13, 599, 913, 19, 754, 1337, 139, 140, 130, 126, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [333, 334, 118, 318, 98, 183, 73, 1027, 1073, 71, 150, 248, 61, 586, 112, 186, 117, 73, 16, 758, 1269, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [313, 71, 529, 217, 218, 585, 73, 529, 821, 990, 12, 13, 537, 811, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [126, 37, 1007, 1319, 68, 29, 19, 16, 758, 1269, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 109, 20, 153, 154, 54, 55, 175, 12, 182, 14, 157, 168, 16, 758, 1269, 14, 109, 46, 50, 529, 217, 218, 33, 2032, 2032, 2032, 2032, 2032, 2032], [62, 1031, 1032, 543, 481, 425, 6, 179, 16, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [14, 154, 263, 140, 46, 464, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [248, 61, 1070, 73, 160, 530, 318, 109, 16, 758, 1269, 130, 2033, 2033, 113, 1017, 170, 2033, 1107, 37, 2033, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [333, 334, 71, 20, 188, 118, 10, 179, 16, 23, 157, 140, 41, 44, 45, 19, 118, 10, 37, 529, 758, 1269, 23, 18, 72, 116, 369, 2033, 33, 2032], [248, 61, 12, 182, 118, 10, 1411, 725, 4, 1025, 241, 365, 366, 73, 123, 537, 811, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [363, 14, 11, 157, 168, 28, 1204, 41, 4, 1031, 1032, 529, 217, 218, 71, 316, 126, 37, 20, 188, 14, 240, 26, 33, 2032, 2032, 2032, 2032, 2032, 2032], [248, 100, 179, 16, 134, 353, 323, 187, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [394, 75, 10, 319, 540, 104, 229, 287, 140, 389, 390, 223, 224, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [394, 311, 7, 632, 1204, 41, 13, 398, 1652, 109, 241, 1796, 54, 55, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [223, 224, 185, 150, 71, 16, 758, 1269, 1204, 41, 1031, 1032, 217, 218, 62, 183, 1511, 649, 13, 1033, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [150, 316, 71, 118, 318, 23, 195, 140, 248, 188, 3, 225, 157, 168, 63, 34, 13, 1031, 1032, 758, 1269, 61, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [313, 71, 108, 228, 140, 379, 2033, 537, 811, 162, 430, 109, 735, 446, 758, 1269, 406, 220, 834, 413, 116, 369, 19, 2033, 33, 2032, 2032, 2032, 2032, 2032], [56, 60, 622, 37, 13, 754, 1337, 84, 106, 130, 1075, 267, 62, 1859, 52, 358, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 2033, 100, 946, 947, 191, 139, 169, 924, 1387, 72, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [228, 109, 2033, 222, 162, 430, 41, 1293, 71, 62, 56, 38, 55, 109, 735, 446, 248, 249, 552, 51, 413, 116, 369, 19, 2033, 33, 2032, 2032, 2032, 2032], [14, 950, 140, 46, 248, 188, 50, 20, 188, 10, 893, 280, 1076, 419, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [313, 71, 529, 758, 1269, 113, 1025, 241, 208, 185, 14, 30, 37, 82, 173, 1289, 143, 113, 266, 267, 5, 680, 117, 33, 2032, 2032, 2032, 2032, 2032, 2032], [248, 61, 109, 111, 379, 1027, 1073, 19, 108, 109, 671, 37, 490, 491, 19, 16, 224, 754, 1337, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [44, 45, 19, 118, 10, 265, 185, 414, 37, 4, 599, 1714, 19, 349, 350, 350, 1272, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [108, 134, 240, 37, 4, 288, 870, 7, 398, 698, 33, 414, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [62, 248, 61, 189, 302, 430, 4, 229, 141, 1421, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [332, 196, 657, 280, 61, 228, 178, 302, 430, 71, 107, 150, 118, 318, 277, 240, 1019, 1020, 467, 225, 118, 318, 30, 13, 490, 491, 430, 431, 33, 2032], [120, 10, 180, 649, 151, 191, 37, 4, 425, 6, 18, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 179, 16, 1155, 513, 379, 112, 359, 1095, 1098, 61, 12, 182, 12, 4, 170, 359, 552, 46, 195, 30, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [179, 649, 82, 1095, 1096, 71, 118, 318, 176, 240, 1024, 140, 71, 637, 315, 20, 440, 248, 71, 2033, 1635, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [332, 248, 61, 228, 37, 376, 321, 19, 732, 401, 1219, 10, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [2033, 2033, 367, 726, 1482, 71, 171, 640, 62, 823, 824, 7, 1329, 68, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 351, 1035, 92, 204, 1512, 41, 945, 944, 1584, 618, 19, 4, 457, 122, 18, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [517, 100, 23, 37, 82, 2033, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 134, 654, 37, 1473, 334, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [228, 654, 944, 945, 100, 454, 854, 2033, 454, 1354, 1372, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [63, 34, 420, 341, 41, 848, 127, 2003, 258, 2033, 1171, 1236, 1926, 10, 71, 20, 760, 692, 1475, 202, 368, 915, 63, 34, 420, 341, 2032, 2032, 2032, 2032], [10, 207, 140, 692, 1475, 62, 30, 4, 980, 815, 775, 19, 457, 122, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 765, 207, 2004, 2004, 92, 108, 12, 73, 225, 108, 169, 624, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [126, 37, 427, 236, 19, 288, 417, 10, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 23, 37, 604, 482, 100, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 37, 4, 154, 322, 54, 19, 4, 222, 566, 1386, 41, 640, 448, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [10, 37, 222, 1532, 724, 100, 71, 62, 10, 37, 604, 438, 100, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [118, 318, 228, 178, 286, 7, 4, 350, 19, 108, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [414, 640, 448, 414, 4, 222, 14, 19, 10, 23, 286, 414, 137, 138, 134, 240, 37, 7, 944, 945, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [175, 179, 274, 37, 130, 16, 756, 631, 71, 16, 756, 631, 19, 401, 157, 33, 414, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [14, 15, 100, 37, 1009, 472, 202, 1028, 62, 800, 547, 266, 434, 296, 469, 117, 41, 100, 222, 1257, 134, 472, 473, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [454, 387, 1354, 225, 240, 1102, 312, 461, 462, 100, 380, 311, 414, 10, 882, 1028, 414, 356, 414, 10, 109, 220, 1028, 41, 457, 122, 61, 33, 414, 2032], [332, 191, 109, 1009, 472, 54, 1009, 472, 624, 399, 222, 576, 857, 71, 946, 947, 191, 988, 791, 72, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [108, 353, 386, 952, 73, 1025, 14, 191, 134, 11, 311, 259, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [100, 119, 37, 248, 175, 222, 437, 130, 2033, 123, 1038, 10, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [265, 185, 37, 202, 1028, 50, 41, 140, 276, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [118, 318, 23, 710, 228, 30, 4, 350, 19, 196, 640, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [126, 37, 773, 286, 19, 16, 2033, 2033, 19, 401, 157, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [16, 623, 112, 384, 41, 202, 229, 175, 118, 318, 991, 995, 113, 13, 724, 401, 640, 448, 41, 806, 489, 19, 60, 6, 61, 33, 2032, 2032, 2032, 2032], [126, 37, 690, 118, 318, 112, 1319, 1478, 1956, 62, 857, 1636, 37, 134, 220, 1149, 46, 33, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [834, 25, 163, 222, 1532, 724, 31, 138, 186, 194, 37, 726, 1482, 62, 560, 584, 33, 1189, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032], [895, 25, 163, 13, 724, 401, 41, 640, 448, 180, 639, 37, 29, 528, 94, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032, 2032]], [21, 7, 15, 12, 28, 26, 11, 13, 21, 8, 14, 17, 16, 8, 13, 18, 19, 26, 30, 8, 20, 29, 27, 17, 14, 25, 29, 17, 23, 26, 15, 8, 12, 13, 11, 16, 20, 28, 25, 11, 11, 20, 16, 22, 15, 11, 24, 10, 7, 22, 29, 18, 24, 9, 14, 15, 21, 23, 25, 16, 12, 26, 14, 24, 20, 18, 12, 11, 29, 12, 22, 22, 13, 14, 16, 6, 6, 11, 26, 15, 13, 9, 6, 15, 14, 10, 21, 17, 23, 29, 21, 12, 13, 10, 12, 12, 26, 18, 18, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Il46Tb3osls-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e0f669c-8c4b-4dc3-8c9f-4e16cd39d8f4"
      },
      "cell_type": "code",
      "source": [
        "print(len(references))\n",
        "print(len(candidates))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rNxMf7KhqKeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bb84ac47-c891-4a84-a3ba-b8adc55d4249"
      },
      "cell_type": "code",
      "source": [
        "# Sample of the output from the attentionless model:\n",
        "\n",
        "source_batches, target_batches = test_data\n",
        "\n",
        "references = []\n",
        "candidates = []\n",
        "vocab = model.target_dict.vocab\n",
        "PAD = model.target_dict.PAD\n",
        "\n",
        "for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "    if i > 1:\n",
        "      break\n",
        "    source_words, source_sent_lens = source\n",
        "    target_words, target_sent_lens = target\n",
        "    infer_target_words = [[PAD for i in range(model.max_target_step)] for b in target_words]\n",
        "\n",
        "    fd = {model.source_words: source_words, model.target_words: infer_target_words,\n",
        "                model.source_sent_lens: source_sent_lens,\n",
        "                model.is_training: False}\n",
        "    predictions = model.sess.run(model.predictions,feed_dict=fd)\n",
        "\n",
        "    references.extend(model.get_target_sentences(target_words,vocab,reference=True))\n",
        "    candidates.extend(model.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "for j,ref in enumerate(references):\n",
        "  if j > 4:\n",
        "    break\n",
        "  print(f\"Human translation: {' '.join(ref[0])}\")\n",
        "  print(f\"Model's translation: {' '.join(candidates[j])}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Human translation: the second quote is from the head of the u.k. financial services <unk> .\n",
            "Model's translation: <unk> <unk> : <unk> <unk> , <unk> <unk> , <unk> <unk> .\n",
            "Human translation: it gets worse .\n",
            "Model's translation: the <unk> is <unk> .\n",
            "Human translation: what &apos;s happening here ? how can this be possible ?\n",
            "Model's translation: what &apos;s the <unk> of the <unk> of the <unk> ?\n",
            "Human translation: unfortunately , the answer is yes .\n",
            "Model's translation: there &apos;s no longer <unk> .\n",
            "Human translation: but there &apos;s an <unk> solution which is coming from what is known as the science of <unk> .\n",
            "Model's translation: but the <unk> of the <unk> , the <unk> of the <unk> is <unk> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aJRLHCPonu_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Best BLEU score for the attentionless model was 4.75. We can see from the sample output that the model is performing very poorly. The BLEU score is probabaly being very generous because the model is overly reliant on the unknown character, it has therefore learnt to put in common words such as \"the\" and \"is\" combined with unknown tokens. This model would not be viable for real translation. "
      ]
    },
    {
      "metadata": {
        "id": "CqVAOeQfjiqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "h3U1BNMih3w4",
        "colab_type": "code",
        "outputId": "d3f0a61b-b95d-4a42-bbce-0f4035beba0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        }
      },
      "cell_type": "code",
      "source": [
        "# Run with attention\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "use_attention = True\n",
        "\n",
        "model2 = NmtModel(source_dict,target_dict,use_attention)\n",
        "model2.build()\n",
        "model2.train(train_data,dev_data,test_data,10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source vocab: 2034, target vocab:2506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training epoch 1/10\n",
            "[100]: loss:5.02\n",
            "[200]: loss:4.36\n",
            "Average epoch loss:4.591693803668022\n",
            "Time used for epoch 1: 1 m 2 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 2.19\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 2/10\n",
            "[100]: loss:4.03\n",
            "[200]: loss:3.90\n",
            "Average epoch loss:3.9303970058759052\n",
            "Time used for epoch 2: 1 m 1 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 3.10\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 3/10\n",
            "[100]: loss:3.73\n",
            "[200]: loss:3.60\n",
            "Average epoch loss:3.62818458378315\n",
            "Time used for epoch 3: 1 m 0 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 5.17\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 4/10\n",
            "[100]: loss:3.44\n",
            "[200]: loss:3.32\n",
            "Average epoch loss:3.335816715161006\n",
            "Time used for epoch 4: 1 m 0 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 8.28\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 5/10\n",
            "[100]: loss:3.16\n",
            "[200]: loss:3.08\n",
            "Average epoch loss:3.083642295996348\n",
            "Time used for epoch 5: 1 m 0 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 9.70\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 6/10\n",
            "[100]: loss:2.96\n",
            "[200]: loss:2.90\n",
            "Average epoch loss:2.893921588857969\n",
            "Time used for epoch 6: 1 m 0 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 10.40\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 7/10\n",
            "[100]: loss:2.80\n",
            "[200]: loss:2.75\n",
            "Average epoch loss:2.7396998395522436\n",
            "Time used for epoch 7: 1 m 1 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 11.45\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 8/10\n",
            "[100]: loss:2.67\n",
            "[200]: loss:2.64\n",
            "Average epoch loss:2.6211642334858576\n",
            "Time used for epoch 8: 1 m 0 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 11.84\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 9/10\n",
            "[100]: loss:2.56\n",
            "[200]: loss:2.53\n",
            "Average epoch loss:2.5178152958552045\n",
            "Time used for epoch 9: 1 m 1 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 12.05\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 10/10\n",
            "[100]: loss:2.47\n",
            "[200]: loss:2.45\n",
            "Average epoch loss:2.427581396698952\n",
            "Time used for epoch 10: 1 m 1 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 12.33\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Training finished!\n",
            "Time used for training: 10 m 44 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 13.01\n",
            "Time used for evaluate on test set: 0 m 3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ixSm63y4xAWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "be81cb0f-fa79-460b-990f-ad26e033d2a7"
      },
      "cell_type": "code",
      "source": [
        "# Sample of the output from the attention model:\n",
        "\n",
        "source_batches, target_batches = test_data\n",
        "\n",
        "references = []\n",
        "candidates = []\n",
        "vocab = model2.target_dict.vocab\n",
        "PAD = model2.target_dict.PAD\n",
        "\n",
        "for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "    if i > 1:\n",
        "      break\n",
        "    source_words, source_sent_lens = source\n",
        "    target_words, target_sent_lens = target\n",
        "    infer_target_words = [[PAD for i in range(model2.max_target_step)] for b in target_words]\n",
        "\n",
        "    fd = {model2.source_words: source_words, model2.target_words: infer_target_words,\n",
        "                model2.source_sent_lens: source_sent_lens,\n",
        "                model2.is_training: False}\n",
        "    predictions = model2.sess.run(model2.predictions,feed_dict=fd)\n",
        "\n",
        "    references.extend(model2.get_target_sentences(target_words,vocab,reference=True))\n",
        "    candidates.extend(model2.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "for j,ref in enumerate(references):\n",
        "  if j > 4:\n",
        "    break\n",
        "  print(f\"Human translation: {' '.join(ref[0])}\")\n",
        "  print(f\"Model's translation: {' '.join(candidates[j])}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Human translation: the second quote is from the head of the u.k. financial services <unk> .\n",
            "Model's translation: the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> .\n",
            "Human translation: it gets worse .\n",
            "Model's translation: the <unk> is much more .\n",
            "Human translation: what &apos;s happening here ? how can this be possible ?\n",
            "Model's translation: what &apos;s happening ? what ? what can be ?\n",
            "Human translation: unfortunately , the answer is yes .\n",
            "Model's translation: it &apos;s not , the answer is right .\n",
            "Human translation: but there &apos;s an <unk> solution which is coming from what is known as the science of <unk> .\n",
            "Model's translation: but there &apos;s a lot of <unk> from the very large way to be a <unk> of <unk> <unk> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ARYP1eDYpJ6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Best BLEU score for the attention based model was 13.01, which is a great improvement over the 4.75 of the attentionless model. This shows that attention has made a significant improvement to the model. We can see from the sample outputs that the translations have improved as well. There are some correct words being used and the 3rd and 4th examples are fairly similar to the human translations. We so see that there are still a large number of unknown tokens used by the model, perhaps more training or adapting the loss function to punish the existence of repeated unknown tokens, might help. "
      ]
    },
    {
      "metadata": {
        "id": "HsK1sxRKh3w-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}