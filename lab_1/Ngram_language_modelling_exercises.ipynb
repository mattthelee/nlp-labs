{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram lab\n",
    "\n",
    "In this lab you will do 4 excercises building ngram language models:\n",
    "1. A Maximum Liklihood Expectation (MLE) unigram model (10 marks)\n",
    "2. A bigram model with add-one smoothing (10 marks)\n",
    "3. A bigram model with general additive smoothing (10 marks)\n",
    "4. (BONUS) A trigram model with Kneser-Ney snoothing (10 marks)\n",
    "\n",
    "There are some examples using small corpus as seen in the lecture first, before you do the exercises using the following 3 files with line-separated text to train the bigger language models on:\n",
    "* training data -- \"switchboard_lm_training.txt\"\n",
    "* heldout data -- \"switchboard_lm_heldout.txt\"\n",
    "* test data -- \"switchboard_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division  # for python 2 this is needed\n",
    "from __future__ import print_function # for python 2 this is needed\n",
    "from collections import Counter\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful methods\n",
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### Examples ############################\n",
    "# Example set of sentences (corpus) from the lecture slides\n",
    "sentences = [\n",
    "            \"I am Sam\",\n",
    "            \"Sam I am\",\n",
    "            \"I do not like green eggs and ham\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 0.0588235294118\n",
      "do 0.0588235294118\n",
      "like 0.0588235294118\n",
      "Sam 0.117647058824\n",
      "I 0.176470588235\n",
      "eggs 0.0588235294118\n",
      "am 0.117647058824\n",
      "green 0.0588235294118\n",
      "not 0.0588235294118\n",
      "</s> 0.176470588235\n",
      "ham 0.0588235294118\n",
      "check if adds to 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example 1. Build a unigram MLE language model from a simple corpus\n",
    "unigrams = Counter()\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    for w in words:\n",
    "        unigrams[w] +=1\n",
    "unigram_total = sum(unigrams.values())\n",
    "# check that all add to one\n",
    "check_if_adds_to_1 = 0\n",
    "for k, v in unigrams.items():\n",
    "    print(k, v/unigram_total)\n",
    "    check_if_adds_to_1 += (v/unigram_total)\n",
    "print(\"check if adds to 1:\", check_if_adds_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy 3.29277019394\n",
      "perplexity 9.79992150705\n"
     ]
    }
   ],
   "source": [
    "# get the perplexity of those same sentences\n",
    "# according to the model\n",
    "# perplexity is always equal to two to the power of the entropy\n",
    "# where entropy is the negative sum of all log probabilities from the model\n",
    "N = 0 # total number of words\n",
    "s = 0  # entropy\n",
    "for sent in sentences:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    for w in words:\n",
    "        N += 1\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, 2)  # get the log of the prob to base 2\n",
    "        s += (-logprob)\n",
    "perplexity = 2 ** (s/N)\n",
    "print(\"cross entropy\", s/N)\n",
    "print(\"perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ['and']\n",
      "check if sums to 1? 1.0\n",
      "context ['am']\n",
      "check if sums to 1? 1.0\n",
      "context ['ham']\n",
      "check if sums to 1? 1.0\n",
      "context ['not']\n",
      "check if sums to 1? 1.0\n",
      "context ['I']\n",
      "check if sums to 1? 1.0\n",
      "context ['Sam']\n",
      "check if sums to 1? 1.0\n",
      "context ['like']\n",
      "check if sums to 1? 1.0\n",
      "context ['<s>']\n",
      "check if sums to 1? 1.0\n",
      "context ['do']\n",
      "check if sums to 1? 1.0\n",
      "context ['eggs']\n",
      "check if sums to 1? 1.0\n",
      "context ['green']\n",
      "check if sums to 1? 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example 2. Get probabilities for bigrams\n",
    "bigrams = Counter()\n",
    "bigram_context = Counter() # like unigrams, but the previous word only (so includes the start symbol)\n",
    "delta = 1  # delta is order - 1\n",
    "for s in sentences:\n",
    "    words = tokenize_sentence(s, 2)\n",
    "    for i in range(delta, len(words)):\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        bigrams[glue_tokens(ngram, 2)] +=1\n",
    "        bigram_context[glue_tokens(context, 1)] += 1\n",
    "bigram_total = sum(bigrams.values())\n",
    "\n",
    "# check if each bigram continuation sums to tomorrow\n",
    "for context, v in bigram_context.items():\n",
    "    context = unglue_tokens(context, 1)\n",
    "    print(\"context\", context)\n",
    "    check_ngram_total_sums_to_1 = 0\n",
    "    # for a given context the continuation probabilities \n",
    "    # over the whole vocab should sum to 1\n",
    "    for u in unigrams.keys():\n",
    "        ngram = context + [u]\n",
    "        numerator = bigrams[glue_tokens(ngram, 2)] + 1\n",
    "        denominator = v + (1 * len(bigram_context.items()))\n",
    "        p = numerator / denominator\n",
    "        # print(glue_tokens(ngram, 2), p)\n",
    "        check_ngram_total_sums_to_1 += p\n",
    "    print(\"check if sums to 1?\", check_ngram_total_sums_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n",
      "0.333333333333\n",
      "0.666666666667\n",
      "0.5\n",
      "0.5\n",
      "0.333333333333\n"
     ]
    }
   ],
   "source": [
    "# Check the estimates for the lecture examples:\n",
    "# p(I|<s>)\n",
    "# p(Sam|<s>)\n",
    "# p(am|I)\n",
    "# p(</s>|Sam)\n",
    "# p(Sam|am)\n",
    "# p(do|I)\n",
    "\n",
    "def bigram_MLE(ngram):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE estimation based on the counters\"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    p = numerator / denominator\n",
    "    return p\n",
    "\n",
    "print(bigram_MLE(['<s>','I']))\n",
    "print(bigram_MLE(['<s>', 'Sam']))\n",
    "print(bigram_MLE(['I', 'am']))\n",
    "print(bigram_MLE(['Sam', '</s>']))\n",
    "print(bigram_MLE(['am', 'Sam']))\n",
    "print(bigram_MLE(['I', 'do']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy 0.559398529666\n",
      "perplexity 1.47365471155\n"
     ]
    }
   ],
   "source": [
    "# We we use the bigram and model to get the perplexity\n",
    "# of each sentence\n",
    "N = 0 # total number of words\n",
    "s = 0 # entropy\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, 2)\n",
    "    for i in range(delta, len(words)):\n",
    "        N += 1\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        numerator = bigrams.get(glue_tokens(ngram, 2)) \n",
    "        denominator = bigram_context.get(glue_tokens(context, 1))\n",
    "        prob = numerator / denominator\n",
    "        s += (-log(prob, 2))  # add the neg log prob\n",
    "perplexity = 2 ** (s/N)\n",
    "print(\"cross entropy\", s/N)\n",
    "print(\"perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from __future__ import division "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Cross entropy', 8.33258046312375)\n",
      "('Perplexity', 322.37151533902477)\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# Exercise 1. Unigram MLE model from a bigger corpus\n",
    "#\n",
    "# Write code to read in the file 'switchboard_language_model_train.txt' which has preprocessed text on each line.\n",
    "# Populate a unigram language model based on that data for an MLE estimation using a Counter (see Example 1 above).\n",
    "# Keep updating the Counter for the model by reading in the data\n",
    "# in 'switchboard_language_model_heldout.txt', however, this time include\n",
    "# an unknown word token <unk/> for any words appearing in this data\n",
    "# that were not in the first training data.\n",
    "# Using this model, return the perplexity of the ENTIRE test corpus 'switchboard_lanaguage_model_test.txt', including\n",
    "# replacing words unknown by the model with <unk/> to avoid not getting a perplexity score\n",
    "##############################################\n",
    "trainingFile = open(\"switchboard_lm_train.txt\",\"r\")\n",
    "totalWordCount = 0\n",
    "unigramCounter = Counter()\n",
    "for line in trainingFile:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    for w in words:\n",
    "        unigramCounter[w] += 1\n",
    "        totalWordCount += 1\n",
    "\n",
    "trainingFile.close()\n",
    "#print( unigramCounter[\"the\"])\n",
    "\n",
    "heldoutFile = open(\"switchboard_lm_heldout.txt\",\"r\")\n",
    "for line in heldoutFile:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    for w in words:\n",
    "        totalWordCount += 1\n",
    "        if unigramCounter[w] == 0:\n",
    "            unigramCounter[\"<unk/>\"] += 1\n",
    "        else:\n",
    "            unigramCounter[w] += 1\n",
    "heldoutFile.close()\n",
    "\n",
    "# Calculate the perplexity of the newly created unigram model\n",
    "\n",
    "entropy = 0\n",
    "testWordCount = 0\n",
    "testingFile = open(\"switchboard_lm_test.txt\",\"r\")\n",
    "for line in testingFile:\n",
    "    words = tokenize_sentence(line, 1)\n",
    "    for w in words:\n",
    "        testWordCount += 1\n",
    "        if unigramCounter[w] == 0:\n",
    "            prob = unigramCounter[\"<unk/>\"]/totalWordCount\n",
    "        else:\n",
    "            prob = unigramCounter[w]/totalWordCount\n",
    "        logprob = log(prob,2)\n",
    "        entropy += (-logprob)\n",
    "testingFile.close()\n",
    "\n",
    "\n",
    "perplexity = 2 ** (entropy/testWordCount)\n",
    "print(\"Cross entropy\",entropy/testWordCount)\n",
    "print(\"Perplexity\", perplexity)\n",
    "\n",
    "#print( unigramCounter[\"<unk/>\"])\n",
    "print ( \"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709545\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Exercise 2. Bigram model with add-one smoothing\n",
    "#\n",
    "# Change your method for reading in and training a language model so it works for bigrams\n",
    "# However, it should use add-one smoothing (see the lecture notes and Jurafsky & Martin Chapter 3)\n",
    "# Remember this involves using the vocabulary size.\n",
    "# Obtain the perplexity score on the test data as above for this bigram model\n",
    "# Use the heldout corpus to get estimations for bigrams with unknown words as you did in Ex. 1.\n",
    "##############################################\n",
    "\n",
    "trainingFile = open(\"switchboard_lm_train.txt\",\"r\")\n",
    "\n",
    "bigramCounter = Counter()\n",
    "bigramContextCounter = Counter()\n",
    "\n",
    "for line in trainingFile:\n",
    "    words= tokenize_sentence(line, 2)\n",
    "    for w in range(delta,len(words):\n",
    "        context = words[w-delta:i]\n",
    "        \n",
    "trainingFile.close()\n",
    "\n",
    "'''# Example 2. Get probabilities for bigrams\n",
    "bigrams = Counter()\n",
    "bigram_context = Counter() # like unigrams, but the previous word only (so includes the start symbol)\n",
    "delta = 1  # delta is order - 1\n",
    "for s in sentences:\n",
    "    words = tokenize_sentence(s, 2)\n",
    "    for i in range(delta, len(words)):\n",
    "        context = words[i-delta:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        bigrams[glue_tokens(ngram, 2)] +=1\n",
    "        bigram_context[glue_tokens(context, 1)] += 1\n",
    "bigram_total = sum(bigrams.values())\n",
    "\n",
    "# check if each bigram continuation sums to tomorrow\n",
    "for context, v in bigram_context.items():\n",
    "    context = unglue_tokens(context, 1)\n",
    "    print(\"context\", context)\n",
    "    check_ngram_total_sums_to_1 = 0\n",
    "    # for a given context the continuation probabilities \n",
    "    # over the whole vocab should sum to 1\n",
    "    for u in unigrams.keys():\n",
    "        ngram = context + [u]\n",
    "        numerator = bigrams[glue_tokens(ngram, 2)] + 1\n",
    "        denominator = v + (1 * len(bigram_context.items()))\n",
    "        p = numerator / denominator\n",
    "        # print(glue_tokens(ngram, 2), p)\n",
    "        check_ngram_total_sums_to_1 += p\n",
    "    print(\"check if sums to 1?\", check_ngram_total_sums_to_1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Exercise 3. Bigram model with general additive (Lidstone) smoothing\n",
    "#\n",
    "# Modify your code from exercise 2 such that it generalizes beyond\n",
    "# adding 1 to all counts, but can add differing counts instead.\n",
    "# Experiment with different values (e.g. 0.2, 0.4, 0.6, 0.8) and\n",
    "# report the perplexity scores for all the different values you test.\n",
    "# See if you can find the best amount to add.\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Exercise 4. Trigram model with Kneser-Ney smoothing\n",
    "#\n",
    "# Kneser-Ney smoothing is a state-of-the-art technique for smoothing n-gram models.\n",
    "# The algorithm is quite complicated, and is implemented for you below for training\n",
    "# on the training data (ngrams_interpolated_kneser_ney)\n",
    "# The application at test time is done with the method kneser_ney_ngram_prob using the trained Counters.\n",
    "# See if you can follow how it works, and refer to the below article on QM plus (pages 7-8 particularly):\n",
    "# \"A Bit of Progress in Language Modeling\" - Joshua T. Goodman\n",
    "#\n",
    "# In this exercise, use the heldout data file to further train the model after you run the below\n",
    "# but also replace words unseen in the first training data with the unknown word\n",
    "# token <unk/>, as you should have done above.\n",
    "# You do not need to modify the algorithms below, but just use them to train the Counters.\n",
    "# Then obtain the perplexity score on the test data with your model and compare it to the other models.\n",
    "# Experiment with different Discount weights (0.7 is used below, which works quite well),\n",
    "# and even different values of n (e.g. 4-gram, 5-gram)\n",
    "# to see if you can get the lowest possible perplexity score.\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney smoothing\n",
    "order = 3\n",
    "discount = 0.7\n",
    "\n",
    "unigram_denominator = 0\n",
    "ngram_numerator_map = Counter() \n",
    "ngram_denominator_map = Counter() \n",
    "ngram_non_zero_map = Counter()\n",
    "\n",
    "\n",
    "def ngrams_interpolated_kneser_ney(tokens,\n",
    "                                   order,\n",
    "                                   ngram_numerator_map,\n",
    "                                   ngram_denominator_map,\n",
    "                                   ngram_non_zero_map,\n",
    "                                   unigram_denominator):\n",
    "    \"\"\"This function counts the n-grams in tokens and also record the\n",
    "    lower order non zero counts necessary for interpolated Kneser-Ney \\\n",
    "    smoothing,\n",
    "    taken from Goodman 2001 and generalized to arbitrary orders\"\"\"\n",
    "    for i in xrange(order-1,len(tokens)): # tokens should have a prefix of order - 1\n",
    "        #print i\n",
    "        for d in xrange(order,0,-1): #go through all the different 'n's\n",
    "            if d == 1:\n",
    "                unigram_denominator += 1\n",
    "                ngram_numerator_map[glue_tokens(tokens[i],d)] += 1\n",
    "            else:\n",
    "                den_key = glue_tokens(tokens[i-(d-1) : i], d)\n",
    "                num_key = glue_tokens(tokens[i-(d-1) : i+1], d)\n",
    "    \n",
    "                ngram_denominator_map[den_key] += 1\n",
    "                # we store this value to check if it's 0\n",
    "                tmp = ngram_numerator_map[num_key]\n",
    "                ngram_numerator_map[num_key] += 1 # we increment it\n",
    "                if tmp == 0: # if this is the first time we see this ngram\n",
    "                    #number of types it's been used as a context for\n",
    "                    ngram_non_zero_map[den_key] += 1\n",
    "                else:\n",
    "                    break \n",
    "                    # if the ngram has already been seen\n",
    "                    # we don't go down to lower order models\n",
    "    return ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "corpus = open(\"switchboard_lm_train.txt\")\n",
    "for line in corpus:\n",
    "    tokens = tokenize_sentence(line, order)\n",
    "    ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "            ngrams_interpolated_kneser_ney(tokens,\n",
    "                                           order,\n",
    "                                           ngram_numerator_map,\n",
    "                                           ngram_denominator_map,\n",
    "                                           ngram_non_zero_map,\n",
    "                                           unigram_denominator)\n",
    "corpus.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneser_ney_ngram_prob(ngram, discount, order):\n",
    "    \"\"\"KN smoothed ngram probability from Goodman 2001.\n",
    "    This is run at test time.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token in ngram: \n",
    "        #put unknown token in for unknown words, only form of held \n",
    "        #out est used\n",
    "        if (not ngram_numerator_map.get(glue_tokens(token,1))) \\\n",
    "        and not token ==\"<s>\": #i.e. never seen at all\n",
    "            tokens.append(\"<unk>\")\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    ngram = tokens #we've added our unk tokens\n",
    "\n",
    "    #calculate the unigram prob of the last token \n",
    "    #as it appears as a numerator\n",
    "    #if we've never seen it at all, it defacto will \n",
    "    #have no probability as a numerator\n",
    "    uni_num = ngram_numerator_map.get(glue_tokens(ngram[-1],1))\n",
    "    if uni_num == None:\n",
    "        uni_num = 0\n",
    "    probability = previous_prob = float(uni_num) / float(unigram_denominator)\n",
    "    if probability == 0.0:\n",
    "        print(\"0 prob!\")\n",
    "        print(glue_tokens(ngram[-1],1))\n",
    "        print(ngram)\n",
    "        print(ngram_numerator_map.get(glue_tokens(ngram[-1],1)))\n",
    "        print(unigram_denominator)\n",
    "        raise Exception\n",
    "\n",
    "    # now we compute the higher order probs and interpolate\n",
    "    for d in xrange(2,order+1):\n",
    "        ngram_den = ngram_denominator_map.get(glue_tokens(ngram[-(d):-1],d))\n",
    "        if ngram_den is None:\n",
    "            ngram_den = 0\n",
    "        #for bigrams this is the number of different continuation types\n",
    "        # (number of trigram types with these two words)\n",
    "        if ngram_den != 0: \n",
    "            #if this context (bigram, for trigrams) has never been seen, \n",
    "            #then we can only get unigram est, starts from two, goes up\n",
    "            ngram_num = ngram_numerator_map.get(glue_tokens(ngram[-(d):],d)) \n",
    "            #this is adding one, use get?\n",
    "            if ngram_num is None:\n",
    "                ngram_num = 0\n",
    "            if ngram_num != 0:\n",
    "                current_prob = (ngram_num - discount) / float(ngram_den)\n",
    "            else:\n",
    "                current_prob = 0.0\n",
    "            nonzero = ngram_non_zero_map.get(\n",
    "                                    glue_tokens(ngram[-(d):-1],d))\n",
    "            if nonzero is None:\n",
    "                nonzero = 0\n",
    "            current_prob += nonzero * discount / ngram_den * previous_prob\n",
    "            previous_prob = current_prob\n",
    "            probability = current_prob\n",
    "        else:\n",
    "            #current unseen contexts just give you the unigram \n",
    "            #back..not ideal.. we can learn <unk> from \n",
    "            #held out data though..\n",
    "            probability = previous_prob\n",
    "            break\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
