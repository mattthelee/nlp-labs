{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab10-colabversion-end-to-end-dialogue.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattthelee/nlp-labs/blob/master/nnnlpLab10/Lab10_colabversion_end_to_end_dialogue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9_ZORURKg-fp"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 10: Dialogue Act Tagging\n",
        "\n",
        "Dialogue act (DA) tagging is an important step in the process of developing dialog systems. DA tagging is a problem usually solved by supervised machine learning approaches that all require large amounts of hand labeled data. A wide range of techniques have been investigated for DA tagging. In this lab, we explore two models for DA classification. We are using the Switchboard Dialog Act Corpus for training.\n",
        "Corpus can be downloaded from http://compprag.christopherpotts.net/swda.html.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ziKyA9R4gyw9"
      },
      "cell_type": "markdown",
      "source": [
        "The downloaded dataset should be kept in a data folder in the same directory as this file. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jmTpKt_uefe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0529186-8b27-456f-9a9b-b407a7c97824"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "# tf.enable_eager_execution()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gUZt48JgrE34"
      },
      "cell_type": "markdown",
      "source": [
        "## Advanced: Creating End-To-End Dialogue System"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zE63Q5guuPdA"
      },
      "cell_type": "markdown",
      "source": [
        "In the last section we want to create end-to-end dialogue system, following on from the seq2seq MT labs you've \n",
        "just done. This is an advanced part of the assignment and worth 10 marks (20%) in total. In end-to-end dialogue systems, the encoder represents each utterance with a vector. The utterance vector is the hidden state after the last token of the utterance has been processed. The context LSTM keeps track of past utterances. The hidden state can be explained as the state of the dialogue system. The next utterance prediction is performed by a decoder LSTM, which takes the hidden state of the last LSTM and produces a probability distribution over the tokens in the next utterance. You can take the DA LSTM state of last section as input to a decoder which tries to generate the next utterance. You can add attention and monitor the performance. Instead of evaluating by an automatic evaluation method, you can show us some of the interesting predictions. \n"
      ]
    },
    {
      "metadata": {
        "id": "XgOn8pFQ3uK5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class NmtModel(object):\n",
        "    def __init__(self,source_dict,target_dict,use_attention):\n",
        "        self.num_layers = 2\n",
        "        self.hidden_size = 200\n",
        "        self.embedding_size = 100\n",
        "        self.hidden_dropout_rate=0.2\n",
        "        self.embedding_dropout_rate = 0.2\n",
        "        self.max_target_step = 30\n",
        "        self.vocab_target_size = len(target_dict.vocab)\n",
        "        self.vocab_source_size = len(source_dict.vocab)\n",
        "        self.target_dict = target_dict\n",
        "        self.source_dict = source_dict\n",
        "        self.SOS = target_dict.word2ids['<start>']\n",
        "        self.EOS = target_dict.word2ids['<end>']\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        self.source_words = tf.placeholder(tf.int32,[None,None],\"source_words\")\n",
        "        self.target_words = tf.placeholder(tf.int32,[None,None],\"target_words\")\n",
        "        self.source_sent_lens = tf.placeholder(tf.int32,[None],\"source_sent_lens\")\n",
        "        self.target_sent_lens = tf.placeholder(tf.int32,[None],\"target_sent_lens\")\n",
        "        self.is_training = tf.placeholder(tf.bool,[],\"is_training\")\n",
        "\n",
        "        self.predictions,self.loss = self.get_predictions_and_loss(self.source_words,self.target_words,self.source_sent_lens,self.target_sent_lens,self.is_training)\n",
        "\n",
        "        trainable_params = tf.trainable_variables()\n",
        "        gradients = tf.gradients(self.loss, trainable_params)\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "        self.train_op = optimizer.apply_gradients(zip(gradients, trainable_params))\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "    def get_predictions_and_loss(self, source_words,target_words, source_sent_lens,target_sent_lens,is_training):\n",
        "        self.embeddings_target = tf.get_variable(\"embeddings_target\", [self.vocab_target_size, self.embedding_size], dtype=tf.float32)\n",
        "        self.embeddings_source = tf.get_variable(\"embeddings_source\", [self.vocab_source_size, self.embedding_size], dtype=tf.float32)\n",
        "\n",
        "        batch_size = shape(target_words, 0)\n",
        "        max_target_sent_len = shape(target_words, 1)\n",
        "\n",
        "        embedding_keep_prob = 1 - (tf.to_float(is_training) * self.embedding_dropout_rate)\n",
        "        hidden_keep_prob = 1 - (tf.to_float(is_training) * self.hidden_dropout_rate)\n",
        "\n",
        "        source_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_source,source_words),embedding_keep_prob)\n",
        "        target_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_target,target_words),embedding_keep_prob)\n",
        "\n",
        "\n",
        "        encoder_outputs, encode_final_states = self.encoder(source_embs,source_sent_lens,hidden_keep_prob, embedding_keep_prob)\n",
        "\n",
        "        time_major_target_embs = tf.transpose(target_embs,[1,0,2])\n",
        "\n",
        "\n",
        "        def _decoder_scan(pre,inputs):\n",
        "            pre_logits, pre_pred, pre_states = pre\n",
        "            step_embeddings = inputs\n",
        "\n",
        "            pred_embeddings = tf.nn.embedding_lookup(self.embeddings_target,pre_pred)\n",
        "\n",
        "            step_embeddings = tf.cond(is_training,lambda :step_embeddings,lambda :pred_embeddings)\n",
        "            curr_logits, curr_states = self.step_decoder(step_embeddings,encoder_outputs,pre_states,hidden_keep_prob)\n",
        "            curr_pred = tf.argmax(curr_logits,1,output_type=tf.int32)\n",
        "\n",
        "            return curr_logits, curr_pred, curr_states\n",
        "\n",
        "        init_logits = tf.zeros([batch_size,self.vocab_target_size])\n",
        "        init_pred = tf.ones([batch_size],tf.int32) * self.SOS\n",
        "\n",
        "        time_major_logits, time_major_preds, _ = tf.scan(_decoder_scan,time_major_target_embs,initializer=(init_logits, init_pred,encode_final_states))\n",
        "        time_major_logits, time_major_preds = tf.stack(time_major_logits),tf.stack(time_major_preds)\n",
        "\n",
        "        logits = tf.transpose(time_major_logits,[1,0,2])\n",
        "        predictions = tf.transpose(time_major_preds,[1,0])\n",
        "\n",
        "        logits_mask = tf.sequence_mask(target_sent_lens-1,max_target_sent_len)\n",
        "        flatten_logits_mask = tf.reshape(logits_mask,[batch_size*max_target_sent_len])\n",
        "        flatten_logits = tf.boolean_mask(tf.reshape(logits,[batch_size*max_target_sent_len,self.vocab_target_size]),flatten_logits_mask)\n",
        "\n",
        "        gold_labels_mask = tf.concat([tf.zeros([batch_size,1],dtype=tf.bool),tf.sequence_mask(target_sent_lens-1,max_target_sent_len-1)],1)\n",
        "        flatten_gold_labels_mask = tf.reshape(gold_labels_mask,[batch_size*max_target_sent_len])\n",
        "        flatten_gold_labels = tf.boolean_mask(tf.reshape(target_words,[batch_size*max_target_sent_len]),flatten_gold_labels_mask)\n",
        "\n",
        "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=flatten_gold_labels,logits=flatten_logits))\n",
        "\n",
        "        return predictions, loss\n",
        "\n",
        "    def encoder(self,embeddings, sent_lens, hidden_keep_prob=1.0, embedding_keep_prob=1.0):\n",
        "        with tf.variable_scope(\"encoder\"):\n",
        "            \"\"\"\n",
        "            Task 1 encoder\n",
        "\n",
        "            Start\n",
        "            \"\"\"\n",
        "\n",
        "            word_embeddings = tf.nn.dropout(embeddings,embedding_keep_prob)\n",
        "            word_lstm_first = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            word_lstm_second = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            lstm_cells = tf.nn.rnn_cell.MultiRNNCell([word_lstm_first, word_lstm_second])\n",
        "            output,state = tf.nn.dynamic_rnn(lstm_cells, word_embeddings,sequence_length=sent_lens, dtype=tf.float32)\n",
        "\n",
        "            \"\"\"\n",
        "            End Task 1\n",
        "            \"\"\"\n",
        "            return output, state\n",
        "\n",
        "    def step_decoder(self,step_embeddings,encoder_outputs, pre_states, hidden_keep_prob=1.0):\n",
        "        with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "            \"\"\"\n",
        "            Task 2 decoder without attention\n",
        "\n",
        "            Start\n",
        "            Secondly, since we donâ€™t process the whole sentences, instead we only process one step of\n",
        "            the LSTM. We can directly call the LSTM cell by feeding the cell the step_embeddings and\n",
        "            the pre_states, it will return the LSTM output of this step (step_decoder_output) and new\n",
        "            states.\n",
        "            \"\"\"\n",
        "            word_lstm_first = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            word_lstm_second = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(self.hidden_size),\n",
        "                                                          state_keep_prob=hidden_keep_prob,\n",
        "                                                          variational_recurrent=True,\n",
        "                                                          dtype=tf.float32)\n",
        "            lstm_cells = tf.nn.rnn_cell.MultiRNNCell([word_lstm_first, word_lstm_second])\n",
        "            # Perform a run through on a single step of the sequence\n",
        "            step_decoder_output, curr_states = lstm_cells(step_embeddings,pre_states)\n",
        "\n",
        "            if not self.use_attention:\n",
        "\n",
        "\n",
        "                output_weights = tf.get_variable(\"output_weights\", [shape(step_decoder_output, 1), self.vocab_target_size])\n",
        "                output_bias = tf.get_variable(\"output_bias\", [self.vocab_target_size])\n",
        "                logits = tf.nn.xw_plus_b(step_decoder_output,output_weights,output_bias)\n",
        "\n",
        "                # End Task 2\n",
        "            else:\n",
        "                #Task 3 attention\n",
        "                # reshape the output to have an additional final dim\n",
        "                reshaped_output = tf.expand_dims(step_decoder_output, -1)\n",
        "\n",
        "                # Calculate the raw score by matrix mutliplication\n",
        "                raw_score = tf.matmul(encoder_outputs,reshaped_output)\n",
        "\n",
        "                # Apply softmax to score to get prob dist\n",
        "                softmax_score = tf.nn.softmax(raw_score,1)\n",
        "\n",
        "                # Create the attention weighted vector\n",
        "                encoded_vector = tf.reduce_sum(softmax_score * encoder_outputs,1)\n",
        "\n",
        "                concat_vec = tf.concat([step_decoder_output, encoded_vector],1)\n",
        "\n",
        "                output_weights = tf.get_variable(\"output_weights\", [shape(concat_vec, 1), self.vocab_target_size])\n",
        "                output_bias = tf.get_variable(\"output_bias\", [self.vocab_target_size])\n",
        "                logits = tf.nn.xw_plus_b(concat_vec,output_weights,output_bias)\n",
        "\n",
        "                #Ends Task 3\n",
        "\n",
        "        return logits, curr_states\n",
        "\n",
        "    def time_used(self, start_time):\n",
        "        curr_time = time.time()\n",
        "        used_time = curr_time-start_time\n",
        "        m = used_time // 60\n",
        "        s = used_time - 60 * m\n",
        "        return \"%d m %d s\" % (m, s)\n",
        "\n",
        "    def train(self,train_data,dev_data,test_data, epochs):\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "            epoch_time = time.time()\n",
        "            losses = []\n",
        "            source_train,target_train = train_data\n",
        "            for i, (source,target) in enumerate(zip(source_train,target_train)):\n",
        "                source_words,source_sent_lens = source\n",
        "                target_words,target_sent_lens = target\n",
        "                fd = {self.source_words:source_words,self.target_words:target_words,\n",
        "                            self.source_sent_lens:source_sent_lens,self.target_sent_lens:target_sent_lens,\n",
        "                            self.is_training:True}\n",
        "\n",
        "                _, loss= self.sess.run([self.train_op, self.loss], feed_dict=fd)\n",
        "\n",
        "                losses.append(loss)\n",
        "                if (i+1) % 100 == 0:\n",
        "                    print(\"[{}]: loss:{:.2f}\".format(i+1, sum(losses[i + 1 - 100:]) / 100.0))\n",
        "            print(\"Average epoch loss:{}\".format(sum(losses) / len(losses)))\n",
        "            print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "            dev_time = time.time()\n",
        "            print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "            self.eval(dev_data)\n",
        "            print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "        print(\"Training finished!\")\n",
        "        print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "        print(\"Evaluating on test set:\")\n",
        "        test_time = time.time()\n",
        "        self.eval(test_data)\n",
        "        print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "    def get_target_sentences(self, sents,vocab,reference=False,isnumpy=False):\n",
        "        str_sents = []\n",
        "        for sent in sents:\n",
        "            str_sent = []\n",
        "            for t in sent:\n",
        "                if isnumpy:\n",
        "                    t = t.item()\n",
        "                if t == self.SOS:\n",
        "                    continue\n",
        "                if t == self.EOS:\n",
        "                    break\n",
        "\n",
        "                str_sent.append(vocab[t])\n",
        "            if reference:\n",
        "                str_sents.append([str_sent])\n",
        "            else:\n",
        "                str_sents.append(str_sent)\n",
        "        return str_sents\n",
        "\n",
        "    def eval(self, dataset):\n",
        "        source_batches, target_batches = dataset\n",
        "        references = []\n",
        "        candidates = []\n",
        "        vocab = self.target_dict.vocab\n",
        "        PAD = self.target_dict.PAD\n",
        "\n",
        "        for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "            source_words, source_sent_lens = source\n",
        "            target_words, target_sent_lens = target\n",
        "            infer_target_words = [[PAD for i in range(self.max_target_step)] for b in target_words]\n",
        "\n",
        "            fd = {self.source_words: source_words, self.target_words: infer_target_words,\n",
        "                        self.source_sent_lens: source_sent_lens,\n",
        "                        self.is_training: False}\n",
        "            predictions = self.sess.run(self.predictions,feed_dict=fd)\n",
        "\n",
        "            references.extend(self.get_target_sentences(target_words,vocab,reference=True))\n",
        "            candidates.extend(self.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "        score = corpus_bleu(references,candidates)\n",
        "        print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "\n",
        "def shape(x, n):\n",
        "    return x.get_shape()[n].value or tf.shape(x)[n]\n",
        "\n",
        "class LanguageDict():\n",
        "    def __init__(self, sents):\n",
        "        word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "        self.vocab = [t for t,c in word_counter.items() if c > 10]\n",
        "        self.vocab.append('<pad>')\n",
        "        self.vocab.append('<unk>')\n",
        "        self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "        self.UNK = self.word2ids['<unk>']\n",
        "        self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "def load_dataset(path, max_num_examples=30000,batch_size=100,add_start_end = False):\n",
        "    lines = [line for line in open(path,'r')]\n",
        "    if max_num_examples > 0:\n",
        "        max_num_examples = min(len(lines), max_num_examples)\n",
        "        lines = lines[:max_num_examples]\n",
        "\n",
        "    sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in lines]\n",
        "    if add_start_end:\n",
        "        for sent in sents:\n",
        "            sent.append('<end>')\n",
        "            sent.insert(0,'<start>')\n",
        "\n",
        "    lang_dict = LanguageDict(sents)\n",
        "\n",
        "    sents = [[lang_dict.word2ids.get(tok,lang_dict.UNK) for tok in sent] for sent in sents]\n",
        "\n",
        "    batches = []\n",
        "    for i in range(len(sents) // batch_size):\n",
        "        batch = sents[i * batch_size:(i + 1) * batch_size]\n",
        "        batch_len = [len(sent) for sent in batch]\n",
        "        max_batch_len = max(batch_len)\n",
        "        for sent in batch:\n",
        "            if len(sent) < max_batch_len:\n",
        "                sent.extend([lang_dict.PAD for _ in range(max_batch_len - len(sent))])\n",
        "        batches.append((batch, batch_len))\n",
        "\n",
        "\n",
        "    unit = len(batches)//10\n",
        "    train_batches = batches[:8*unit]\n",
        "    dev_batches = batches[8*unit:9*unit]\n",
        "    test_batches = batches[9*unit:]\n",
        "\n",
        "    return train_batches,dev_batches,test_batches,lang_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2reAMttECOn",
        "colab_type": "code",
        "outputId": "4582abbd-9384-45cc-f893-905ae4c1c724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1530
        }
      },
      "cell_type": "code",
      "source": [
        "max_example = 30000\n",
        "batch_size = 100\n",
        "source_train, source_dev, source_test, source_dict = load_dataset(\"/content/gdrive/My Drive/dialogueSimpleText.csv\",max_num_examples=max_example,batch_size=batch_size)\n",
        "target_train, target_dev, target_test, target_dict = load_dataset(\"/content/gdrive/My Drive/dialogueSimpleLabels.csv\", max_num_examples=max_example,batch_size=batch_size, add_start_end=True)\n",
        "print(\"read %d/%d/%d train/dev/test batches\" % (len(source_train),len(source_dev), len(source_test)))\n",
        "\n",
        "train_data = (source_train,target_train)\n",
        "dev_data = (source_dev,target_dev)\n",
        "test_data = (source_test,target_test)\n",
        "\n",
        "use_attention = True\n",
        "tf.reset_default_graph()\n",
        "\n",
        "model = NmtModel(source_dict,target_dict,use_attention)\n",
        "model.build()\n",
        "model.train(train_data,dev_data,test_data,10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 240/30/30 train/dev/test batches\n",
            "source vocab: 1471, target vocab:1473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training epoch 1/10\n",
            "[100]: loss:4.86\n",
            "[200]: loss:3.81\n",
            "Average epoch loss:4.223575775822003\n",
            "Time used for epoch 1: 1 m 26 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 3.08\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 2/10\n",
            "[100]: loss:3.99\n",
            "[200]: loss:3.42\n",
            "Average epoch loss:3.654303560654322\n",
            "Time used for epoch 2: 1 m 22 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 3.33\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 3/10\n",
            "[100]: loss:3.71\n",
            "[200]: loss:3.28\n",
            "Average epoch loss:3.4556012719869615\n",
            "Time used for epoch 3: 1 m 22 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 3.26\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 4/10\n",
            "[100]: loss:3.58\n",
            "[200]: loss:3.18\n",
            "Average epoch loss:3.349430455764135\n",
            "Time used for epoch 4: 1 m 23 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 2.90\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 5/10\n",
            "[100]: loss:3.49\n",
            "[200]: loss:3.12\n",
            "Average epoch loss:3.2715172211329144\n",
            "Time used for epoch 5: 1 m 23 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 3.12\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 6/10\n",
            "[100]: loss:3.41\n",
            "[200]: loss:3.06\n",
            "Average epoch loss:3.2093472381432853\n",
            "Time used for epoch 6: 1 m 22 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 2.30\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 7/10\n",
            "[100]: loss:3.36\n",
            "[200]: loss:3.02\n",
            "Average epoch loss:3.1622112184762954\n",
            "Time used for epoch 7: 1 m 22 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 3.40\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 8/10\n",
            "[100]: loss:3.30\n",
            "[200]: loss:2.99\n",
            "Average epoch loss:3.1192890971899034\n",
            "Time used for epoch 8: 1 m 22 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 3.29\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 9/10\n",
            "[100]: loss:3.26\n",
            "[200]: loss:2.95\n",
            "Average epoch loss:3.0799408932526906\n",
            "Time used for epoch 9: 1 m 22 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 3.31\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 10/10\n",
            "[100]: loss:3.22\n",
            "[200]: loss:2.93\n",
            "Average epoch loss:3.046676634748777\n",
            "Time used for epoch 10: 1 m 23 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 3.30\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Training finished!\n",
            "Time used for training: 14 m 24 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 2.60\n",
            "Time used for evaluate on test set: 0 m 3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UqCidXf367jJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1445
        },
        "outputId": "4f78a1d3-557d-49ba-d5e7-b34117f3b5ef"
      },
      "cell_type": "code",
      "source": [
        "# Sample of the output from the attention-based model:\n",
        "\n",
        "source_batches, target_batches = test_data\n",
        "\n",
        "references = []\n",
        "candidates = []\n",
        "inputs = []\n",
        "vocab = model.target_dict.vocab\n",
        "PAD = model.target_dict.PAD\n",
        "\n",
        "for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "    if i > 1:\n",
        "      break\n",
        "    source_words, source_sent_lens = source\n",
        "    target_words, target_sent_lens = target\n",
        "    infer_target_words = [[PAD for i in range(model.max_target_step)] for b in target_words]\n",
        "\n",
        "    fd = {model.source_words: source_words, model.target_words: infer_target_words,\n",
        "                model.source_sent_lens: source_sent_lens,\n",
        "                model.is_training: False}\n",
        "    predictions = model.sess.run(model.predictions,feed_dict=fd)\n",
        "    \n",
        "    inputs.extend(model.get_target_sentences(source_words,model.source_dict.vocab))\n",
        "    references.extend(model.get_target_sentences(target_words,vocab,reference=True))\n",
        "    candidates.extend(model.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "for j,ref in enumerate(references):\n",
        "  if j > 20:\n",
        "    break\n",
        "  print(f\"\\nLine before: {' '.join(inputs[j]).replace('<pad>','')}\")\n",
        "  print(f\"Human response: {' '.join(ref[0])}\")\n",
        "  print(f\"Model's response: {' '.join(candidates[j])}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Line before: yeah,                                           \n",
            "Human response: definitely.\n",
            "Model's response: i think that they <unk>\n",
            "\n",
            "Line before: definitely.                                           \n",
            "Human response: in fact, i'm [ even going, + {f uh, }  <unk> ] again [ for, + for ] my p <unk> <unk>\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: in fact, i'm [ even going, + {f uh, }  <unk> ] again [ for, + for ] my p <unk> <unk>                     \n",
            "Human response: [ it's, + it's ] a close <unk> right now <laughter>.\n",
            "Model's response: {c and } i think that <unk> <unk> <unk> <unk>\n",
            "\n",
            "Line before: [ it's, + it's ] a close <unk> right now <laughter>.                                 \n",
            "Human response: very good.\n",
            "Model's response: {c and } i think that <unk> <unk> <unk> <unk>\n",
            "\n",
            "Line before: very good.                                          \n",
            "Human response: {f uh, }\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: {f uh, }                                         \n",
            "Human response: [ what, + {f uh, } what ] do you use as a  <unk> to <unk> which college or  university you go <unk>\n",
            "Model's response: -- {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: [ what, + {f uh, } what ] do\n",
            "Human response: {d well } <unk> [ i <unk> + {f uh, } {f uh, } {f uh, }  i have <unk> ] {f uh, } several <unk> <unk> on what, -\n",
            "Model's response: {c and } i think that's <unk>\n",
            "\n",
            "Line before: {d well } <unk> [ i <unk> + {f uh, } {f uh, } {f uh, }  i have <unk> ] {f uh, } several <unk> <unk> on what, -             \n",
            "Human response: for <unk>  [ [ i, + i, ] + {d you know, } my ] <unk> were much different [ than + --\n",
            "Model's response: i think that i <unk> {f uh, } <unk> <unk>\n",
            "\n",
            "Line before: for <unk>  [ [ i, + i, ] + {d\n",
            "Human response: uh-huh.\n",
            "Model's response: uh-huh.\n",
            "\n",
            "Line before: uh-huh.                                           \n",
            "Human response: -- than ] {d like } for graduate school.\n",
            "Model's response: -- {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: -- than ] {d like } for graduate school.                                   \n",
            "Human response: {f uh, } [ when i, + when i ] wanted to go to an <unk> <unk> i was looking for something  that was, {d you know, } rather small and easy to get around, {d you know. }\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: {f uh, } [ when i, + when i ] wanted to go to an <unk> <unk> i was looking for something  that was, {d\n",
            "Human response: uh-huh.\n",
            "Model's response: {c and } i think that <unk> <unk> <unk> <unk>\n",
            "\n",
            "Line before: uh-huh.                                           \n",
            "Human response: <throat_clearing>  and [ what i would, + what i thought ]  would be easy to get <unk>\n",
            "Model's response: -- {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: <throat_clearing>  and [ what i would, + what i thought ]  would be easy to get <unk>                         \n",
            "Human response: yeah\n",
            "Model's response: # uh-huh. #\n",
            "\n",
            "Line before: yeah                                           \n",
            "Human response: <laughter> what you thought, {f huh. }\n",
            "Model's response: <laughter>.   *[[ slash error]]\n",
            "\n",
            "Line before: <laughter> what\n",
            "Human response: yeah\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: yeah                                           \n",
            "Human response: <laughter>.  [ {c and, } + {c and, } ]  {f uh, } my choice of graduate schools [ was, + ]  {f uh, } {d you know, } it changed a whole lot.\n",
            "Model's response: <laughter>.   *[[ slash error]]\n",
            "\n",
            "Line before: <laughter>.  [ {c and, } + {c and, } ]  {f uh, } my choice of graduate schools [ was, + ]  {f uh, } {d\n",
            "Human response: [  i, + i ] got to where [ i, + i ] wanted to, {f uh, } <inhaling> graduate [ with a little, + {d you know, } with a name ]  behind me, rather than, {f uh } -- -\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: [  i, + i ] got to where [ i, + i ] wanted to, {f uh, } <inhaling> graduate [ with a little, + {d\n",
            "Human response: just the degree <unk>\n",
            "Model's response: i think that's <unk>\n",
            "\n",
            "Line before: just the degree <unk>                                        \n",
            "Human response: -- yeah,\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n",
            "\n",
            "Line before: -- yeah,                                          \n",
            "Human response: yeah.\n",
            "Model's response: {c and } i think that <unk> <unk> <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H5otnczbEs9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model does give a few different answers, they're not very good though. Most commonly: \"{c and } i think that UKNOWN\". This is probably because those words are very common, therefore the model is able to reduce loss by answering with that. The BLEU score only reached 3.24 at max for the attention-less model and 3.40 for the attention-based model . The BLEU score is not a perfect measure by any means but to have such a low score indicates the response are probabaly not going to be very good. \n",
        "\n",
        "The model is only taking the last line spoken in the utterence as an input, it therefore ignores previous utterences. This may be affecting the quality of it's answers, however from the examples i've printed out we can see that it's responses are not appropriate for the last input, let alone the wider context of the conversation. I think one major improvement would be to clean up the data so that each line represents a turn rather than an utterance, also the removal of the bracketed parts, which are not actual text but are notes made by those creating the transcripts. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1XWRpdMmCt7V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}