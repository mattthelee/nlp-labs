{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_ZORURKg-fp"
   },
   "source": [
    "# Lab 10: Dialogue Act Tagging\n",
    "\n",
    "Dialogue act (DA) tagging is an important step in the process of developing dialog systems. DA tagging is a problem usually solved by supervised machine learning approaches that all require large amounts of hand labeled data. A wide range of techniques have been investigated for DA tagging. In this lab, we explore two models for DA classification. We are using the Switchboard Dialog Act Corpus for training.\n",
    "Corpus can be downloaded from http://compprag.christopherpotts.net/swda.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziKyA9R4gyw9"
   },
   "source": [
    "The downloaded dataset should be kept in a data folder in the same directory as this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmTpKt_uefe5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6E8axaw1hAbM"
   },
   "outputs": [],
   "source": [
    " \n",
    "f = glob.glob(\"swda/sw*/sw*.csv\")\n",
    "frames = []\n",
    "for i in range(0, len(f)):\n",
    "    frames.append(pd.read_csv(f[i]))\n",
    "\n",
    "result = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7hKGF7EhM4s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of converations in the dataset: 223606\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of converations in the dataset:\",len(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ttyB2lQhc7B"
   },
   "source": [
    "The dataset has many different features, we are only using act_tag and text for this training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jUifIdshhD0"
   },
   "outputs": [],
   "source": [
    "reduced_df = result[['act_tag','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SB4oj99hiJy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fp</td>\n",
       "      <td>Hi,  /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fp</td>\n",
       "      <td>how are you today? /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fp</td>\n",
       "      <td>I'm great &lt;laughter&gt;. /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fp</td>\n",
       "      <td>Good.  /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sd</td>\n",
       "      <td>{D Well } as a matter of fact [ I'm, +   befor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  act_tag                                               text\n",
       "0      fp                                             Hi,  /\n",
       "1      fp                               how are you today? /\n",
       "2      fp                            I'm great <laughter>. /\n",
       "3      fp                                           Good.  /\n",
       "4      sd  {D Well } as a matter of fact [ I'm, +   befor..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UNy0vvhhqpD"
   },
   "source": [
    "Theere are 43 tags in this dataset. Some of the tags are Yes-No-Question('qy'), Statement-non-opinion('sd') and Statement-opinion('sv'). Tags information can be found here http://compprag.christopherpotts.net/swda.html#tags. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dR1rKmkh9QG"
   },
   "source": [
    "You can check the frequency of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2DzS0iUh-bU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sd           70464\n",
       "b            36180\n",
       "sv           25696\n",
       "+            17813\n",
       "%            15547\n",
       "aa           10136\n",
       "ba            4523\n",
       "qy            3785\n",
       "x             3628\n",
       "ny            2826\n",
       "fc            2404\n",
       "b^r           2102\n",
       "sd^e          1939\n",
       "qw            1890\n",
       "sd(^q)        1341\n",
       "bk            1254\n",
       "nn            1230\n",
       "h             1218\n",
       "qy^d          1217\n",
       "bh            1044\n",
       "^q             972\n",
       "bf             934\n",
       "sd^t           929\n",
       "aa^r           916\n",
       "+@             867\n",
       "o              801\n",
       "na             764\n",
       "^2             714\n",
       "b^m            688\n",
       "ad             666\n",
       "             ...  \n",
       "^2^r             1\n",
       "na^m^t           1\n",
       "ba,fe            1\n",
       "qh^g             1\n",
       "qw^d@            1\n",
       "br,o@            1\n",
       "ft^m             1\n",
       "cc^t             1\n",
       "h@               1\n",
       "qr^d*            1\n",
       "sd,sv            1\n",
       "sd;qy^d          1\n",
       "bh,sd,o@         1\n",
       "bk,sd,o@         1\n",
       "sd^t*            1\n",
       "b^m,sd,o@        1\n",
       "qy^d(^q)         1\n",
       "ny^c^r           1\n",
       "bf^2             1\n",
       "sv^e^r           1\n",
       "%,o@             1\n",
       "ad,o@            1\n",
       "nn^r@            1\n",
       "bf,nn,o@         1\n",
       "h,sd             1\n",
       "sd;sv            1\n",
       "qw(^q)           1\n",
       "ar^m             1\n",
       "bk^t             1\n",
       "sd,qy^g          1\n",
       "Name: act_tag, Length: 303, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df['act_tag'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6G4zBbtB5Ipz"
   },
   "source": [
    "## Baseline BiLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9biiyP8UiGDe"
   },
   "source": [
    "To get unique tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxn2s_4jiKkU"
   },
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "for tag in reduced_df['act_tag']:\n",
    "    unique_tags.add(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMOX5KwgiPmu"
   },
   "outputs": [],
   "source": [
    "one_hot_encoding_dic = pd.get_dummies(list(unique_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPHPCxE3iPby"
   },
   "outputs": [],
   "source": [
    "tags_encoding = []\n",
    "for i in range(0, len(reduced_df)):\n",
    "    tags_encoding.append(one_hot_encoding_dic[reduced_df['act_tag'].iloc[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVI8QyVzjqWh"
   },
   "source": [
    "The tags are one hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQJTiffPjUtu"
   },
   "source": [
    "To create sentence embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmkyD1TfjWGO"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in range(0, len(reduced_df)):\n",
    "    sentences.append(reduced_df['text'].iloc[i].split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlD6L6e3jV-7"
   },
   "outputs": [],
   "source": [
    "wordvectors = {}\n",
    "index = 1\n",
    "for s in sentences:\n",
    "    for w in s:\n",
    "        if w not in wordvectors:\n",
    "            wordvectors[w] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7_cjDHrjV1c"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = len(max(sentences, key=len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LX6DidEvjVWs"
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = []\n",
    "for s in sentences:\n",
    "    sentence_emb = []\n",
    "    for w in s:\n",
    "        sentence_emb.append(wordvectors[w])\n",
    "    sentence_embeddings.append(sentence_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr4iEyNTjmlu"
   },
   "source": [
    "Then we split the dataset into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiNZ-iI_jnOF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, np.array(tags_encoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RqMeWe_jron"
   },
   "source": [
    "And pad the sentences with zero to make all sentences of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ai9cwv82jufe"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FItlHC1Fjz6y"
   },
   "source": [
    " The model architecture is as follows: Embedding Layer (to generate word embeddings) Next layer Bidirectional LSTM. Feed forward layer with number of neurons = number of tags. Softmax activation to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCaX-ptaj8G2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 137, 100)          4373100   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 303)               39087     \n",
      "=================================================================\n",
      "Total params: 4,496,667\n",
      "Trainable params: 4,496,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
    "from keras.optimizers import Adam\n",
    "#Building the network\n",
    "numberOfTags = len(reduced_df['act_tag'].value_counts())\n",
    "embed_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(wordvectors), output_dim=embed_dim, input_length=MAX_LENGTH))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(numberOfTags,activation='softmax' ))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()\n",
    "weightsFilePath =\"weights.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167704 samples, validate on 55902 samples\n",
      "Epoch 1/10\n",
      "167424/167704 [============================>.] - ETA: 0s - loss: 1.8970 - acc: 0.5208"
     ]
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "checkpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(train_sentences_X,y_train,epochs=10,batch_size=512,validation_data=(test_sentences_X, y_test), callbacks=[checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LkONUKQkSrL"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ab0ZL1dqkTY4"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", score[1]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHwoVCEwjEz7"
   },
   "source": [
    "In addition to overall accuracy, you need to look at the accuracy of some minority classes. Signal-non-understanding ('br') is a good indicator of \"other-repair\" or cases in which the other conversational participant attempts to repair the speaker's error. Summarize/reformulate ('bf') has been used in dialogue summarization. Report the accuracy for these classes and some frequent errors you notice the system makes in predicting them. What do you think the reasons are？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKHbOs4WkFaP"
   },
   "source": [
    "\n",
    "As the dataset is highly imbalanced, we can simply weight up the minority classes proportionally to their underrepresentation while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6L4kNdf6kGEa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_integers = np.argmax(tags_encoding, axis=1)\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB2McUREkL4B"
   },
   "outputs": [],
   "source": [
    "model.fit(train_sentences_X, y_train, batch_size=100, epochs=5, class_weight = d_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fM7VWweco0Et"
   },
   "source": [
    "Report the overall accuracy and the accuracy of  'br' and 'bf'  classes. Suggest other ways to handle imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW4g5mQkkaFv"
   },
   "source": [
    "Can we improve things by using context information?  Next we try to build a model which predicts DA tag from the sequence of \n",
    "previous DA tags, plus the utterance representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfrGWuZ6nk4y"
   },
   "source": [
    "##Using Context for Dialog Act Classification\n",
    "We expect there is valuable sequential information among the DA tags. So in this section we apply a BiLSTM on top of the sentence CNN representation. The CNN model learns textual information in each sentence for DA classification. Here, we use bidirectional-LSTM (BLSTM) to learn the context before and after the current sentence. The left-to-right LSTM output and the one from the reverse direction are concatenated and input to a hidden layer for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sz693CIUvcca"
   },
   "source": [
    "Functions for creating weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hMj-KaKvfHb"
   },
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oitwAO5ivkbk"
   },
   "outputs": [],
   "source": [
    "def bias_init(shape):\n",
    "    return tf.Variable(tf.zeros(shape=shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuJLqgjWqcIf"
   },
   "source": [
    " This is classical CNN layer used to convolve over embedings tensor and gether useful information from it. The data is represented by hierarchy of features, which can be modelled using a CNN.\n",
    "    \n",
    "      Input(s): \n",
    "              input - word_embedings\n",
    "              filter_size - size of width and height of the Conv kernel\n",
    "              number_of_channels - in this case it is always 1\n",
    "              number_of_filters - how many representation of the input utterance are we going to output from this layer \n",
    "              strides - how many does kernel move to the side and up/down\n",
    "              activation - a activation function\n",
    "              max_pool - boolean value which will trigger a max_pool operation on the output tensor\n",
    "      Output(s): \n",
    "               text_conv layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuaiZkOjZGx1"
   },
   "outputs": [],
   "source": [
    "def text_conv(input, filter_size, number_of_channels, number_of_filters, strides=(1, 1), activation=tf.nn.relu, max_pool=True):\n",
    "    \n",
    "    weights = weights_init([filter_size, filter_size, number_of_channels, number_of_filters])\n",
    "    bias = bias_init([number_of_filters])\n",
    "    \n",
    "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if max_pool:\n",
    "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU0Xa3QOqwS3"
   },
   "source": [
    "    This method is used to create LSTM layer. And the data we’re working with has temporal properties which we want to model as well — hence the use of a LSTM. You can create a BiLSTM by modifying this.\n",
    "    \n",
    "    Input(s): lstm_cell_unitis - used to define the number of units in a LSTM layer\n",
    "              number_of_layers - used to define how many of LSTM layers do we want in the network\n",
    "              batch_size - in this method this information is used to build starting state for the network\n",
    "              dropout_rate - used to define how many cells in a layer do we want to 'turn off'\n",
    "              \n",
    "    Output(s): cell - lstm layer\n",
    "               init_state - zero vectors used as a starting state for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaxUrmPIZI7S"
   },
   "outputs": [],
   "source": [
    "def lstm_layer(lstm_size, number_of_layers, batch_size, dropout_rate):\n",
    "\n",
    "    def cell(size, dropout_rate=None):\n",
    "        layer = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        return tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "            \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell(lstm_size, dropout_rate) for _ in range(number_of_layers)])\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxlRv9dDrcy5"
   },
   "source": [
    "    Use to transform/reshape conv output to 2d matrix, if it's necessary\n",
    "    \n",
    "    Input(s): Layer - text_cnn layer\n",
    "              batch_size - how many samples do we feed at once\n",
    "              seq_len - number of time steps\n",
    "              \n",
    "    Output(s): reshaped_layer - the layer with new shape\n",
    "               number_of_elements - this param is used as a in_size for next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYxHsBvwZRA4"
   },
   "outputs": [],
   "source": [
    "def flatten(layer, batch_size, seq_len):\n",
    "\n",
    "    dims = layer.get_shape()\n",
    "    number_of_elements = dims[2:].num_elements()\n",
    "    \n",
    "    reshaped_layer = tf.reshape(layer, [batch_size, int(seq_len/2), number_of_elements])\n",
    "    return reshaped_layer, number_of_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMOjnT8Drmpa"
   },
   "source": [
    "    Output layer for the lstm netowrk\n",
    "    \n",
    "    Input(s): lstm_outputs - outputs from the RNN part of the network\n",
    "              input_size - in this case it is RNN size (number of neuros in RNN layer)\n",
    "              output_size - number of neuros for the output layer == number of classes\n",
    "              \n",
    "    Output(s) - logits, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d84nFOulZkhP"
   },
   "outputs": [],
   "source": [
    "def dense_layer(input, in_size, out_size, dropout=False, activation=tf.nn.relu):\n",
    "  \n",
    "    weights = weights_init([in_size, out_size])\n",
    "    bias = bias_init([out_size])\n",
    "    \n",
    "    layer = tf.matmul(input, weights) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if dropout:\n",
    "        layer = tf.nn.dropout(layer, 0.5)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VX3KofjJruhZ"
   },
   "source": [
    "    Function used to calculate loss and minimize it\n",
    "    \n",
    "    Input(s): rnn_out - logits from the fully_connected layer\n",
    "              targets - targets used to train network\n",
    "              learning_rate/step_size\n",
    "    \n",
    "    \n",
    "    Output(s): optimizer - optimizer of choice\n",
    "               loss - calculated loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNu0_J2VZlKB"
   },
   "outputs": [],
   "source": [
    "def loss_optimizer(logits, targets, learning_rate, ):\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return loss, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3Acv6U_r1rG"
   },
   "source": [
    "To create the model you can use these inputs:     \n",
    "       \n",
    "       Input(s): learning_rate/step_size - how fast are we going to find global minima\n",
    "                  batch_size -  the nuber of samples to feed at once\n",
    "                  seq_len - the number of timesteps in unrolled RNN\n",
    "                  vocab_size - the number of nunique words in the vocab\n",
    "                  embed_size - length of word embed vectors\n",
    "                  conv_filters - number of filters in output tensor from CNN layer\n",
    "                  conv_filter_size - height and width of conv kernel\n",
    "                  number_of_lstm_layers - the number of layers used in the LSTM part of the network\n",
    "                  lstm_units - the number of neurons/cells in a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qT7qNHxCZrrr"
   },
   "outputs": [],
   "source": [
    "class DATagging(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, batch_size=100, seq_len=250, vocab_size=10000, embed_size=300,\n",
    "                conv_filters=32, conv_filter_size=5, number_of_lstm_layers=1, lstm_units=128):\n",
    "        \n",
    "\n",
    "        \n",
    "        #Embedding layer\n",
    "\n",
    "        \n",
    "        #Building the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3bVXv36axfu"
   },
   "outputs": [],
   "source": [
    "model = DATagging(learning_rate=0.001, \n",
    "                     batch_size=50, \n",
    "                     seq_len=250, \n",
    "                     vocab_size=len(vocab_to_int) + 1, \n",
    "                     embed_size=300,\n",
    "                     conv_filters=32, \n",
    "                     conv_filter_size=5, \n",
    "                     number_of_lstm_layers=1, \n",
    "                     lstm_units=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKMmrfuisKGJ"
   },
   "source": [
    "Compared to the baseline using BiLSTM for utterance classification, the second method effectively leverage context information and achieve better performance. Report your overall accuracy. Did context help disambiguate and better predict the minority classes ('br' and 'bf')? What are frequent errors? Show one positive example where adding context changed the prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUZt48JgrE34"
   },
   "source": [
    "## Advanced: Creating End-To-End Dialogue System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE63Q5guuPdA"
   },
   "source": [
    "In the last section we want to create end-to-end dialogue system, following on from the seq2seq MT labs you've \n",
    "just done. This is an advanced part of the assignment and worth 10 marks (20%) in total. In end-to-end dialogue systems, the encoder represents each utterance with a vector. The utterance vector is the hidden state after the last token of the utterance has been processed. The context LSTM keeps track of past utterances. The hidden state can be explained as the state of the dialogue system. The next utterance prediction is performed by a decoder LSTM, which takes the hidden state of the last LSTM and produces a probability distribution over the tokens in the next utterance. You can take the DA LSTM state of last section as input to a decoder which tries to generate the next utterance. You can add attention and monitor the performance. Instead of evaluating by an automatic evaluation method, you can show us some of the interesting predictions. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "eenlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
