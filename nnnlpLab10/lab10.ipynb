{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_ZORURKg-fp"
   },
   "source": [
    "# Lab 10: Dialogue Act Tagging\n",
    "\n",
    "Dialogue act (DA) tagging is an important step in the process of developing dialog systems. DA tagging is a problem usually solved by supervised machine learning approaches that all require large amounts of hand labeled data. A wide range of techniques have been investigated for DA tagging. In this lab, we explore two models for DA classification. We are using the Switchboard Dialog Act Corpus for training.\n",
    "Corpus can be downloaded from http://compprag.christopherpotts.net/swda.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziKyA9R4gyw9"
   },
   "source": [
    "The downloaded dataset should be kept in a data folder in the same directory as this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmTpKt_uefe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# tf.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6E8axaw1hAbM"
   },
   "outputs": [],
   "source": [
    " \n",
    "f = glob.glob(\"swda/sw*/sw*.csv\")\n",
    "frames = []\n",
    "for i in range(0, len(f)):\n",
    "    frames.append(pd.read_csv(f[i]))\n",
    "\n",
    "result = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7hKGF7EhM4s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of converations in the dataset: 223606\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of converations in the dataset:\",len(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ttyB2lQhc7B"
   },
   "source": [
    "The dataset has many different features, we are only using act_tag and text for this training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jUifIdshhD0"
   },
   "outputs": [],
   "source": [
    "reduced_df = result[['act_tag','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SB4oj99hiJy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act_tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fp</td>\n",
       "      <td>Hi,  /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fp</td>\n",
       "      <td>how are you today? /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fp</td>\n",
       "      <td>I'm great &lt;laughter&gt;. /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fp</td>\n",
       "      <td>Good.  /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sd</td>\n",
       "      <td>{D Well } as a matter of fact [ I'm, +   befor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  act_tag                                               text\n",
       "0      fp                                             Hi,  /\n",
       "1      fp                               how are you today? /\n",
       "2      fp                            I'm great <laughter>. /\n",
       "3      fp                                           Good.  /\n",
       "4      sd  {D Well } as a matter of fact [ I'm, +   befor..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UNy0vvhhqpD"
   },
   "source": [
    "Theere are 43 tags in this dataset. Some of the tags are Yes-No-Question('qy'), Statement-non-opinion('sd') and Statement-opinion('sv'). Tags information can be found here http://compprag.christopherpotts.net/swda.html#tags. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dR1rKmkh9QG"
   },
   "source": [
    "You can check the frequency of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2DzS0iUh-bU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leem/anaconda3/envs/eenlp/lib/python3.6/site-packages/pandas/core/frame.py:3798: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n"
     ]
    }
   ],
   "source": [
    "act_tag_counts = reduced_df['act_tag'].value_counts()\n",
    "# Reduce the number of classes by replacing rare labels with the 'other' token\n",
    "# By reducing the number i help to reduce the class imbalance \n",
    "# and also scale the confusion matrix down to a size that allows for interpretation\n",
    "threshold = 20\n",
    "for label in act_tag_counts.keys():\n",
    "    if act_tag_counts[label] < threshold:\n",
    "        reduced_df.replace(label,'other', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6G4zBbtB5Ipz"
   },
   "source": [
    "## Baseline BiLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9biiyP8UiGDe"
   },
   "source": [
    "To get unique tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxn2s_4jiKkU"
   },
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "for tag in reduced_df['act_tag']:\n",
    "    unique_tags.add(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMOX5KwgiPmu"
   },
   "outputs": [],
   "source": [
    "# Load the onehot encoding dataframe from disk to avoid re-runs incorrectly mapping vectors\n",
    "import os.path\n",
    "fname = \"one_hot_encoding_dic.pickle\"\n",
    "if os.path.isfile(fname):\n",
    "    pickle_in = open(fname,\"rb\")\n",
    "    one_hot_encoding_dic = pickle.load(pickle_in)\n",
    "else:\n",
    "    one_hot_encoding_dic = pd.get_dummies(list(unique_tags))\n",
    "\n",
    "\n",
    "pickle_out = open(fname,\"wb\")\n",
    "pickle.dump(one_hot_encoding_dic, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPHPCxE3iPby"
   },
   "outputs": [],
   "source": [
    "tags_encoding = []\n",
    "for i in range(0, len(reduced_df)):\n",
    "    tags_encoding.append(one_hot_encoding_dic[reduced_df['act_tag'].iloc[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVI8QyVzjqWh"
   },
   "source": [
    "The tags are one hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQJTiffPjUtu"
   },
   "source": [
    "To create sentence embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmkyD1TfjWGO"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in range(0, len(reduced_df)):\n",
    "    sentences.append(reduced_df['text'].iloc[i].split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlD6L6e3jV-7"
   },
   "outputs": [],
   "source": [
    "wordvectors = {}\n",
    "index = 1\n",
    "for s in sentences:\n",
    "    for w in s:\n",
    "        if w not in wordvectors:\n",
    "            wordvectors[w] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7_cjDHrjV1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "#MAX_LENGTH = len(max(sentences, key=len))\n",
    "MAX_LENGTH = 250\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LX6DidEvjVWs"
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = []\n",
    "for s in sentences:\n",
    "    sentence_emb = []\n",
    "    for w in s:\n",
    "        sentence_emb.append(wordvectors[w])\n",
    "    sentence_embeddings.append(sentence_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr4iEyNTjmlu"
   },
   "source": [
    "Then we split the dataset into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiNZ-iI_jnOF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, np.array(tags_encoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RqMeWe_jron"
   },
   "source": [
    "And pad the sentences with zero to make all sentences of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ai9cwv82jufe"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FItlHC1Fjz6y"
   },
   "source": [
    " The model architecture is as follows: Embedding Layer (to generate word embeddings) Next layer Bidirectional LSTM. Feed forward layer with number of neurons = number of tags. Softmax activation to get probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCaX-ptaj8G2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 50)           2186600   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               58880     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 94)                12126     \n",
      "=================================================================\n",
      "Total params: 2,257,606\n",
      "Trainable params: 2,257,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
    "from keras.optimizers import Adam\n",
    "#Building the network\n",
    "numberOfTags = len(reduced_df['act_tag'].value_counts())\n",
    "embed_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(wordvectors)+1, output_dim=embed_dim, input_length=MAX_LENGTH))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(numberOfTags,activation='softmax' ))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()\n",
    "weightsFilePath =\"weights.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Commented out to allow working on other things without running this cell\\ncheckpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\\nhistory = model.fit(train_sentences_X,y_train,epochs=5,batch_size=512,validation_data=(test_sentences_X, y_test), callbacks=[checkpoint],verbose=1)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best weights to a file so we get the model with the best val acc\n",
    "\n",
    "'''\n",
    "# Commented out to allow working on other things without running this cell\n",
    "checkpoint = ModelCheckpoint(weightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(train_sentences_X,y_train,epochs=5,batch_size=512,validation_data=(test_sentences_X, y_test), callbacks=[checkpoint],verbose=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LkONUKQkSrL"
   },
   "outputs": [],
   "source": [
    "#model.load_weights(weightsFilePath)\n",
    "#score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ab0ZL1dqkTY4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.0\n",
      "70.0\n",
      "17485.0\n"
     ]
    }
   ],
   "source": [
    "bfTotal = 0.0\n",
    "brTotal = 0.0\n",
    "sdTotal = 0.0\n",
    "for i,row in enumerate(y_test):\n",
    "    if np.array_equal(row,one_hot_encoding_dic['br']):\n",
    "        brTotal += 1.0\n",
    "    elif np.array_equal(row,one_hot_encoding_dic['bf']):\n",
    "        bfTotal += 1.0\n",
    "    elif np.array_equal(row,one_hot_encoding_dic['sd']):\n",
    "        sdTotal += 1.0\n",
    "        \n",
    "print(bfTotal)\n",
    "print(brTotal)\n",
    "print(sdTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "28\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "for i,key in enumerate(one_hot_encoding_dic.keys()):\n",
    "    if key == 'br':\n",
    "        brConfusionMatrixIndex = i\n",
    "    elif key == 'bf':\n",
    "        bfConfusionMatrixIndex = i\n",
    "    elif key == 'sd':\n",
    "        sdConfusionMatrixIndex = i\n",
    "print(brConfusionMatrixIndex)\n",
    "print(bfConfusionMatrixIndex)\n",
    "print(sdConfusionMatrixIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out so I can run notebook without taking time to do this \n",
    "# y_pred = model.predict(test_sentences_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Create dict to convert y vectors back to strings\\nvec_to_string = {}\\nfor key in one_hot_encoding_dic.keys():\\n    vec_to_string[one_hot_encoding_dic[key].argmax()] = key\\n\\ny_pred_strings = []\\nfor vec in y_pred:\\n    y_pred_strings.append(vec_to_string[vec.argmax()])\\n\\ny_test_strings = []\\nfor vec in y_test:\\n    y_test_strings.append(vec_to_string[vec.argmax()])\\n    \\ncm = confusion_matrix(y_test_strings,y_pred_strings, labels=one_hot_encoding_dic.keys())\\nbrAccuracy = float(cm[brConfusionMatrixIndex][brConfusionMatrixIndex] )/ float(brTotal)\\nbfAccuracy = float(cm[bfConfusionMatrixIndex][bfConfusionMatrixIndex]) / float(bfTotal)\\nsdAccuracy = float(cm[sdConfusionMatrixIndex][sdConfusionMatrixIndex]) / float(sdTotal)\\nprint(f\"br Accuracy: {brAccuracy}, bf accuracy: {bfAccuracy},sd Accuracy: {sdAccuracy} \")\\nprint(f\"Overall accuracy: {np.diag(cm).sum()*100.0 / len(y_test)}\")\\n\\nbrCommonMistakes = cm[brConfusionMatrixIndex]\\nprint(f\"The br was most commonly confused with: {one_hot_encoding_dic.keys()[brCommonMistakes.argmax()]}\")\\n\\nbfCommonMistakes = cm[bfConfusionMatrixIndex]\\nprint(f\"The bf was most commonly confused with: {one_hot_encoding_dic.keys()[bfCommonMistakes.argmax()]}\")\\n\\n# The br and bf sentences are incorrectly labelled as aa and sd. \\n# I think this is because aa and sd are the most common classes in the dataset\\n# The 0% accuracy for bf and br was concering so i decided to plot thw whole confusion matrix to see what was happening\\n# The confusion matrix is mostly dark with a small number of bright dots. These are the classes it tries to classify\\n# This indicates that the large class imbalance has caused the model to only output a small number of classes rather than the full range\\n# It simply accepts it will get the other classes wrong as it is still able to achieve good (>60%) accuracy without using them\\nplt.imshow(cm)\\nplt.colorbar()\\n\\nplt.show()\\n\\nplt.savefig(\"confusionmatrix.png\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commented out for running efficieny\n",
    "'''\n",
    "\n",
    "# Create dict to convert y vectors back to strings\n",
    "vec_to_string = {}\n",
    "for key in one_hot_encoding_dic.keys():\n",
    "    vec_to_string[one_hot_encoding_dic[key].argmax()] = key\n",
    "\n",
    "y_pred_strings = []\n",
    "for vec in y_pred:\n",
    "    y_pred_strings.append(vec_to_string[vec.argmax()])\n",
    "\n",
    "y_test_strings = []\n",
    "for vec in y_test:\n",
    "    y_test_strings.append(vec_to_string[vec.argmax()])\n",
    "    \n",
    "cm = confusion_matrix(y_test_strings,y_pred_strings, labels=one_hot_encoding_dic.keys())\n",
    "brAccuracy = float(cm[brConfusionMatrixIndex][brConfusionMatrixIndex] )/ float(brTotal)\n",
    "bfAccuracy = float(cm[bfConfusionMatrixIndex][bfConfusionMatrixIndex]) / float(bfTotal)\n",
    "sdAccuracy = float(cm[sdConfusionMatrixIndex][sdConfusionMatrixIndex]) / float(sdTotal)\n",
    "print(f\"br Accuracy: {brAccuracy}, bf accuracy: {bfAccuracy},sd Accuracy: {sdAccuracy} \")\n",
    "print(f\"Overall accuracy: {np.diag(cm).sum()*100.0 / len(y_test)}\")\n",
    "\n",
    "brCommonMistakes = cm[brConfusionMatrixIndex]\n",
    "print(f\"The br was most commonly confused with: {one_hot_encoding_dic.keys()[brCommonMistakes.argmax()]}\")\n",
    "\n",
    "bfCommonMistakes = cm[bfConfusionMatrixIndex]\n",
    "print(f\"The bf was most commonly confused with: {one_hot_encoding_dic.keys()[bfCommonMistakes.argmax()]}\")\n",
    "\n",
    "# The br and bf sentences are incorrectly labelled as aa and sd. \n",
    "# I think this is because aa and sd are the most common classes in the dataset\n",
    "# The 0% accuracy for bf and br was concering so i decided to plot thw whole confusion matrix to see what was happening\n",
    "# The confusion matrix is mostly dark with a small number of bright dots. These are the classes it tries to classify\n",
    "# This indicates that the large class imbalance has caused the model to only output a small number of classes rather than the full range\n",
    "# It simply accepts it will get the other classes wrong as it is still able to achieve good (>60%) accuracy without using them\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"confusionmatrix.png\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHwoVCEwjEz7"
   },
   "source": [
    "In addition to overall accuracy, you need to look at the accuracy of some minority classes. Signal-non-understanding ('br') is a good indicator of \"other-repair\" or cases in which the other conversational participant attempts to repair the speaker's error. Summarize/reformulate ('bf') has been used in dialogue summarization. Report the accuracy for these classes and some frequent errors you notice the system makes in predicting them. What do you think the reasons are？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKHbOs4WkFaP"
   },
   "source": [
    "\n",
    "As the dataset is highly imbalanced, we can simply weight up the minority classes proportionally to their underrepresentation while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6L4kNdf6kGEa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_integers = np.argmax(tags_encoding, axis=1)\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "classBalancedWeightsFilePath = \"classBalanced\" + weightsFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xB2McUREkL4B"
   },
   "outputs": [],
   "source": [
    "\n",
    "classBalancedCheckpoint = ModelCheckpoint(classBalancedWeightsFilePath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# Commented out to allow running of whole notebook efficiently\n",
    "#model.fit(train_sentences_X, y_train, batch_size=512, epochs=5, class_weight = d_class_weights, validation_data=(test_sentences_X, y_test), callbacks=[classBalancedCheckpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fM7VWweco0Et"
   },
   "source": [
    "Report the overall accuracy and the accuracy of  'br' and 'bf'  classes. Suggest other ways to handle imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW4g5mQkkaFv"
   },
   "source": [
    "Can we improve things by using context information?  Next we try to build a model which predicts DA tag from the sequence of \n",
    "previous DA tags, plus the utterance representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfrGWuZ6nk4y"
   },
   "source": [
    "##Using Context for Dialog Act Classification\n",
    "We expect there is valuable sequential information among the DA tags. So in this section we apply a BiLSTM on top of the sentence CNN representation. The CNN model learns textual information in each sentence for DA classification. Here, we use bidirectional-LSTM (BLSTM) to learn the context before and after the current sentence. The left-to-right LSTM output and the one from the reverse direction are concatenated and input to a hidden layer for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sz693CIUvcca"
   },
   "source": [
    "Functions for creating weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hMj-KaKvfHb"
   },
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oitwAO5ivkbk"
   },
   "outputs": [],
   "source": [
    "def bias_init(shape):\n",
    "    return tf.Variable(tf.zeros(shape=shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuJLqgjWqcIf"
   },
   "source": [
    " This is classical CNN layer used to convolve over embedings tensor and gether useful information from it. The data is represented by hierarchy of features, which can be modelled using a CNN.\n",
    "    \n",
    "      Input(s): \n",
    "              input - word_embedings\n",
    "              filter_size - size of width and height of the Conv kernel\n",
    "              number_of_channels - in this case it is always 1\n",
    "              number_of_filters - how many representation of the input utterance are we going to output from this layer \n",
    "              strides - how many does kernel move to the side and up/down\n",
    "              activation - a activation function\n",
    "              max_pool - boolean value which will trigger a max_pool operation on the output tensor\n",
    "      Output(s): \n",
    "               text_conv layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuaiZkOjZGx1"
   },
   "outputs": [],
   "source": [
    "def text_conv(input, filter_size, number_of_channels, number_of_filters, strides=(1, 1), activation=tf.nn.relu, max_pool=True):\n",
    "    #print('Conv')\n",
    "    #print(input)\n",
    "    #print(filter_size)\n",
    "    #print('conv end')\n",
    "    weights = weights_init([filter_size, filter_size, number_of_channels, number_of_filters])\n",
    "    print(tf.shape(input))\n",
    "    bias = bias_init([number_of_filters])\n",
    "    print('bias done')\n",
    "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n",
    "    print('done')\n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    print('this works')\n",
    "    if max_pool:\n",
    "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU0Xa3QOqwS3"
   },
   "source": [
    "    This method is used to create LSTM layer. And the data we’re working with has temporal properties which we want to model as well — hence the use of a LSTM. You can create a BiLSTM by modifying this.\n",
    "    \n",
    "    Input(s): lstm_cell_unitis - used to define the number of units in a LSTM layer\n",
    "              number_of_layers - used to define how many of LSTM layers do we want in the network\n",
    "              batch_size - in this method this information is used to build starting state for the network\n",
    "              dropout_rate - used to define how many cells in a layer do we want to 'turn off'\n",
    "              \n",
    "    Output(s): cell - lstm layer\n",
    "               init_state - zero vectors used as a starting state for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaxUrmPIZI7S"
   },
   "outputs": [],
   "source": [
    "def lstm_layer(lstm_size, number_of_layers, batch_size, dropout_rate):\n",
    "\n",
    "    def cell(size, dropout_rate=None):\n",
    "        layer = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        return tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "            \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell(lstm_size, dropout_rate) for _ in range(number_of_layers)])\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxlRv9dDrcy5"
   },
   "source": [
    "    Use to transform/reshape conv output to 2d matrix, if it's necessary\n",
    "    \n",
    "    Input(s): Layer - text_cnn layer\n",
    "              batch_size - how many samples do we feed at once\n",
    "              seq_len - number of time steps\n",
    "              \n",
    "    Output(s): reshaped_layer - the layer with new shape\n",
    "               number_of_elements - this param is used as a in_size for next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYxHsBvwZRA4"
   },
   "outputs": [],
   "source": [
    "def flatten(layer, batch_size, seq_len):\n",
    "\n",
    "    dims = layer.get_shape()\n",
    "    number_of_elements = dims[2:].num_elements()\n",
    "    \n",
    "    reshaped_layer = tf.reshape(layer, [batch_size, int(seq_len/2), number_of_elements])\n",
    "    return reshaped_layer, number_of_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMOjnT8Drmpa"
   },
   "source": [
    "    Output layer for the lstm netowrk\n",
    "    \n",
    "    Input(s): lstm_outputs - outputs from the RNN part of the network\n",
    "              input_size - in this case it is RNN size (number of neuros in RNN layer)\n",
    "              output_size - number of neuros for the output layer == number of classes\n",
    "              \n",
    "    Output(s) - logits, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d84nFOulZkhP"
   },
   "outputs": [],
   "source": [
    "def dense_layer(input, in_size, out_size, dropout=False, activation=tf.nn.relu):\n",
    "  \n",
    "    weights = weights_init([in_size, out_size])\n",
    "    bias = bias_init([out_size])\n",
    "    \n",
    "    layer = tf.matmul(input, weights) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if dropout:\n",
    "        layer = tf.nn.dropout(layer, 0.5)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VX3KofjJruhZ"
   },
   "source": [
    "    Function used to calculate loss and minimize it\n",
    "    \n",
    "    Input(s): rnn_out - logits from the fully_connected layer\n",
    "              targets - targets used to train network\n",
    "              learning_rate/step_size\n",
    "    \n",
    "    \n",
    "    Output(s): optimizer - optimizer of choice\n",
    "               loss - calculated loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNu0_J2VZlKB"
   },
   "outputs": [],
   "source": [
    "def loss_optimizer(logits, targets, learning_rate ):\n",
    "    #loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "    loss =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "    print(loss)\n",
    "    print(targets)\n",
    "    print(logits)\n",
    "\n",
    "    print(learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return loss, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3Acv6U_r1rG"
   },
   "source": [
    "To create the model you can use these inputs:     \n",
    "       \n",
    "       Input(s): learning_rate/step_size - how fast are we going to find global minima\n",
    "                  batch_size -  the nuber of samples to feed at once\n",
    "                  seq_len - the number of timesteps in unrolled RNN\n",
    "                  vocab_size - the number of nunique words in the vocab\n",
    "                  embed_size - length of word embed vectors\n",
    "                  conv_filters - number of filters in output tensor from CNN layer\n",
    "                  conv_filter_size - height and width of conv kernel\n",
    "                  number_of_lstm_layers - the number of layers used in the LSTM part of the network\n",
    "                  lstm_units - the number of neurons/cells in a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qT7qNHxCZrrr"
   },
   "outputs": [],
   "source": [
    "class DATagging(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, batch_size=100, seq_len=250, vocab_size=10000, embed_size=300,\n",
    "                conv_filters=32, conv_filter_size=5, number_of_lstm_layers=1, lstm_units=128):\n",
    "        \n",
    "        # Clear tf graph for multiple cell running\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Create placeholders\n",
    "        inputSentences = tf.placeholder(tf.int32, [batch_size, seq_len], name='inputSentences')\n",
    "        targetLabels= tf.placeholder(tf.float32, [batch_size, numberOfTags], name='targetLabels')    \n",
    "\n",
    "        \n",
    "        \n",
    "        #Embedding layer\n",
    "        # Initialise word embeddings with random uniform dist\n",
    "        wordEmbeddings = tf.Variable(tf.random_uniform([vocab_size, embed_size]))\n",
    "        # Embedding layer\n",
    "        embeddingLayer = tf.nn.embedding_lookup(wordEmbeddings, inputSentences)\n",
    "        \n",
    "        # Reshape embedding layer to have a \"channel\"\n",
    "        embeddingLayer = tf.expand_dims(embeddingLayer, -1)\n",
    "\n",
    "        # Conv layer\n",
    "        number_of_channels = 1\n",
    "        sentence_embedding = text_conv(embeddingLayer, conv_filter_size,number_of_channels, conv_filters)\n",
    "        flat_sentence_embedding, size = flatten(sentence_embedding, batch_size, seq_len)\n",
    "        \n",
    "        \n",
    "        lstm_cell, init_state = lstm_layer(lstm_units, number_of_lstm_layers, batch_size, dropout_rate = 0.3)\n",
    "\n",
    "        # single LSTM\n",
    "        outputs,states = tf.nn.dynamic_rnn(lstm_cell, flat_sentence_embedding,\n",
    "                                                       dtype=tf.float32)\n",
    "        shapedOutputs = tf.squeeze(tf.slice(outputs,[0,0,0],[-1,1,-1]))\n",
    "        denseLayer = dense_layer(shapedOutputs, lstm_units, numberOfTags, activation=None)\n",
    "\n",
    "        '''\n",
    "        # BiLSTM layers\n",
    "        \n",
    "        (outputs,states) = tf.nn.bidirectional_dynamic_rnn(lstm_cell,\n",
    "                                                       lstm_cell, \n",
    "                                                       flat_sentence_embedding,\n",
    "                                                       dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # Need to reshape the outputs to be include only the last output from the sequence, \n",
    "        # but every instance in batch and every lstm output\n",
    "        (forwardOutputs,backwardOutputs) = outputs\n",
    "        print('***')\n",
    "        print(forwardOutputs)\n",
    "        print(tf.squeeze(tf.slice(forwardOutputs, [0,-1,0],[-1,1,-1])))\n",
    "        reshapedForward = tf.squeeze(tf.slice(forwardOutputs, [0,-1,0],[-1,1,-1]))\n",
    "        reshapedBackward = tf.squeeze(tf.slice(backwardOutputs, [0,-1,0],[-1,1,-1]))\n",
    "        shapedOutputs = tf.concat([reshapedForward,reshapedBackward], 1)\n",
    "        #shapedOutputs = tf.concat(outputs,2)\n",
    "        \n",
    "        print(shapedOutputs)\n",
    "        # Hidden layer, with relu activation as we want it to output word embeddings\n",
    "        denseLayer = dense_layer(shapedOutputs, lstm_units*2, numberOfTags)\n",
    "        '''\n",
    "        print(denseLayer)\n",
    "        print(targetLabels)\n",
    "        self.loss, self.optimizer = loss_optimizer(denseLayer, targetLabels, learning_rate)\n",
    "        \n",
    "        correct = tf.equal(tf.cast(tf.round(denseLayer), tf.int32), tf.cast(targetLabels, tf.int32))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3bVXv36axfu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "bias done\n",
      "done\n",
      "this works\n",
      "Tensor(\"add:0\", shape=(50, 94), dtype=float32)\n",
      "Tensor(\"targetLabels:0\", shape=(50, 94), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Tensor(\"targetLabels:0\", shape=(50, 94), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(50, 94), dtype=float32)\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "model2 = DATagging(learning_rate=0.001, \n",
    "                     batch_size=50, \n",
    "                     seq_len=MAX_LENGTH, \n",
    "                     vocab_size=len(wordvectors) + 1, \n",
    "                     embed_size=300,\n",
    "                     conv_filters=32, \n",
    "                     conv_filter_size=5, \n",
    "                     number_of_lstm_layers=1, \n",
    "                     lstm_units=128)\n",
    "#model2.summary()\n",
    "#history = model2.fit(train_sentences_X,y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEmbeddings = tf.Variable(tf.random_uniform([len(wordvectors)+1, 300]))\n",
    "# Embedding layer\n",
    "embeddingLayer = tf.nn.embedding_lookup(wordEmbeddings, train_sentences_X[:10])\n",
    "embeddingLayer = tf.expand_dims(embeddingLayer, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(10, 250, 300, 1) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "bias done\n",
      "done\n",
      "this works\n"
     ]
    }
   ],
   "source": [
    "number_of_channels = 1\n",
    "\n",
    "sentence_embedding = text_conv(embeddingLayer, 5, number_of_channels=1, number_of_filters=32)\n",
    "flat_sentence_embedding, size = flatten(sentence_embedding, 10, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-902c7c1092d4>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "lstm_cell, init_state = lstm_layer(128, 1, 10, dropout_rate = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # single LSTM\n",
    "outputs,states = tf.nn.dynamic_rnn(lstm_cell, flat_sentence_embedding,\n",
    "                                    dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapedOutputs = tf.squeeze(tf.slice(outputs,[0,0,0],[-1,1,-1]))\n",
    "#         denseLayer = dense_layer(shapedOutputs, lstm_units, numberOfTags, activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKMmrfuisKGJ"
   },
   "source": [
    "Compared to the baseline using BiLSTM for utterance classification, the second method effectively leverage context information and achieve better performance. Report your overall accuracy. Did context help disambiguate and better predict the minority classes ('br' and 'bf')? What are frequent errors? Show one positive example where adding context changed the prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUZt48JgrE34"
   },
   "source": [
    "## Advanced: Creating End-To-End Dialogue System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE63Q5guuPdA"
   },
   "source": [
    "In the last section we want to create end-to-end dialogue system, following on from the seq2seq MT labs you've \n",
    "just done. This is an advanced part of the assignment and worth 10 marks (20%) in total. In end-to-end dialogue systems, the encoder represents each utterance with a vector. The utterance vector is the hidden state after the last token of the utterance has been processed. The context LSTM keeps track of past utterances. The hidden state can be explained as the state of the dialogue system. The next utterance prediction is performed by a decoder LSTM, which takes the hidden state of the last LSTM and produces a probability distribution over the tokens in the next utterance. You can take the DA LSTM state of last section as input to a decoder which tries to generate the next utterance. You can add attention and monitor the performance. Instead of evaluating by an automatic evaluation method, you can show us some of the interesting predictions. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "eenlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
